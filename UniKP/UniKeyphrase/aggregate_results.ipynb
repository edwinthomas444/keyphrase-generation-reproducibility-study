{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062bf834-2bd8-43a8-be6b-750f06b7f33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec3e099-c0f9-4db2-b6b3-ff78e66b0654",
   "metadata": {},
   "source": [
    "# Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "893cca74-b4a7-4d57-8ef4-9a853d4240d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_list = ['kp20k','krapivin','inspec','semeval','nus']\n",
    "results = {\n",
    "    'Dataset': dataset_list,\n",
    "    'P@M-Present':[],\n",
    "    'R@M-Present':[],\n",
    "    'F1@M-Present':[],\n",
    "    'P@M-Absent':[],\n",
    "    'R@M-Absent':[],\n",
    "    'F1@M-Absent':[],\n",
    "    'P@k-Present':[],\n",
    "    'R@k-Present':[],\n",
    "    'F1@k-Present':[],\n",
    "    'P@k-Absent':[],\n",
    "    'R@k-Absent':[],\n",
    "    'F1@k-Absent':[],\n",
    "    'P@o-Present':[],\n",
    "    'R@o-Present':[],\n",
    "    'F1@o-Present':[],\n",
    "    'P@o-Absent':[],\n",
    "    'R@o-Absent':[],\n",
    "    'F1@o-Absent':[],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6aec0ae3-127a-404f-9925-474ce14f3add",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  kp20k\n",
      "dataset:  krapivin\n",
      "dataset:  inspec\n",
      "dataset:  semeval\n",
      "dataset:  nus\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds_dict = {}\n",
    "for dataset in dataset_list:\n",
    "    file_path = f'/home/thomased/work/codebase/coopsummer2023/UniKP/UniKeyphrase/results/{dataset}/kp20k_5ep_b128_l384_ddp_final_BeamNGram/metrics'\n",
    "    print('dataset: ', dataset)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        # micro\n",
    "        # present (2, 4, 6)\n",
    "        present_prfm = lines[1].strip().split(\" \")\n",
    "        results['P@M-Present'].append(round(float(present_prfm[0])*100,2))\n",
    "        results['R@M-Present'].append(round(float(present_prfm[1])*100,2))\n",
    "        results['F1@M-Present'].append(round(float(present_prfm[2])*100,2))\n",
    "\n",
    "        present_prf5 = lines[3].strip().split(\" \")\n",
    "        results['P@k-Present'].append(round(float(present_prf5[0])*100,2))\n",
    "        results['R@k-Present'].append(round(float(present_prf5[1])*100,2))\n",
    "        results['F1@k-Present'].append(round(float(present_prf5[2])*100,2))\n",
    "        \n",
    "        present_prfo = lines[5].strip().split(\" \")\n",
    "        results['P@o-Present'].append(round(float(present_prfo[0])*100,2))\n",
    "        results['R@o-Present'].append(round(float(present_prfo[1])*100,2))\n",
    "        results['F1@o-Present'].append(round(float(present_prfo[2])*100,2))\n",
    "        \n",
    "        # absent (16, 18, 20)\n",
    "        absent_prfm = lines[15].strip().split(\" \")\n",
    "        results['P@M-Absent'].append(round(float(absent_prfm[0])*100,2))\n",
    "        results['R@M-Absent'].append(round(float(absent_prfm[1])*100,2))\n",
    "        results['F1@M-Absent'].append(round(float(absent_prfm[2])*100,2))\n",
    "\n",
    "        absent_prf5 = lines[17].strip().split(\" \")\n",
    "        results['P@k-Absent'].append(round(float(absent_prf5[0])*100,2))\n",
    "        results['R@k-Absent'].append(round(float(absent_prf5[1])*100,2))\n",
    "        results['F1@k-Absent'].append(round(float(absent_prf5[2])*100,2))\n",
    "        \n",
    "        absent_prfo = lines[19].strip().split(\" \")\n",
    "        results['P@o-Absent'].append(round(float(absent_prfo[0])*100,2))\n",
    "        results['R@o-Absent'].append(round(float(absent_prfo[1])*100,2))\n",
    "        results['F1@o-Absent'].append(round(float(absent_prfo[2])*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64afbf1b-aded-4bd5-b07d-d2e19187899a",
   "metadata": {},
   "source": [
    "# UniLM + SRL + BOKP (UniKP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "739627fc-356f-4c64-89c5-b9db3f79d90f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>P@M-Present</th>\n",
       "      <th>R@M-Present</th>\n",
       "      <th>F1@M-Present</th>\n",
       "      <th>P@M-Absent</th>\n",
       "      <th>R@M-Absent</th>\n",
       "      <th>F1@M-Absent</th>\n",
       "      <th>P@k-Present</th>\n",
       "      <th>R@k-Present</th>\n",
       "      <th>F1@k-Present</th>\n",
       "      <th>P@k-Absent</th>\n",
       "      <th>R@k-Absent</th>\n",
       "      <th>F1@k-Absent</th>\n",
       "      <th>P@o-Present</th>\n",
       "      <th>R@o-Present</th>\n",
       "      <th>F1@o-Present</th>\n",
       "      <th>P@o-Absent</th>\n",
       "      <th>R@o-Absent</th>\n",
       "      <th>F1@o-Absent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kp20k</td>\n",
       "      <td>21.30</td>\n",
       "      <td>67.36</td>\n",
       "      <td>32.37</td>\n",
       "      <td>4.72</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.62</td>\n",
       "      <td>30.90</td>\n",
       "      <td>44.67</td>\n",
       "      <td>36.53</td>\n",
       "      <td>5.01</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.60</td>\n",
       "      <td>47.18</td>\n",
       "      <td>46.30</td>\n",
       "      <td>46.74</td>\n",
       "      <td>5.07</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>krapivin</td>\n",
       "      <td>17.98</td>\n",
       "      <td>55.09</td>\n",
       "      <td>27.11</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.63</td>\n",
       "      <td>27.64</td>\n",
       "      <td>40.79</td>\n",
       "      <td>32.95</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.66</td>\n",
       "      <td>37.98</td>\n",
       "      <td>36.46</td>\n",
       "      <td>37.20</td>\n",
       "      <td>3.48</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inspec</td>\n",
       "      <td>32.91</td>\n",
       "      <td>36.10</td>\n",
       "      <td>34.44</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.24</td>\n",
       "      <td>39.02</td>\n",
       "      <td>24.13</td>\n",
       "      <td>29.82</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.30</td>\n",
       "      <td>37.10</td>\n",
       "      <td>31.14</td>\n",
       "      <td>33.86</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semeval</td>\n",
       "      <td>22.01</td>\n",
       "      <td>52.50</td>\n",
       "      <td>31.02</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.62</td>\n",
       "      <td>41.96</td>\n",
       "      <td>31.26</td>\n",
       "      <td>35.83</td>\n",
       "      <td>4.48</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.63</td>\n",
       "      <td>37.85</td>\n",
       "      <td>37.33</td>\n",
       "      <td>37.59</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nus</td>\n",
       "      <td>29.57</td>\n",
       "      <td>60.62</td>\n",
       "      <td>39.75</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.40</td>\n",
       "      <td>45.52</td>\n",
       "      <td>41.70</td>\n",
       "      <td>43.52</td>\n",
       "      <td>5.81</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.41</td>\n",
       "      <td>45.86</td>\n",
       "      <td>47.95</td>\n",
       "      <td>46.88</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset  P@M-Present  R@M-Present  F1@M-Present  P@M-Absent  R@M-Absent  \\\n",
       "0     kp20k        21.30        67.36         32.37        4.72        1.81   \n",
       "1  krapivin        17.98        55.09         27.11        3.20        1.10   \n",
       "2    inspec        32.91        36.10         34.44        1.87        0.93   \n",
       "3   semeval        22.01        52.50         31.02        3.66        0.34   \n",
       "4       nus        29.57        60.62         39.75        5.43        0.80   \n",
       "\n",
       "   F1@M-Absent  P@k-Present  R@k-Present  F1@k-Present  P@k-Absent  \\\n",
       "0         2.62        30.90        44.67         36.53        5.01   \n",
       "1         1.63        27.64        40.79         32.95        3.44   \n",
       "2         1.24        39.02        24.13         29.82        2.13   \n",
       "3         0.62        41.96        31.26         35.83        4.48   \n",
       "4         1.40        45.52        41.70         43.52        5.81   \n",
       "\n",
       "   R@k-Absent  F1@k-Absent  P@o-Present  R@o-Present  F1@o-Present  \\\n",
       "0        1.76         2.60        47.18        46.30         46.74   \n",
       "1        1.10         1.66        37.98        36.46         37.20   \n",
       "2        0.93         1.30        37.10        31.14         33.86   \n",
       "3        0.34         0.63        37.85        37.33         37.59   \n",
       "4        0.80         1.41        45.86        47.95         46.88   \n",
       "\n",
       "   P@o-Absent  R@o-Absent  F1@o-Absent  \n",
       "0        5.07        1.76         2.61  \n",
       "1        3.48        1.10         1.67  \n",
       "2        1.97        0.84         1.18  \n",
       "3        4.35        0.34         0.63  \n",
       "4        5.68        0.80         1.41  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UniLM + SRL + BOKP (UniKeyPhrase)\n",
    "df_results = pd.DataFrame(data=results, columns = list(results.keys()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24a44934-350d-40c9-b885-84acfa1d3591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>P@M-Present</th>\n",
       "      <th>R@M-Present</th>\n",
       "      <th>F1@M-Present</th>\n",
       "      <th>P@M-Absent</th>\n",
       "      <th>R@M-Absent</th>\n",
       "      <th>F1@M-Absent</th>\n",
       "      <th>P@k-Present</th>\n",
       "      <th>R@k-Present</th>\n",
       "      <th>F1@k-Present</th>\n",
       "      <th>P@k-Absent</th>\n",
       "      <th>R@k-Absent</th>\n",
       "      <th>F1@k-Absent</th>\n",
       "      <th>P@o-Present</th>\n",
       "      <th>R@o-Present</th>\n",
       "      <th>F1@o-Present</th>\n",
       "      <th>P@o-Absent</th>\n",
       "      <th>R@o-Absent</th>\n",
       "      <th>F1@o-Absent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kp20k</td>\n",
       "      <td>21.30</td>\n",
       "      <td>67.36</td>\n",
       "      <td>32.37</td>\n",
       "      <td>5.61</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.59</td>\n",
       "      <td>30.90</td>\n",
       "      <td>44.67</td>\n",
       "      <td>36.53</td>\n",
       "      <td>5.71</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.59</td>\n",
       "      <td>47.18</td>\n",
       "      <td>46.30</td>\n",
       "      <td>46.74</td>\n",
       "      <td>5.71</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>krapivin</td>\n",
       "      <td>17.98</td>\n",
       "      <td>55.09</td>\n",
       "      <td>27.11</td>\n",
       "      <td>4.36</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.85</td>\n",
       "      <td>27.64</td>\n",
       "      <td>40.79</td>\n",
       "      <td>32.95</td>\n",
       "      <td>4.37</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.85</td>\n",
       "      <td>37.98</td>\n",
       "      <td>36.46</td>\n",
       "      <td>37.20</td>\n",
       "      <td>4.39</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inspec</td>\n",
       "      <td>32.91</td>\n",
       "      <td>36.10</td>\n",
       "      <td>34.44</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.21</td>\n",
       "      <td>39.02</td>\n",
       "      <td>24.13</td>\n",
       "      <td>29.82</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.21</td>\n",
       "      <td>37.10</td>\n",
       "      <td>31.14</td>\n",
       "      <td>33.86</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semeval</td>\n",
       "      <td>22.01</td>\n",
       "      <td>52.50</td>\n",
       "      <td>31.02</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.43</td>\n",
       "      <td>41.96</td>\n",
       "      <td>31.26</td>\n",
       "      <td>35.83</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.43</td>\n",
       "      <td>37.85</td>\n",
       "      <td>37.33</td>\n",
       "      <td>37.59</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nus</td>\n",
       "      <td>29.57</td>\n",
       "      <td>60.62</td>\n",
       "      <td>39.75</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.01</td>\n",
       "      <td>45.52</td>\n",
       "      <td>41.70</td>\n",
       "      <td>43.52</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.01</td>\n",
       "      <td>45.86</td>\n",
       "      <td>47.95</td>\n",
       "      <td>46.88</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset  P@M-Present  R@M-Present  F1@M-Present  P@M-Absent  R@M-Absent  \\\n",
       "0     kp20k        21.30        67.36         32.37        5.61        1.68   \n",
       "1  krapivin        17.98        55.09         27.11        4.36        1.17   \n",
       "2    inspec        32.91        36.10         34.44        2.16        0.84   \n",
       "3   semeval        22.01        52.50         31.02        4.26        0.23   \n",
       "4       nus        29.57        60.62         39.75        5.00        0.56   \n",
       "\n",
       "   F1@M-Absent  P@k-Present  R@k-Present  F1@k-Present  P@k-Absent  \\\n",
       "0         2.59        30.90        44.67         36.53        5.71   \n",
       "1         1.85        27.64        40.79         32.95        4.37   \n",
       "2         1.21        39.02        24.13         29.82        2.18   \n",
       "3         0.43        41.96        31.26         35.83        4.65   \n",
       "4         1.01        45.52        41.70         43.52        5.04   \n",
       "\n",
       "   R@k-Absent  F1@k-Absent  P@o-Present  R@o-Present  F1@o-Present  \\\n",
       "0        1.68         2.59        47.18        46.30         46.74   \n",
       "1        1.17         1.85        37.98        36.46         37.20   \n",
       "2        0.84         1.21        37.10        31.14         33.86   \n",
       "3        0.23         0.43        37.85        37.33         37.59   \n",
       "4        0.56         1.01        45.86        47.95         46.88   \n",
       "\n",
       "   P@o-Absent  R@o-Absent  F1@o-Absent  \n",
       "0        5.71        1.67         2.59  \n",
       "1        4.39        1.17         1.85  \n",
       "2        2.18        0.84         1.21  \n",
       "3        4.65        0.23         0.43  \n",
       "4        5.04        0.56         1.01  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UniLM + SRL + BOKP (UniKeyPhrase)\n",
    "df_results = pd.DataFrame(data=results, columns = list(results.keys()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92957ae7-9393-40ba-8bc9-4f0c1b43c1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>P@M-Present</th>\n",
       "      <th>R@M-Present</th>\n",
       "      <th>F1@M-Present</th>\n",
       "      <th>P@M-Absent</th>\n",
       "      <th>R@M-Absent</th>\n",
       "      <th>F1@M-Absent</th>\n",
       "      <th>P@k-Present</th>\n",
       "      <th>R@k-Present</th>\n",
       "      <th>F1@k-Present</th>\n",
       "      <th>P@k-Absent</th>\n",
       "      <th>R@k-Absent</th>\n",
       "      <th>F1@k-Absent</th>\n",
       "      <th>P@o-Present</th>\n",
       "      <th>R@o-Present</th>\n",
       "      <th>F1@o-Present</th>\n",
       "      <th>P@o-Absent</th>\n",
       "      <th>R@o-Absent</th>\n",
       "      <th>F1@o-Absent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kp20k</td>\n",
       "      <td>21.30</td>\n",
       "      <td>67.36</td>\n",
       "      <td>32.37</td>\n",
       "      <td>6.05</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.79</td>\n",
       "      <td>30.90</td>\n",
       "      <td>44.67</td>\n",
       "      <td>36.53</td>\n",
       "      <td>6.05</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.79</td>\n",
       "      <td>47.18</td>\n",
       "      <td>46.30</td>\n",
       "      <td>46.74</td>\n",
       "      <td>6.05</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>krapivin</td>\n",
       "      <td>17.98</td>\n",
       "      <td>55.09</td>\n",
       "      <td>27.11</td>\n",
       "      <td>4.40</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.85</td>\n",
       "      <td>27.64</td>\n",
       "      <td>40.79</td>\n",
       "      <td>32.95</td>\n",
       "      <td>4.40</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.85</td>\n",
       "      <td>37.98</td>\n",
       "      <td>36.46</td>\n",
       "      <td>37.20</td>\n",
       "      <td>4.40</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inspec</td>\n",
       "      <td>32.91</td>\n",
       "      <td>36.10</td>\n",
       "      <td>34.44</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.21</td>\n",
       "      <td>39.02</td>\n",
       "      <td>24.13</td>\n",
       "      <td>29.82</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.21</td>\n",
       "      <td>37.10</td>\n",
       "      <td>31.14</td>\n",
       "      <td>33.86</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semeval</td>\n",
       "      <td>22.01</td>\n",
       "      <td>52.50</td>\n",
       "      <td>31.02</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.43</td>\n",
       "      <td>41.96</td>\n",
       "      <td>31.26</td>\n",
       "      <td>35.83</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.43</td>\n",
       "      <td>37.85</td>\n",
       "      <td>37.33</td>\n",
       "      <td>37.59</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nus</td>\n",
       "      <td>29.57</td>\n",
       "      <td>60.62</td>\n",
       "      <td>39.75</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.00</td>\n",
       "      <td>45.52</td>\n",
       "      <td>41.70</td>\n",
       "      <td>43.52</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.00</td>\n",
       "      <td>45.86</td>\n",
       "      <td>47.95</td>\n",
       "      <td>46.88</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset  P@M-Present  R@M-Present  F1@M-Present  P@M-Absent  R@M-Absent  \\\n",
       "0     kp20k        21.30        67.36         32.37        6.05        1.82   \n",
       "1  krapivin        17.98        55.09         27.11        4.40        1.17   \n",
       "2    inspec        32.91        36.10         34.44        2.18        0.84   \n",
       "3   semeval        22.01        52.50         31.02        4.35        0.23   \n",
       "4       nus        29.57        60.62         39.75        4.70        0.56   \n",
       "\n",
       "   F1@M-Absent  P@k-Present  R@k-Present  F1@k-Present  P@k-Absent  \\\n",
       "0         2.79        30.90        44.67         36.53        6.05   \n",
       "1         1.85        27.64        40.79         32.95        4.40   \n",
       "2         1.21        39.02        24.13         29.82        2.18   \n",
       "3         0.43        41.96        31.26         35.83        4.35   \n",
       "4         1.00        45.52        41.70         43.52        4.70   \n",
       "\n",
       "   R@k-Absent  F1@k-Absent  P@o-Present  R@o-Present  F1@o-Present  \\\n",
       "0        1.81         2.79        47.18        46.30         46.74   \n",
       "1        1.17         1.85        37.98        36.46         37.20   \n",
       "2        0.84         1.21        37.10        31.14         33.86   \n",
       "3        0.23         0.43        37.85        37.33         37.59   \n",
       "4        0.56         1.00        45.86        47.95         46.88   \n",
       "\n",
       "   P@o-Absent  R@o-Absent  F1@o-Absent  \n",
       "0        6.05        1.81         2.79  \n",
       "1        4.40        1.17         1.85  \n",
       "2        2.18        0.84         1.21  \n",
       "3        4.44        0.23         0.43  \n",
       "4        4.70        0.56         1.00  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(data=results, columns = list(results.keys()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8a96184-654b-456b-9c94-be27d5a52e76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_path = f'/home/thomased/work/codebase/coopsummer2023/UniKP/UniKeyphrase/results/metrics_micro.csv'\n",
    "df_results.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a1dbe-7030-4b19-95cd-ebaef713f08a",
   "metadata": {},
   "source": [
    "# Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "840c3e0c-1123-4d42-90a3-dedc9e150926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_list = ['kp20k','krapivin','inspec','semeval','nus']\n",
    "results = {\n",
    "    'Dataset': dataset_list,\n",
    "    'P@M-Present':[],\n",
    "    'R@M-Present':[],\n",
    "    'F1@M-Present':[],\n",
    "    'P@M-Absent':[],\n",
    "    'R@M-Absent':[],\n",
    "    'F1@M-Absent':[],\n",
    "    'P@k-Present':[],\n",
    "    'R@k-Present':[],\n",
    "    'F1@k-Present':[],\n",
    "    'P@k-Absent':[],\n",
    "    'R@k-Absent':[],\n",
    "    'F1@k-Absent':[],\n",
    "    'P@o-Present':[],\n",
    "    'R@o-Present':[],\n",
    "    'F1@o-Present':[],\n",
    "    'P@o-Absent':[],\n",
    "    'R@o-Absent':[],\n",
    "    'F1@o-Absent':[],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "717fbdbd-6c7f-4f14-b500-26b2de019106",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  kp20k\n",
      "dataset:  krapivin\n",
      "dataset:  inspec\n",
      "dataset:  semeval\n",
      "dataset:  nus\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds_dict = {}\n",
    "for dataset in dataset_list:\n",
    "    file_path = f'/home/thomased/work/codebase/coopsummer2023/UniKP/UniKeyphrase/results/{dataset}/kp20k_5ep_b128_l384_ddp_final_BeamNGram/metrics'\n",
    "    print('dataset: ', dataset)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        # macro\n",
    "        # present (8, 10, 12)\n",
    "        present_prfm = lines[7].strip().split(\" \")\n",
    "        results['P@M-Present'].append(round(float(present_prfm[0])*100,2))\n",
    "        results['R@M-Present'].append(round(float(present_prfm[1])*100,2))\n",
    "        results['F1@M-Present'].append(round(float(present_prfm[2])*100,2))\n",
    "\n",
    "        present_prf5 = lines[9].strip().split(\" \")\n",
    "        results['P@k-Present'].append(round(float(present_prf5[0])*100,2))\n",
    "        results['R@k-Present'].append(round(float(present_prf5[1])*100,2))\n",
    "        results['F1@k-Present'].append(round(float(present_prf5[2])*100,2))\n",
    "        \n",
    "        present_prfo = lines[11].strip().split(\" \")\n",
    "        results['P@o-Present'].append(round(float(present_prfo[0])*100,2))\n",
    "        results['R@o-Present'].append(round(float(present_prfo[1])*100,2))\n",
    "        results['F1@o-Present'].append(round(float(present_prfo[2])*100,2))\n",
    "        \n",
    "        # absent (22, 24, 26)\n",
    "        absent_prfm = lines[21].strip().split(\" \")\n",
    "        results['P@M-Absent'].append(round(float(absent_prfm[0])*100,2))\n",
    "        results['R@M-Absent'].append(round(float(absent_prfm[1])*100,2))\n",
    "        results['F1@M-Absent'].append(round(float(absent_prfm[2])*100,2))\n",
    "\n",
    "        absent_prf5 = lines[23].strip().split(\" \")\n",
    "        results['P@k-Absent'].append(round(float(absent_prf5[0])*100,2))\n",
    "        results['R@k-Absent'].append(round(float(absent_prf5[1])*100,2))\n",
    "        results['F1@k-Absent'].append(round(float(absent_prf5[2])*100,2))\n",
    "        \n",
    "        absent_prfo = lines[25].strip().split(\" \")\n",
    "        results['P@o-Absent'].append(round(float(absent_prfo[0])*100,2))\n",
    "        results['R@o-Absent'].append(round(float(absent_prfo[1])*100,2))\n",
    "        results['F1@o-Absent'].append(round(float(absent_prfo[2])*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3873f441-0597-4b5f-8bf6-4003a6aeb126",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>P@M-Present</th>\n",
       "      <th>R@M-Present</th>\n",
       "      <th>F1@M-Present</th>\n",
       "      <th>P@M-Absent</th>\n",
       "      <th>R@M-Absent</th>\n",
       "      <th>F1@M-Absent</th>\n",
       "      <th>P@k-Present</th>\n",
       "      <th>R@k-Present</th>\n",
       "      <th>F1@k-Present</th>\n",
       "      <th>P@k-Absent</th>\n",
       "      <th>R@k-Absent</th>\n",
       "      <th>F1@k-Absent</th>\n",
       "      <th>P@o-Present</th>\n",
       "      <th>R@o-Present</th>\n",
       "      <th>F1@o-Present</th>\n",
       "      <th>P@o-Absent</th>\n",
       "      <th>R@o-Absent</th>\n",
       "      <th>F1@o-Absent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kp20k</td>\n",
       "      <td>20.68</td>\n",
       "      <td>64.40</td>\n",
       "      <td>31.30</td>\n",
       "      <td>3.54</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.22</td>\n",
       "      <td>30.95</td>\n",
       "      <td>50.13</td>\n",
       "      <td>38.27</td>\n",
       "      <td>3.79</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.22</td>\n",
       "      <td>39.84</td>\n",
       "      <td>39.38</td>\n",
       "      <td>39.61</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>krapivin</td>\n",
       "      <td>19.12</td>\n",
       "      <td>59.47</td>\n",
       "      <td>28.93</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.05</td>\n",
       "      <td>27.67</td>\n",
       "      <td>47.50</td>\n",
       "      <td>34.97</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.06</td>\n",
       "      <td>36.85</td>\n",
       "      <td>36.12</td>\n",
       "      <td>36.48</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inspec</td>\n",
       "      <td>34.77</td>\n",
       "      <td>40.39</td>\n",
       "      <td>37.37</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.51</td>\n",
       "      <td>39.64</td>\n",
       "      <td>29.44</td>\n",
       "      <td>33.79</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.54</td>\n",
       "      <td>38.11</td>\n",
       "      <td>33.31</td>\n",
       "      <td>35.55</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semeval</td>\n",
       "      <td>25.71</td>\n",
       "      <td>57.69</td>\n",
       "      <td>35.56</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.67</td>\n",
       "      <td>42.07</td>\n",
       "      <td>37.64</td>\n",
       "      <td>39.73</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.67</td>\n",
       "      <td>40.95</td>\n",
       "      <td>40.79</td>\n",
       "      <td>40.87</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nus</td>\n",
       "      <td>32.75</td>\n",
       "      <td>64.51</td>\n",
       "      <td>43.44</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.23</td>\n",
       "      <td>45.70</td>\n",
       "      <td>48.03</td>\n",
       "      <td>46.83</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.23</td>\n",
       "      <td>46.25</td>\n",
       "      <td>47.86</td>\n",
       "      <td>47.04</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset  P@M-Present  R@M-Present  F1@M-Present  P@M-Absent  R@M-Absent  \\\n",
       "0     kp20k        20.68        64.40         31.30        3.54        1.61   \n",
       "1  krapivin        19.12        59.47         28.93        2.90        1.58   \n",
       "2    inspec        34.77        40.39         37.37        1.73        1.35   \n",
       "3   semeval        25.71        57.69         35.56        3.00        0.38   \n",
       "4       nus        32.75        64.51         43.44        3.79        0.73   \n",
       "\n",
       "   F1@M-Absent  P@k-Present  R@k-Present  F1@k-Present  P@k-Absent  \\\n",
       "0         2.22        30.95        50.13         38.27        3.79   \n",
       "1         2.05        27.67        47.50         34.97        2.93   \n",
       "2         1.51        39.64        29.44         33.79        1.80   \n",
       "3         0.67        42.07        37.64         39.73        3.00   \n",
       "4         1.23        45.70        48.03         46.83        3.87   \n",
       "\n",
       "   R@k-Absent  F1@k-Absent  P@o-Present  R@o-Present  F1@o-Present  \\\n",
       "0        1.57         2.22        39.84        39.38         39.61   \n",
       "1        1.58         2.06        36.85        36.12         36.48   \n",
       "2        1.35         1.54        38.11        33.31         35.55   \n",
       "3        0.38         0.67        40.95        40.79         40.87   \n",
       "4        0.73         1.23        46.25        47.86         47.04   \n",
       "\n",
       "   P@o-Absent  R@o-Absent  F1@o-Absent  \n",
       "0        3.80        1.57         2.22  \n",
       "1        2.93        1.58         2.06  \n",
       "2        1.70        1.15         1.37  \n",
       "3        3.00        0.38         0.67  \n",
       "4        3.79        0.73         1.23  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UniLM + SRL + BOKP (UniKeyPhrase)\n",
    "df_results = pd.DataFrame(data=results, columns = list(results.keys()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c541282-97ca-4c27-9ee4-1bd2edc057eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>P@M-Present</th>\n",
       "      <th>R@M-Present</th>\n",
       "      <th>F1@M-Present</th>\n",
       "      <th>P@M-Absent</th>\n",
       "      <th>R@M-Absent</th>\n",
       "      <th>F1@M-Absent</th>\n",
       "      <th>P@k-Present</th>\n",
       "      <th>R@k-Present</th>\n",
       "      <th>F1@k-Present</th>\n",
       "      <th>P@k-Absent</th>\n",
       "      <th>R@k-Absent</th>\n",
       "      <th>F1@k-Absent</th>\n",
       "      <th>P@o-Present</th>\n",
       "      <th>R@o-Present</th>\n",
       "      <th>F1@o-Present</th>\n",
       "      <th>P@o-Absent</th>\n",
       "      <th>R@o-Absent</th>\n",
       "      <th>F1@o-Absent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kp20k</td>\n",
       "      <td>20.68</td>\n",
       "      <td>64.40</td>\n",
       "      <td>31.30</td>\n",
       "      <td>3.57</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.14</td>\n",
       "      <td>30.95</td>\n",
       "      <td>50.13</td>\n",
       "      <td>38.27</td>\n",
       "      <td>3.74</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.16</td>\n",
       "      <td>39.84</td>\n",
       "      <td>39.38</td>\n",
       "      <td>39.61</td>\n",
       "      <td>3.74</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>krapivin</td>\n",
       "      <td>19.12</td>\n",
       "      <td>59.47</td>\n",
       "      <td>28.93</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.17</td>\n",
       "      <td>27.67</td>\n",
       "      <td>47.50</td>\n",
       "      <td>34.97</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.17</td>\n",
       "      <td>36.85</td>\n",
       "      <td>36.12</td>\n",
       "      <td>36.48</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inspec</td>\n",
       "      <td>34.77</td>\n",
       "      <td>40.39</td>\n",
       "      <td>37.37</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.36</td>\n",
       "      <td>39.64</td>\n",
       "      <td>29.44</td>\n",
       "      <td>33.79</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.36</td>\n",
       "      <td>38.11</td>\n",
       "      <td>33.31</td>\n",
       "      <td>35.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semeval</td>\n",
       "      <td>25.71</td>\n",
       "      <td>57.69</td>\n",
       "      <td>35.56</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.47</td>\n",
       "      <td>42.07</td>\n",
       "      <td>37.64</td>\n",
       "      <td>39.73</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.47</td>\n",
       "      <td>40.95</td>\n",
       "      <td>40.79</td>\n",
       "      <td>40.87</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nus</td>\n",
       "      <td>32.75</td>\n",
       "      <td>64.51</td>\n",
       "      <td>43.44</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.53</td>\n",
       "      <td>45.70</td>\n",
       "      <td>48.03</td>\n",
       "      <td>46.83</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.53</td>\n",
       "      <td>46.25</td>\n",
       "      <td>47.86</td>\n",
       "      <td>47.04</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset  P@M-Present  R@M-Present  F1@M-Present  P@M-Absent  R@M-Absent  \\\n",
       "0     kp20k        20.68        64.40         31.30        3.57        1.52   \n",
       "1  krapivin        19.12        59.47         28.93        3.26        1.63   \n",
       "2    inspec        34.77        40.39         37.37        1.80        1.10   \n",
       "3   semeval        25.71        57.69         35.56        2.00        0.27   \n",
       "4       nus        32.75        64.51         43.44        3.32        1.00   \n",
       "\n",
       "   F1@M-Absent  P@k-Present  R@k-Present  F1@k-Present  P@k-Absent  \\\n",
       "0         2.14        30.95        50.13         38.27        3.74   \n",
       "1         2.17        27.67        47.50         34.97        3.26   \n",
       "2         1.36        39.64        29.44         33.79        1.80   \n",
       "3         0.47        42.07        37.64         39.73        2.00   \n",
       "4         1.53        45.70        48.03         46.83        3.32   \n",
       "\n",
       "   R@k-Absent  F1@k-Absent  P@o-Present  R@o-Present  F1@o-Present  \\\n",
       "0        1.52         2.16        39.84        39.38         39.61   \n",
       "1        1.63         2.17        36.85        36.12         36.48   \n",
       "2        1.10         1.36        38.11        33.31         35.55   \n",
       "3        0.27         0.47        40.95        40.79         40.87   \n",
       "4        1.00         1.53        46.25        47.86         47.04   \n",
       "\n",
       "   P@o-Absent  R@o-Absent  F1@o-Absent  \n",
       "0        3.74        1.52         2.16  \n",
       "1        3.26        1.63         2.17  \n",
       "2        1.80        1.10         1.36  \n",
       "3        2.00        0.27         0.47  \n",
       "4        3.32        1.00         1.53  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UniLM + SRL + BOKP (UniKeyPhrase)\n",
    "df_results = pd.DataFrame(data=results, columns = list(results.keys()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11689536-8bd5-4258-a3ee-f29ebbbda30f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>P@M-Present</th>\n",
       "      <th>R@M-Present</th>\n",
       "      <th>F1@M-Present</th>\n",
       "      <th>P@M-Absent</th>\n",
       "      <th>R@M-Absent</th>\n",
       "      <th>F1@M-Absent</th>\n",
       "      <th>P@k-Present</th>\n",
       "      <th>R@k-Present</th>\n",
       "      <th>F1@k-Present</th>\n",
       "      <th>P@k-Absent</th>\n",
       "      <th>R@k-Absent</th>\n",
       "      <th>F1@k-Absent</th>\n",
       "      <th>P@o-Present</th>\n",
       "      <th>R@o-Present</th>\n",
       "      <th>F1@o-Present</th>\n",
       "      <th>P@o-Absent</th>\n",
       "      <th>R@o-Absent</th>\n",
       "      <th>F1@o-Absent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kp20k</td>\n",
       "      <td>20.68</td>\n",
       "      <td>64.40</td>\n",
       "      <td>31.30</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.20</td>\n",
       "      <td>30.95</td>\n",
       "      <td>50.13</td>\n",
       "      <td>38.27</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.20</td>\n",
       "      <td>39.84</td>\n",
       "      <td>39.38</td>\n",
       "      <td>39.61</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>krapivin</td>\n",
       "      <td>19.12</td>\n",
       "      <td>59.47</td>\n",
       "      <td>28.93</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.17</td>\n",
       "      <td>27.67</td>\n",
       "      <td>47.50</td>\n",
       "      <td>34.97</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.17</td>\n",
       "      <td>36.85</td>\n",
       "      <td>36.12</td>\n",
       "      <td>36.48</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inspec</td>\n",
       "      <td>34.77</td>\n",
       "      <td>40.39</td>\n",
       "      <td>37.37</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.36</td>\n",
       "      <td>39.64</td>\n",
       "      <td>29.44</td>\n",
       "      <td>33.79</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.36</td>\n",
       "      <td>38.11</td>\n",
       "      <td>33.31</td>\n",
       "      <td>35.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semeval</td>\n",
       "      <td>25.71</td>\n",
       "      <td>57.69</td>\n",
       "      <td>35.56</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.47</td>\n",
       "      <td>42.07</td>\n",
       "      <td>37.64</td>\n",
       "      <td>39.73</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.47</td>\n",
       "      <td>40.95</td>\n",
       "      <td>40.79</td>\n",
       "      <td>40.87</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nus</td>\n",
       "      <td>32.75</td>\n",
       "      <td>64.51</td>\n",
       "      <td>43.44</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.53</td>\n",
       "      <td>45.70</td>\n",
       "      <td>48.03</td>\n",
       "      <td>46.83</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.53</td>\n",
       "      <td>46.25</td>\n",
       "      <td>47.86</td>\n",
       "      <td>47.04</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset  P@M-Present  R@M-Present  F1@M-Present  P@M-Absent  R@M-Absent  \\\n",
       "0     kp20k        20.68        64.40         31.30        3.65        1.58   \n",
       "1  krapivin        19.12        59.47         28.93        3.26        1.63   \n",
       "2    inspec        34.77        40.39         37.37        1.80        1.10   \n",
       "3   semeval        25.71        57.69         35.56        2.00        0.27   \n",
       "4       nus        32.75        64.51         43.44        3.32        1.00   \n",
       "\n",
       "   F1@M-Absent  P@k-Present  R@k-Present  F1@k-Present  P@k-Absent  \\\n",
       "0         2.20        30.95        50.13         38.27        3.65   \n",
       "1         2.17        27.67        47.50         34.97        3.26   \n",
       "2         1.36        39.64        29.44         33.79        1.80   \n",
       "3         0.47        42.07        37.64         39.73        2.00   \n",
       "4         1.53        45.70        48.03         46.83        3.32   \n",
       "\n",
       "   R@k-Absent  F1@k-Absent  P@o-Present  R@o-Present  F1@o-Present  \\\n",
       "0        1.58         2.20        39.84        39.38         39.61   \n",
       "1        1.63         2.17        36.85        36.12         36.48   \n",
       "2        1.10         1.36        38.11        33.31         35.55   \n",
       "3        0.27         0.47        40.95        40.79         40.87   \n",
       "4        1.00         1.53        46.25        47.86         47.04   \n",
       "\n",
       "   P@o-Absent  R@o-Absent  F1@o-Absent  \n",
       "0        3.65        1.57         2.20  \n",
       "1        3.26        1.63         2.17  \n",
       "2        1.80        1.10         1.36  \n",
       "3        2.00        0.27         0.47  \n",
       "4        3.32        1.00         1.53  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UniLM + SRL + BOKP (UniKeyPhrase)\n",
    "df_results = pd.DataFrame(data=results, columns = list(results.keys()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "907b7d13-0a0d-46a1-bf51-ddf5734ad5e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_path = f'/home/thomased/work/codebase/coopsummer2023/UniKP/UniKeyphrase/results/metrics_macro.csv'\n",
    "df_results.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151ee4a-25f3-4aae-945e-f0e4db2e1eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c731c-34b6-4e5f-935f-89f56389f716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99d2fd53-0e1c-4113-9c88-661694e56cb7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enhancing product recommender systems on sparse binary data . commercial recommender systems use various data mining techniques to make appropriate recommendations to users during online , real time sessions . published algorithms focus more on the discrete user ratings instead of binary results , which hampers their predictive capabilities when usage data is sparse . the system proposed in this paper , e <unk> , is an association mining based recommender tool designed to overcome these problems through a two phase approach . in the first phase , batches of customer historical data are analyzed through association mining in order to determine the association rules for the second phase . during the second phase , a scoring algorithm is used to rank the recommendations online for the customer . the second phase differs from the traditional approach and an empirical comparison between the methods used in e <unk> and other collaborative filtering methods including dependency networks , item based , and association mining is provided in this paper . this comparison evaluates the algorithms used in each of the above methods using two internal customer datasets and a benchmark dataset . the results of this comparison clearly show that e <unk> performs well compared to dependency networks and association mining . in general , item based algorithms with cosine similarity measures have the best performance .\n",
      "['recommender systems', 'association mining', 'collaborative filtering', 'dependency networks', 'customer relationship management', 'e commerce <eos>']\n",
      "['association rules']\n",
      "['sparse binary data', 'recommender systems', 'scoring algorithm', 'data mining', 'binary data', 'product recommender system', 'sparse data mining', 'data mining']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['recommender systems']\n",
      "\n",
      "\n",
      "logic based subsumption architecture . we describe a logic based ai architecture based on brooks ' subsumption architecture . in this architecture , we axiomatize different layers of control in first order logic ( fol ) and use independent theorem provers to derive each layer ' s outputs given its inputs . we implement the subsumption of lower layers by higher layers using nonmonotonic reasoning principles . in particular , we use circumscription to make default assumptions in lower layers , and nonmonotonically retract those assumptions when higher layers draw new conclusions . we also give formal semantics to our approach . finally , we describe layers designed for the task of robot control and a system that we have implemented that uses this architecture for the control of a nomad <digit> mobile robot . our system combines the virtues of using the represent and reason paradigm and the behavioral decomposition paradigm . it allows multiple goals to be serviced simultaneously and reactively . it also allows high level tasks and is tolerant to different changes and elaborations of its knowledge in runtime . finally , it allows us to give more commonsense knowledge to robots . we report on several experiments that empirically show the feasibility of using fully expressive fol theorem provers for robot control with our architecture and the benefits claimed above .\n",
      "['subsumption architecture', 'logical representations', 'cognitive robotics <eos>']\n",
      "['nonmonotonic reasoning']\n",
      "['subsumption architecture', 'circumscription', 'first order logic', 'logic based ai architecture', 'logic based architecture']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['subsumption architecture']\n",
      "\n",
      "\n",
      "a model of bgp routing for network engineering . the performance of ip networks depends on a wide variety of dynamic conditions . traffic shifts , equipment failures , planned maintenance , and topology changes in other parts of the internet can all degrade performance . to maintain good performance , network operators must continually reconfigure the routing protocols . operators configure bgp to control how traffic flows to neighboring autonomous systems ( ases ) , as well as how traffic traverses their networks . however , because bgp route selection is distributed , indirectly controlled by configurable policies , and influenced by complex interactions with intradomain routing protocols , operators can not predict how a particular bgp configuration would behave in practice . to avoid inadvertently degrading network performance , operators need to evaluate the effects of configuration changes before deploying them on a live network . we propose an algorithm that computes the outcome of the bgp route selection process for each router in a single as , given only a static snapshot of the network state , without simulating the complex details of bgp message passing . we describe a bgp emulator based on this algorithm the emulator exploits the unique characteristics of routing data to reduce computational overhead . using data from a large isp , we show that the emulator correctly computes bgp routing decisions and has a running time that is acceptable for many tasks , such as traffic engineering and capacity planning .\n",
      "['modeling', 'bgp', 'routing', 'traffic engineering <eos>']\n",
      "['bgp', 'algorithms']\n",
      "['network engineering', 'routing', 'bgp routing', 'experimentation', 'network performance', 'performance evaluation']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['routing']\n",
      "\n",
      "\n",
      "collective mining of bayesian networks from distributed heterogeneous data . we present a collective approach to learning a bayesian network from distributed heterogeneous data . in this approach , we first learn a local bayesian network at each site using the local data . then each site identifies the observations that are most likely to be evidence of coupling between local and non local variables and transmits a subset of these observations to a central site . another bayesian network is learnt at the central site using the data transmitted from the local site . the local and central bayesian networks are combined to obtain a collective bayesian network , which models the entire data . experimental results and theoretical justification that demonstrate the feasibility of our approach are presented .\n",
      "['bayesian network', 'heterogeneous data', 'web log mining', 'distributed data mining', 'collective data mining <eos>']\n",
      "['distributed heterogeneous data']\n",
      "['bayesian networks', 'collective mining', 'collective intelligence', 'heterogeneous data', 'distributed data mining']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['heterogeneous data']\n",
      "\n",
      "\n",
      "automatic performance evaluation of web search engines . measuring the information retrieval effectiveness of world wide web search engines is costly because of human relevance judgments involved . however , both for business enterprises and people it is important to know the most effective web search engines , since such search engines help their users find higher number of relevant web pages with less effort . furthermore , this information can be used for several practical purposes . in this study we introduce automatic web search engine evaluation method as an efficient and effective assessment tool of such systems . the experiments based on eight web search engines , <digit> queries , and binary user relevance judgments show that our method provides results consistent with human based evaluations . it is shown that the observed consistencies are statistically significant . this indicates that the new method can be successfully used in the evaluation of web search engines .\n",
      "['performance', 'search engine', 'information retrieval', 'world wide web <eos>']\n",
      "['Present predictions:']\n",
      "['web search engine', 'performance evaluation', 'information retrieval', 'search engines', 'web search engine evaluation', 'information retrieval', 'user evaluation']\n",
      "1 7 7\n",
      "additional scores from absent units to present :  ['information retrieval', 'information retrieval']\n",
      "\n",
      "\n",
      "an experimental evaluation of continuous testing during development . continuous testing uses excess cycles on a developer ' s workstation to continuously run regression tests in the background , providing rapid feedback about test failures as source code is edited . it is intended to reduce the time and energy required to keep code well tested and prevent regression errors from persisting uncaught for long periods of time . this paper reports on a controlled human experiment to evaluate whether students using continuous testing are more successful in completing programming assignments . we also summarize users ' subjective impressions and discuss why the results may generalize . the experiment indicates that the tool has a statistically significant effect on success in completing a programming task , but no such effect on time worked . participants using continuous testing were three times more likely to complete the task before the deadline than those without . participants using continuous compilation were twice as likely to complete the task , providing empirical support to a common feature in modern development environments . most participants found continuous testing to be useful and believed that it helped them write better code faster , and <digit> % would recommend the tool to others . the participants did not find the tool distracting , and intuitively developed ways of incorporating the feedback into their workflow .\n",
      "['continuous testing', 'continuous compilation', 'test first development', 'unit testing <eos>']\n",
      "['regression tests']\n",
      "['continuous testing', 'software testing', 'test case generation']\n",
      "1 3 4\n",
      "additional scores from absent units to present :  ['continuous testing']\n",
      "\n",
      "\n",
      "test input generation with java pathfinder . we show how model checking and symbolic execution can be used to generate test inputs to achieve structural coverage of code that manipulates complex data structures . we focus on obtaining branch coverage during unit testing of some of the core methods of the red black tree implementation in the java treemap library , using the java pathfinder model checker . three different test generation techniques will be introduced and compared , namely , straight model checking of the code , model checking used in a black box fashion to generate all inputs up to a fixed size , and lastly , model checking used during white box test input generation . the main contribution of this work is to show how efficient white box test input generation can be done for code manipulating complex data , taking into account complex method preconditions .\n",
      "['model checking', 'symbolic execution', 'coverage', 'red black trees', 'testing object oriented programs <eos>']\n",
      "['branch coverage']\n",
      "['java pathfinder', 'input generation', 'model checking', 'test generation', 'test generation', 'verification']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['model checking']\n",
      "\n",
      "\n",
      "improving the adaptability of multi mode systems via program steering . a multi mode software system contains several distinct modes of operation and a controller for deciding when to switch between modes . even when developers rigorously test a multi mode system before deployment , they can not foresee and test for every possible usage scenario . as a result , unexpected situations in which the program fails or underperforms ( for example , by choosing a non optimal mode ) may arise . this research aims to mitigate such problems by creating a new mode selector that examines the current situation , then chooses a mode that has been successful in the past , in situations like the current one . the technique , called program steering , creates a new mode selector via machine learning from good behavior in testing or in successful operation . such a strategy , which generalizes the knowledge that a programmer has built into the system , may select an appropriate mode even when the original controller can not . we have performed experiments on robot control programs written in a month long programming competition . augmenting these programs via our program steering technique had a substantial positive effect on their performance in new environments .\n",
      "['adaptability', 'multi mode systems', 'program steering', 'mode selection <eos>']\n",
      "['machine learning']\n",
      "['program steering', 'robot control', 'multi mode systems', 'program steering', 'multi mode systems']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['program steering', 'multi mode systems', 'program steering', 'multi mode systems']\n",
      "\n",
      "\n",
      "model checking xml manipulating software . the use of xml as the de facto data exchange standard has allowed integration of heterogeneous web based software systems regardless of implementation platforms and programming languages . on the other hand , the rich tree structured data representation , and the expressive xml query languages ( such as xpath ) make formal specification and verification of software systems that manipulate xml data a challenge . in this paper , we present our initial efforts in automated verification of xml data manipulation operations using the spin model checker . we present algorithms for translating ( bounded ) xml data and xpath expressions to promela , the input language of spin . the techniques presented in this paper constitute the basis of our web service analysis tool ( wsat ) which verifies ltl properties of composite web services .\n",
      "['model checking', 'xml', 'xpath', 'spin', 'promela', 'web service', 'msl', 'xml schema <eos>']\n",
      "['web services', 'xpath']\n",
      "['software systems', 'xml', 'xpath', 'model checking', 'software verification', 'xml data exchange']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['xpath', 'xml', 'xpath', 'model checking']\n",
      "\n",
      "\n",
      "correlation clustering . we consider the following clustering problem we have a complete graph on n vertices ( items ) , where each edge ( u , v ) is labeled either or depending on whether u and v have been deemed to be similar or different . the goal is to produce a partition of the vertices ( a clustering ) that agrees as much as possible with the edge labels . that is , we want a clustering that maximizes the number of edges within clusters , plus the number of edges between clusters ( equivalently , minimizes the number of disagreements the number of edges inside clusters plus the number of edges between clusters ) . this formulation is motivated from a document clustering problem in which one has a pairwise similarity function f learned from past data , and the goal is to partition the current set of documents in a way that correlates with f as much as possible semi it can also be viewed as a kind of agnostic learning problem . an interesting feature of this clustering formulation is that one does not need to specify the number of clusters k as a separate parameter , as in measures such as k median or min sum or min max clustering . instead , in our formulation , the optimal number of clusters could be any value between <digit> and n , depending on the edge labels . we look at approximation algorithms for both minimizing disagreements and for maximizing agreements . for minimizing disagreements , we give a constant factor approximation . for maximizing agreements we give a ptas , building on ideas of goldreich , goldwasser , and ron ( <digit> ) and de la veg ( <digit> ) . we also show how to extend some of these results to graphs with edge labels in <digit> , <digit> , and give some results for the case of random noise .\n",
      "['clustering', 'approximation algorithm', 'document classification <eos>']\n",
      "['agnostic learning']\n",
      "['clustering', 'approximation algorithms', 'document clustering', 'correlation clustering']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['clustering']\n",
      "\n",
      "\n",
      "accurate conjugate gradient methods for families of shifted systems . we consider the solution of the linear system real values of . this family of shifted systems arises , for example , in tikhonov regularization and computations in lattice quantum chromodynamics . for each single shift this system can be solved using the conjugate gradient method for least squares problems ( cgls ) . in literature various implementations of the , so called , multishift cgls methods have been proposed . these methods are mathematically equivalent to applying the cgls method to each shifted system separately but they solve all systems simultaneously and require only two matrix vector products ( one by a and one by at ) and two inner products per iteration step . unfortunately , numerical experiments show that , due to roundoff errors , in some cases these implementations of the multishift cgls method can only attain an accuracy that depends on the square of condition number of the matrix a . in this paper we will argue that , in the multishift cgls method , the impact on the attainable accuracy of rounding errors in the lanczos part of the method is independent of the effect of roundoff errors made in the construction of the iterates . by making suitable design choices for both parts , we derive a new ( and efficient ) implementation that tries to remove the limitation of previous proposals . a partial roundoff error analysis and various numerical experiments show promising results .\n",
      "['shifted systems', 'tikhonov regularization', 'accuracy', 'finite precision arithmetic', 'iterative methods <eos>']\n",
      "['quantum chromodynamics']\n",
      "['shifted systems', 'least squares problems', 'tikhonov regularization', 'conjugate gradient method', 'shifted linear system', 'conjugate gradient method', 'quantum computing', '<unk>']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['shifted systems', 'tikhonov regularization']\n",
      "\n",
      "\n",
      "computing smallest singular triplets with implicitly restarted lanczos bidiagonalization . a matrix free algorithm , <unk> , for the efficient computation of the smallest singular triplets of large and possibly sparse matrices is described . key characteristics of the approach are its use of lanczos bidiagonalization , implicit restarting , and harmonic ritz values . the algorithm also uses a deflation strategy that can be applied directly on lanczos bidiagonalization . a refinement postprocessing phase is applied to the converged singular vectors . the computational costs of the above techniques are kept small as they make direct use of the bidiagonal form obtained in the course of the lanczos factorization . several numerical experiments with the method are presented that illustrate its effectiveness and indicate that it performs well compared to existing codes .\n",
      "['lanczos bidiagonalization', 'implicit restarting', 'harmonic ritz values', 'deflation', 'pseudospectrum', 'refined singular vectors <eos>']\n",
      "['singular triplets']\n",
      "['lanczos bidiagonalization', 'deflation', 'smallest singular triplets', 'refinement phase', 'matrix free algorithm', 'lanczos bidiagonalization', 'singular matrix', 'matrix free methods', '<unk>']\n",
      "1 9 10\n",
      "additional scores from absent units to present :  ['lanczos bidiagonalization', 'deflation', 'lanczos bidiagonalization']\n",
      "\n",
      "\n",
      "unifying proof methodologies of duration calculus and timed linear temporal logic . linear temporal logic ( ltl ) has been widely used for specification and verification of reactive systems . its standard model is sequences of states ( or state transitions ) , and formulas describe sequencing of state transitions . when ltl is used to model real time systems , a state is extended with a time stamp to record when a state transition takes place . duration calculus ( dc ) is another well studied approach for real time systems development . dc models behaviours of a system by functions from the domain of reals representing time to the system states . this paper extends this time domain to the cartesian product of the real and the natural numbers . with the extended time domain , we provide the chop modality with a non overlapping interpretation . this allows some linear temporal operators explicitly dealing with the discrete dimension of time to be derivable from the chop modality in essentially the same way that their continuous time counterparts are in the classical dc . this provides a nice embedding of some timed ltl ( tltl ) modalities into dc to unify the methods from dc and ltl for real time systems development requirements and high level design decisions are interval properties and are therefore specified and reasoned about in dc , while properties of an implementation , as well as the refinement relation between two implementations , are specified and verified compositionally and inductively in ltl . implementation properties are related to requirement and design properties by rules for lifting ltl formulas to dc formulas .\n",
      "['specification', 'verification', 'real time', 'design', 'refinement <eos>']\n",
      "['linear temporal logic', 'real time systems']\n",
      "['timed linear temporal logic', 'verification', 'duration calculus', 'state transition', 'timed systems', 'state transition systems', 'formal methods', 'real time systems', 'verification of systems']\n",
      "2 9 11\n",
      "additional scores from absent units to present :  ['verification']\n",
      "\n",
      "\n",
      "tight bounds on the competitive ratio on accommodating sequences for the seat reservation problem . the unit price seat reservation problem is investigated . the seat reservation problem is the problem of assigning seat numbers on line to requests for reservations in a train traveling through k stations . we are considering the version where all tickets have the same price and where requests are treated fairly , that is , a request which can be fulfilled must be granted . for fair deterministic algorithms , we provide an asymptotically matching upper bound to the existing lower bound which states that all fair algorithms for this problem are competitive on accommodating sequences , when there are at least three seats . additionally , we give an asymptotic upper bound of <digit> <digit> for fair randomized algorithms against oblivious adversaries . we also examine concrete on line algorithms , first fit and random for the special case of two seats . tight analyses of their performance are given .\n",
      "['competitive ratio', 'seat reservation problem', 'on line algorithms', 'accomodating sequences <eos>']\n",
      "['Present predictions:']\n",
      "['seat reservation problem', 'unit price seat reservation problem', 'competitive ratio', 'accommodating sequences']\n",
      "1 4 4\n",
      "additional scores from absent units to present :  ['seat reservation problem', 'competitive ratio']\n",
      "\n",
      "\n",
      "admission control with immediate notification . when admission control is used , an on line scheduler chooses whether or not to complete each individual job successfully by its deadline . an important consideration is at what point in time the scheduler determines if a job request will be satisfied , and thus at what point the scheduler is able to provide notification to the job owner as to the fate of the request . in the loosest model , often seen in real time systems , such a decision can be deferred up until the job ' s deadline passes . in the strictest model , more suitable for customer based applications , a scheduler would be required to give notification at the instant that a job request arrives . unfortunately there seems to be little existing research which explicitly studies the effect of the notification model on the performance guarantees of a scheduler . we undertake such a study by reexamining a problem from the literature . specifically , we study the effect of the notification model on the non preemptive scheduling of a single resource in order to maximize utilization . at first glance , it appears severely more restrictive to compare a scheduler required to give immediate notification to one which need not give any notification . yet we are able to present alternate algorithms which provide immediate notification , while matching most of the performance guarantees which are possible by schedulers which provide no such notification . in only one case are we able to give evidence that providing immediate notification may be more difficult .\n",
      "['admission control', 'notification', 'online scheduling', 'firm deadlines <eos>']\n",
      "['Present predictions:']\n",
      "['scheduling', 'notification', 'admission control']\n",
      "1 3 3\n",
      "additional scores from absent units to present :  ['notification', 'admission control']\n",
      "\n",
      "\n",
      "distributed council election . this paper studies the problem of electing a small number of representatives ( council ) out of a ( possible large ) group of anonymous candidates . the problem arises in scenarios such as multicast where , to avoid feedback implosion , a small subset of the receivers is chosen to provide feedback on network conditions . we present several algorithms for this problem and analyze the expected number of messages and rounds required for their convergence . in particular , we present an algorithm that almost always converges in one round using a small number of messages ( for typical council size ) when the number of hosts is known . in the case where the number of hosts is unknown ( and too large to be polled ) , our algorithms converge in a small number of rounds that improves previous results by bolot et al . ( <digit> ) .\n",
      "['multicast', 'leader election <eos>']\n",
      "['council election']\n",
      "['multicast', 'distributed algorithms', 'anonymous networks', 'distributed algorithms']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['multicast']\n",
      "\n",
      "\n",
      "rsa oaep is secure under the rsa assumption . recently victor shoup noted that there is a gap in the widely believed security result of oaep against adaptive chosen ciphertext attacks . moreover , he showed that , presumably , oaep can not be proven secure from the one wayness of the underlying trapdoor permutation . this paper establishes another result on the security of oaep . it proves that oaep offers semantic security against adaptive chosen ciphertext attacks , in the random oracle model , under the partial domain one wayness of the underlying permutation . therefore , this uses a formally stronger assumption . nevertheless , since partial domain one wayness of the rsa function is equivalent to its ( full domain ) <unk> , it follows that the security of rsa oaep can actually be proven under the sole rsa assumption , although the reduction is not tight .\n",
      "['rsa', 'oaep', 'provable security', 'public key encryption <eos>']\n",
      "['semantic security', 'rsa', 'chosen ciphertext attack']\n",
      "['rsa assumption', 'oaep', 'security', 'random oracle model', 'adaptive chosen ciphertext attack', 'rsa oaep']\n",
      "3 6 9\n",
      "additional scores from absent units to present :  ['oaep']\n",
      "\n",
      "\n",
      "corpus structure , language models , and ad hoc information retrieval . most previous work on the recently developed language modeling approach to information retrieval focuses on document specific characteristics , and therefore does not take into account the structure of the surrounding corpus . we propose a novel algorithmic framework in which information provided by document based language models is enhanced by the incorporation of information drawn from clusters of similar documents . using this framework , we develop a suite of new algorithms . even the simplest typically outperforms the standard language modeling approach in precision and recall , and our new interpolation algorithm posts statistically significant improvements for both metrics over all three corpora tested .\n",
      "['language modeling', 'clustering', 'smoothing', 'aspect models', 'interpolation model', 'cluster based language models <eos>']\n",
      "['Present predictions:']\n",
      "['information retrieval', 'language modeling', 'ad hoc information retrieval', 'corpus structure', 'information retrieval', 'corpus structure', 'information retrieval']\n",
      "1 7 7\n",
      "additional scores from absent units to present :  ['language modeling']\n",
      "\n",
      "\n",
      "solving equations in the relational algebra . enumerating all solutions of a relational algebra equation is a natural and powerful operation which , when added as a query language primitive to the nested relational algebra , yields a query language for nested relational databases , equivalent to the well known powerset algebra . we study sparse equations , which are equations with at most polynomially many solutions . we look at their complexity and compare their expressive power with that of similar notions in the powerset algebra .\n",
      "['equation', 'relational algebra', 'nested relation', 'parity', \"fagin ' s theorem\", 'sparse expression <eos>']\n",
      "['sparse equations', 'query language', 'databases']\n",
      "['databases', 'nested relational algebra', 'query languages', 'relational algebra']\n",
      "3 4 7\n",
      "additional scores from absent units to present :  ['relational algebra']\n",
      "\n",
      "\n",
      "development of a verified erlang program for resource locking . in this paper , we describe a tool to verify erlang programs and show , by means of an industrial case study , how this tool is used . the tool includes a number of components , including a translation component , a state space generation component and a model checking component . to verify properties of the code , the tool first translates the erlang code into a process algebraic specification . the outcome of the translation is made more efficient by taking advantage of the fact that software written in erlang builds upon software design patterns such as clientserver behaviours . a labelled transition system is constructed from the specification by use of the crl toolset . the resulting labelled transition system is model checked against a set of properties formulated in the calculus using the caesar <unk> toolset . as a case study we focus on a simplified resource manager modelled on a real implementation in the control software of the axd <digit> atm switch . some of the key properties we verified for the program are mutual exclusion and non starvation . since the toolset supports only the regular alternation free calculus , some ingenuity is needed for checking the liveness property non starvation . the case study has been refined step by step to provide more functionality , with each step motivated by a corresponding formal verification using model checking .\n",
      "['erlang', 'model checking', 'formal methods', 'functional programming', 'software verification <eos>']\n",
      "['mutual exclusion', 'formal verification', 'crl']\n",
      "['resource locking', 'erlang', 'labelled transition system', 'model checking', 'erlang and toolset', 'verification']\n",
      "3 6 9\n",
      "additional scores from absent units to present :  ['erlang', 'model checking']\n",
      "\n",
      "\n",
      "poset valued sets or how to build models for linear logics . we describe a method for constructing models of linear logic based on the category of sets and relations . the resulting categories are non degenerate in general in particular they are not compact closed nor do they have biproducts . the construction is simple , lifting the structure of a poset to the new category . the underlying poset thus controls the structure of this category , and different posets give rise to differently flavoured models . as a result , this technique allows the construction of models for both , intuitionistic or classical linear logic as desired . a number of well known models , for example coherence spaces and hypercoherences , are instances of this method .\n",
      "['linear logic', 'categorical models <eos>']\n",
      "['coherence spaces']\n",
      "['linear logic', 'poset', 'poset valued sets', 'poset valued logic', '<unk>']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['linear logic']\n",
      "\n",
      "\n",
      "optimistic register coalescing . graph coloring register allocators eliminate copies by coalescing the source and target nodes of a copy if they do not interfere in the interference graph . coalescing , however , can be harmful to the colorability of the graph because it tends to yield a graph with nodes of higher degrees . unlike aggressive coalescing , which coalesces any pair of noninterfering copy related nodes , conservative coalescing or iterated coalescing perform safe coalescing that preserves the colorability . unfortunately , these heuristics give up coalescing too early , losing many opportunities for coalescing that would turn out to be safe . moreover , they ignore the fact that coalescing may even improve the colorability of the graph by reducing the degree of neighbor nodes that are interfering with both the source and target nodes being coalesced . this article proposes a new heuristic called optimistic coalescing which optimistically performs aggressive coalescing , thus exploiting the positive impact of coalescing aggressively , but when a coalesced node is to be spilled , it is split back into separate nodes . since there is a better chance of coloring one of those splits , we can reduce the overall spill amount .\n",
      "['graph coloring', 'register allocation', 'noncopy coalescing', 'copy coalescing <eos>']\n",
      "['register coalescing', 'algorithms']\n",
      "['coalescing', 'algorithms', 'graph coloring', 'optimistic coalescing', 'algorithms', 'theory', 'graph algorithms', 'algorithms']\n",
      "2 8 10\n",
      "additional scores from absent units to present :  ['graph coloring']\n",
      "\n",
      "\n",
      "computation in networks of passively mobile finite state sensors . we explore the computational power of networks of small resource limited mobile agents . we define two new models of computation based on pairwise interactions of finite state agents in populations of finite but unbounded size . with a fairness condition on interactions , we define the concept of stable computation of a function or predicate , and give protocols that stably compute functions in a class including boolean combinations of threshold k , parity , majority , and simple arithmetic . we prove that all stably computable predicates are in nl . with uniform random sampling of pairs to interact , we define the model of conjugating automata and show that any counter machine with o ( <digit> ) counters of capacity o ( n ) can be simulated with high probability by a protocol in a population of size n . we prove that all predicates computable with high probability in this model are in p rl . several open problems and promising future directions are discussed .\n",
      "['mobile agent', 'finite state agent', 'stable computation', 'sensor net', 'diffuse computation', 'intermittent communication <eos>']\n",
      "['mobile agents']\n",
      "['fairness', 'stable computation', 'mobile computing', 'mobile computing']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['stable computation']\n",
      "\n",
      "\n",
      "asynchronous group key exchange with failures . group key exchange protocols allow a group of servers communicating over an asynchronous network of point to point links to establish a common key , such that an adversary which fully controls the network links ( but not the group members ) can not learn the key . currently known group key exchange protocols rely on the assumption that all group members participate in the protocol and if a single server crashes , then no server may terminate the protocol . in this paper , we propose the first purely asynchronous group key exchange protocol that tolerates a minority of servers to crash . our solution uses a constant number of rounds , which makes it suitable for use in practice . furthermore , we also investigate how to provide forward secrecy with respect to an adversary that may break into some servers and observe their internal state . we show that any group key exchange protocol among n servers that tolerates tc > <digit> servers to crash can only provide forward secrecy if the adversary breaks into less than n <unk> servers , and propose a group key exchange protocol that achieves this bound .\n",
      "['group key exchange', 'provable security', 'group communication', 'universal composability <eos>']\n",
      "['Present predictions:']\n",
      "['group key exchange', 'forward secrecy', 'key exchange', 'group key exchange protocols', 'asynchronous communication']\n",
      "1 5 5\n",
      "additional scores from absent units to present :  ['group key exchange']\n",
      "\n",
      "\n",
      "a linear algebraic approach to metering schemes . a metering scheme is a method by which an audit agency is able to measure the interaction between servers and clients during a certain number of time frames . naor and pinkas ( vol . <digit> of lncs , pp . <digit> ) proposed metering schemes where any server is able to compute a proof ( i . e . , a value to be shown to the audit agency at the end of each time frame ) , if and only if it has been visited by a number of clients larger than or equal to some threshold h during the time frame . masucci and stinson ( vol . <digit> of lncs , pp . <digit> ) showed how to construct a metering scheme realizing any access structure , where the access structure is the family of all subsets of clients which enable a server to compute its proof . they also provided lower bounds on the communication complexity of metering schemes . in this paper we describe a linear algebraic approach to design metering schemes realizing any access structure . namely , given any access structure , we present a method to construct a metering scheme realizing it from any linear secret sharing scheme with the same access structure . besides , we prove some properties about the relationship between metering schemes and secret sharing schemes . these properties provide some new bounds on the information distributed to clients and servers in a metering scheme . according to these bounds , the optimality of the metering schemes obtained by our method relies upon the optimality of the linear secret sharing schemes for the given access structure .\n",
      "['metering', 'secret sharing', 'distributed audit', 'entropy', 'cryptography <eos>']\n",
      "['access structure']\n",
      "['metering schemes', 'secret sharing', 'linear algebraic approach', 'linear algebraic equations', 'cryptography']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['secret sharing']\n",
      "\n",
      "\n",
      "finite state machines for strings over infinite alphabets . motivated by formal models recently proposed in the context of xml , we study automata and logics on strings over infinite alphabets . these are conservative extensions of classical automata and logics defining the regular languages on finite alphabets . specifically , we consider register and pebble automata , and extensions of first order logic and monadic second order logic . for each type of automaton we consider one way and two way variants , as well as deterministic , nondeterministic , and alternating control . we investigate the expressiveness and complexity of the automata and their connection to the logics , as well as standard decision problems . some of our results answer open questions of kaminski and francez on register automata .\n",
      "['infinite alphabets', 'xml', 'automata', 'registers', 'pebbles', 'first order logic', 'monadic second order logic', 'expressiveness <eos>']\n",
      "['monadic second order logic', 'formal models']\n",
      "['infinite alphabets', 'xml', 'automata', 'first order logic', 'finite state machines', 'finite state automata', 'formal languages']\n",
      "2 7 9\n",
      "additional scores from absent units to present :  ['infinite alphabets', 'xml', 'automata', 'first order logic']\n",
      "\n",
      "\n",
      "reflective metalogical frameworks . a metalogical framework is a logic with an associated methodology that is used to represent other logics and to reason about their metalogical properties . we propose that logical frameworks can be good metalogical frameworks when their theories always have initial models and they support reflective and parameterized reasoning . we develop this thesis both abstractly and concretely . abstractly , we formalize our proposal as a set of requirements and explain how any logic satisfying these requirements can be used for metalogical reasoning . concretely , we present membership equational logic as a particular metalogic that satisfies these requirements . using membership equational logic , and its realization in the maude system , we show how reflection can be used for different , nontrivial kinds of formal metatheoretic reasoning . in particular , one can prove metatheorems that relate theories or establish properties of parameterized classes of theories .\n",
      "['reflection', 'metalogics', 'membership equational logic', 'rewriting logic <eos>']\n",
      "['equational logic', 'metalogical frameworks']\n",
      "['metalogical', 'maude', 'membership equational logic', 'theory', 'formal methods']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['membership equational logic']\n",
      "\n",
      "\n",
      "relating communicating processes with different interfaces . we present here an implementation relation intended to formalise the notion that a system built of communicating processes is an acceptable implementation of another base , or target , system in the event that the two systems have different interfaces . such a treatment has clear applicability in the software development process , where ( the interface of ) an implementation component may be expressed at a different level of abstraction to ( the interface of ) the relevant specification component . technically , processes are formalised using hoare ' s csp language , with its standard failures divergences model . the implementation relation is formulated in terms of failures and divergences of the implementation and target processes . interface difference is modelled by endowing the implementation relation with parameters called extraction patterns . these are intended to interpret implementation behaviour as target behaviour , and suitably constrain the former in connection to well formedness and deadlock properties . we extend the results of our previous work and replace implementation relations previously presented by a single , improved scheme . we also remove all the restrictions previously placed upon target processes . two basic kinds of results are obtained realisability and compositionality . the latter means that a target composed of several connected systems may be implemented by connecting their respective implementations . the former means that , if target and implementation in fact have the same interface , then the implementation relation they should satisfy collapses into standard implementation pre order . we also show how to represent processes and extraction patterns in a manner amenable to computer implementation , and detail a graph theoretic restatement of the conditions defining the implementation relation , from which algorithms for their automatic verification are easily derived .\n",
      "['compositionality', 'verification', 'theory of parallel and distributed computation', 'refinement', 'communicating sequential processes', 'behaviour abstraction <eos>']\n",
      "['deadlock', 'verification']\n",
      "['interface', 'realisability', 'communicating processes', 'interface verification', 'software process', 'process calculi', 'formal methods', 'verification']\n",
      "2 8 10\n",
      "additional scores from absent units to present :  ['verification', 'verification']\n",
      "\n",
      "\n",
      "minimization and np multifunctions . the implicit characterizations of the polynomial time computable functions fp given by bellantoni cook and leivant suggest that this class is the complexity theoretic analog of the primitive recursive functions . hence , it is natural to add minimization operators to these characterizations and investigate the resulting class of partial functions as a candidate for the analog of the partial recursive functions . we do so in this paper for cobham ' s definition of fp by bounded recursion and for bellantoni cook ' s safe recursion and prove that the resulting classes capture exactly npmv , the nondeterministic polynomial time computable partial multifunctions . we also consider the relationship between our schemes and a notion of nondeterministic recursion defined by leivant and show that the latter characterizes the total functions of npmv . we view these results as giving evidence that npmv is the appropriate analog of partial recursive . this view is reinforced by earlier results of <unk> and stahl who show that for many of the relationships between partial recursive functions and r . e . sets , analogous relationships hold between npmv and np sets . furthermore , since npmv is obtained from fp in the same way as the recursive functions are obtained from the primitive recursive functions ( when defined via function schemes ) , this also gives further evidence that fp is properly seen as playing the role of primitive recursion .\n",
      "['minimization', 'safe recursion', 'implicit computational complexity', 'non deterministic partial multifunctions <eos>']\n",
      "['partial recursive functions']\n",
      "['minimization', 'polynomial', 'np multifunctions', 'np completeness', 'analog computation', '<unk>']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['minimization']\n",
      "\n",
      "\n",
      "electronic jury voting protocols . this work stresses the fact that all current proposals for electronic voting schemes disclose the final tally of the votes . in certain situations , like jury voting , this may be undesirable . we present a robust and universally verifiable membership testing scheme ( mts ) that allows , among other things , a collection of voters to cast votes and determine whether their tally belongs to some pre specified small set ( e . g . , exceeds a given threshold ) our scheme discloses no additional information than that implied from the knowledge of such membership . we discuss several extensions of our basic mts . all the constructions presented combine features of two parallel lines of research concerning electronic voting schemes , those based on mix networks and in homomorphic encryption .\n",
      "['membership testing', 'mix networks', 'majority voting', 'election scheme <eos>']\n",
      "['homomorphic encryption', 'jury voting']\n",
      "['mix networks', 'voting schemes', 'electronic voting', 'electronic voting', 'parallel computing', 'jury voting']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['mix networks']\n",
      "\n",
      "\n",
      "symmetry groups for beta lattices . we present a construction of symmetry plane groups for quasiperiodic point sets named beta lattices . the framework is issued from beta integers counting systems . beta lattices are vector superpositions of beta integers . when > <digit> is a quadratic pisot <unk> algebraic unit , the set of beta integers can be equipped with an abelian group structure and an internal multiplicative law . when we show that these arithmetic and algebraic structures lead to freely generated symmetry plane groups for beta lattices . these plane groups are based on repetitions of discrete adapted rotations and translations we shall refer to as beta rotations and beta translations . hence beta lattices , endowed with beta rotations and beta translations , can be viewed like lattices . the quasiperiodic function s ( n ) , defined on the set of beta integers as counting the number of small tiles between the origin and the nth beta integer , plays a central part in these new group structures . in particular , this function behaves asymptotically like a linear function . as an interesting consequence , beta lattices and their symmetries behave asymptotically like lattices and lattice symmetries , respectively .\n",
      "['beta lattices', 'plane groups', 'tilings', 'pisot numbers', 'quasicrystals <eos>']\n",
      "['quasiperiodic function']\n",
      "['beta lattices', 'pisot plane', 'symmetry plane group', 'beta beta lattices', '<unk>']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['beta lattices']\n",
      "\n",
      "\n",
      "mining the space of graph properties . existing data mining algorithms on graphs look for nodes satisfying specific properties , such as specific notions of structural similarity or specific measures of link based importance . while such analyses for predetermined properties can be effective in well understood domains , sometimes identifying an appropriate property for analysis can be a challenge , and focusing on a single property may neglect other important aspects of the data . in this paper , we develop a foundation for mining the properties themselves . we present a theoretical framework defining the space of graph properties , a variety of mining queries enabled by the framework , techniques to handle the enormous size of the query space , and an experimental system called f miner that demonstrates the utility and feasibility of property mining .\n",
      "['data mining', 'graph mining <eos>']\n",
      "['property mining']\n",
      "['data mining', 'graph properties', 'graph mining', 'data mining']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['data mining', 'data mining']\n",
      "\n",
      "\n",
      "fast , distributed approximation algorithms for positive linear programming with applications to flow control . we study combinatorial optimization problems in which a set of distributed agents must achieve a global objective using only local information . papadimitriou and yannakakis proceedings of the 25th acm symposium on theory of computing , <digit> , pp . <digit> <digit> initiated the study of such problems in a framework where distributed decision makers must generate feasible solutions to positive linear programs with information only about local constraints . we extend their model by allowing these distributed decision makers to perform local communication to acquire information over time and then explore the tradeoff between the amount of communication and the quality of the solution to the linear program that the decision makers can obtain . our main result is a distributed algorithm that obtains a ( <digit> approximation to the optimal linear programming solution while using only a polylogarithmic number of rounds of local communication . this algorithm offers a significant improvement over the logarithmic approximation ratio previously obtained by awerbuch and azar proceedings of the 35th annual ieee symposium on foundations of computer science , <digit> , pp . <digit> <digit> for this problem while providing a comparable running time . our results apply directly to the application of network flow control , an application in which distributed routers must quickly choose how to allocate bandwidth to connections using only local information to achieve global objectives . the sequential version of our algorithm is faster and considerably simpler than the best known approximation algorithms capable of achieving a ( <digit> approximation ratio for positive linear programming .\n",
      "['approximation algorithm', 'linear programming', 'flow control', 'primal dual <eos>']\n",
      "['linear programming', 'approximation algorithms']\n",
      "['flow control', 'combinatorial optimization', 'positive linear programming', 'distributed approximation algorithms', 'distributed algorithms']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['flow control']\n",
      "\n",
      "\n",
      "splitting a matrix of laurent polynomials with symmetry and its application to symmetric framelet filter banks . let m be a <digit> times <digit> matrix of laurent polynomials with real coefficients and symmetry . in this paper , we obtain a necessary and sufficient condition for the existence of four laurent polynomials ( or finite impulse response filters ) u1 , u2 , v1 , v2 with real coefficients and symmetry such that left begin matrix u <digit> ( z ) v <digit> ( z ) u <digit> ( z ) v <digit> ( z ) end matrix right left begin matrix u <digit> ( <digit> z ) u <digit> ( <digit> z ) v <digit> ( <digit> z ) v <digit> ( <digit> z ) end matrix right m ( z ) qquad forall z in cc bs <digit> and <unk> ( z ) sv2 ( z ) <unk> ( z ) sv1 ( z ) , where sp ( z ) p ( z ) p ( <digit> z ) for a nonzero laurent polynomial p . our criterion can be easily checked and a step by step algorithm will be given to construct the symmetric filters u1 , u2 , v1 , v2 . as an application of this result to symmetric framelet filter banks , we present a necessary and sufficient condition for the construction of a symmetric multiresolution analysis tight wavelet frame with two compactly supported generators derived from a given symmetric refinable function . once such a necessary and sufficient condition is satisfied , an algorithm will be used to construct a symmetric framelet filter bank with two high pass filters which is of interest in applications such as signal denoising and image processing . as an illustration of our results and algorithms in this paper , we give several examples of symmetric framelet filter banks with two high pass filters which have good vanishing moments and are derived from various symmetric low pass filters including some b spline filters .\n",
      "['symmetry', 'framelet filter banks', 'tight wavelet frames', 'refinable functions', 'low pass and high pass filters', 'matrix splitting <eos>']\n",
      "['framelet filter banks', 'multiresolution analysis', 'framelet']\n",
      "['symmetric filter banks', 'symmetry', 'laurent polynomials', 'symmetric filter banks', 'laurent polynomial', 'matrix splitting']\n",
      "3 6 9\n",
      "additional scores from absent units to present :  ['symmetry']\n",
      "\n",
      "\n",
      "inference of termination conditions for numerical loops in prolog . we present a new approach to termination analysis of numerical computations in logic programs . traditional approaches fail to analyse them due to non well foundedness of the integers . we present a technique that allows overcoming these difficulties . our approach is based on transforming a program in a way that allows integrating and extending techniques originally developed for analysis of numerical computations in the framework of query mapping pairs with the well known framework of acceptability . such an integration not only contributes to the understanding of termination behaviour of numerical computations , but also allows us to perform a correct analysis of such computations automatically , by extending previous work on a constraint based approach to termination . finally , we discuss possible extensions of the technique , including incorporating general term orderings .\n",
      "['termination analysis', 'numerical computation <eos>']\n",
      "['logic programming']\n",
      "['prolog', 'inference', 'numerical computations', 'query mapping', 'termination analysis', 'constraint solving', 'program analysis', 'verification']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['termination analysis']\n",
      "\n",
      "\n",
      "probabilistic symbolic model checking with prism a hybrid approach . in this paper we present efficient symbolic techniques for probabilistic model checking . these have been implemented in prism , a tool for the analysis of probabilistic models such as discrete time markov chains , continuous time markov chains and markov decision processes using specifications in the probabilistic temporal logics pctl and csl . motivated by the success of model checkers such as smv which use bdds ( binary decision diagrams ) , we have developed an implementation of pctl and csl model checking based on mtbdds ( multi terminal bdds ) and bdds . existing work in this direction has been hindered by the generally poor performance of mtbdd based numerical computation , which is often substantially slower than explicit methods using sparse matrices . the focus of this paper is a novel hybrid technique which combines aspects of symbolic and explicit approaches to overcome these performance problems . for typical examples , we achieve a dramatic improvement over the purely symbolic approach . in addition , thanks to the compact model representation using mtbdds , we can verify systems an order of magnitude larger than with sparse matrices , while almost matching or even beating them for speed .\n",
      "['symbolic model checking', 'probabilistic model checking', 'binary decision diagrams <eos>']\n",
      "['markov decision process']\n",
      "['hybrid technique', 'prism', 'continuous time markov chains', 'model checking', 'symbolic model checking', 'probabilistic model checking', 'hybrid systems', 'probabilistic model checking']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['symbolic model checking', 'probabilistic model checking', 'probabilistic model checking']\n",
      "\n",
      "\n",
      "test sequence generation and model checking using dynamic transition relations . the task of finding a set of test sequences that provides good coverage of industrial circuits is infeasible because of the size of the circuits . for small critical subcircuits of the design , however , designers can create a set of test sequences that achieve good coverage . these sequences can not be used on the full design because the inputs to the subcircuit may not be accessible . in this work we present an efficient test generation algorithm that receives a test sequence created for the subcircuit and finds a test sequence for the full design that reproduces the given sequence on the subcircuit . the algorithm uses a new technique called dynamic transition relations to increase its efficiency . the most common and most expensive step in our algorithm is the computation of the set of predecessors of a set of states . to make this computation more efficient we exploit a partitioning of the transition relation into a set of simpler relations . at every step we use only those that are necessary , resulting in a smaller relation than the original one . a different relation is used for each step , hence the name dynamic transition relations . the same idea can be used to improve symbolic model checking for the temporal logic ctl . we have implemented the new method in smv and run it on several large circuits . our experiments indicate that the new method can provide gains of up to two orders of magnitude in time and space during verification . these results show that dynamic transition relations can make it possible to verify circuits that were previously unmanageable due to their size and complexity .\n",
      "['test sequence generation', 'symbolic model checking', 'binary decision diagrams <eos>']\n",
      "['temporal logic']\n",
      "['dynamic transition relations', 'model checking', 'transition relations', 'test sequence generation', 'dynamic transition relations', 'test sequence generation']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['test sequence generation', 'test sequence generation']\n",
      "\n",
      "\n",
      "on the performance of group key agreement protocols . group key agreement is a fundamental building block for secure peer group communication systems . several group key management techniques were proposed in the last decade , all assuming the existence of an underlying group communication infrastructure to provide reliable and ordered message delivery as well as group membership information . despite analysis , implementation , and deployment of some of these techniques , the actual costs associated with group key management have been poorly understood so far . this resulted in an undesirable tendency on the one hand , adopting suboptimal security for reliable group communication , while , on the other hand , constructing excessively costly group key management protocols . this paper presents a thorough performance evaluation of five notable distributed key management techniques ( for collaborative peer groups ) integrated with a reliable group communication system . an in depth comparison and analysis of the five techniques is presented based on experimental results obtained in actual local and wide area networks . the extensive performance measurement experiments conducted for all methods offer insights into their scalability and practicality . furthermore , our analysis of the experimental results highlights several observations that are not obvious from the theoretical analysis .\n",
      "['peer groups', 'group communication', 'group key management', 'secure communication <eos>']\n",
      "['peer integrated', 'key agreement', 'distributed key management']\n",
      "['performance evaluation', 'group key management', 'group key management', 'peer to peer networks', 'distributed systems']\n",
      "3 5 8\n",
      "additional scores from absent units to present :  ['group key management', 'group key management']\n",
      "\n",
      "\n",
      "sizing router buffers . all internet routers contain buffers to hold packets during times of congestion . today , the size of the buffers is determined by the dynamics of tcp ' s congestion control algorithm . in particular , the goal is to make sure that when a link is congested , it is busy <digit> % of the time which is equivalent to making sure its buffer never goes empty . a widely used rule of thumb states that each link needs a buffer of size is the average round trip time of a flow passing across the link , and c is the data rate of the link . for example , a 10gb s router linecard needs approximately 250ms x <digit> . 5gbits of buffers and the amount of buffering grows linearly with the line rate . such large buffers are challenging for router manufacturers , who must use large , slow , off chip drams . and queueing delays can be long , have high variance , and may destabilize the congestion control algorithms . in this paper we argue that the rule of thumb now outdated and incorrect for backbone routers . this is because of the large number of flows ( tcp connections ) multiplexed together on a single backbone link . using theory , simulation and experiments on a network of real routers , we show that a link with n flows requires no more than long lived or short lived tcp flows . the consequences on router design are enormous a <digit> . 5gb s link carrying <digit> , <digit> flows could reduce its buffers by <digit> % with negligible difference in throughput and a 10gb s link carrying <digit> , <digit> flows requires only 10mbits of buffering , which can easily be implemented using fast , on chip sram .\n",
      "['internet router', 'tcp', 'bandwidth delay product', 'buffer size <eos>']\n",
      "['congestion control', 'router buffers']\n",
      "['tcp', 'network theory', 'network on chip', 'router architecture', 'performance evaluation']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['tcp']\n",
      "\n",
      "\n",
      "a layered naming architecture for the internet . currently the internet has only one level of name resolution , dns , which converts user level domain names into ip addresses . in this paper we borrow liberally from the literature to argue that there should be three levels of name resolution from user level descriptors to service identifiers from service identifiers to endpoint identifiers and from endpoint identifiers to ip addresses . these additional levels of naming and resolution ( <digit> ) allow services and data to be first class internet objects ( in that they can be directly and persistently named ) , ( <digit> ) seamlessly accommodate mobility and multi homing and ( <digit> ) integrate middleboxes ( such as nats and firewalls ) into the internet architecture . we further argue that flat names are a natural choice for the service and endpoint identifiers . hence , this architecture requires scalable resolution of flat names , a capability that distributed hash tables ( dhts ) can provide .\n",
      "['naming', 'name resolution', 'middleboxes', 'internet architecture', 'distributed hash tables', 'global identifiers <eos>']\n",
      "['naming architecture', 'internet', 'distributed hash tables']\n",
      "['internet', 'dns', 'naming', 'naming', 'internet security']\n",
      "3 5 8\n",
      "additional scores from absent units to present :  ['naming', 'naming']\n",
      "\n",
      "\n",
      "style based inverse kinematics . this paper presents an inverse kinematics system based on a learned model of human poses . given a set of constraints , our system can produce the most likely pose satisfying those constraints , in real time . training the model on different input data leads to different styles of ik . the model is represented as a probability distribution over the space of all possible poses . this means that our ik system can generate any pose , but prefers poses that are most similar to the space of poses in the training data . we represent the probability with a novel model called a scaled gaussian process latent variable model . the parameters of the model are all learned automatically no manual tuning is required for the learning component of the system . we additionally describe a novel procedure for interpolating between styles . our style based ik can replace conventional ik , wherever it is used in computer animation and computer vision . we demonstrate our system in the context of a number of applications interactive character posing , trajectory keyframing , real time motion capture with missing markers , and posing from a 2d image .\n",
      "['inverse kinematics', 'gaussian processes', 'non linear dimensionality reduction', 'character animation', 'motion style', 'machine learning', 'style interpolation <eos>']\n",
      "['latent variable model']\n",
      "['inverse kinematics', 'interactive animation']\n",
      "1 2 3\n",
      "additional scores from absent units to present :  ['inverse kinematics']\n",
      "\n",
      "\n",
      "parametric search made practical . in this paper we show that in sorting based applications of parametric search , quicksort can replace the parallel sorting algorithms that are usually advocated . because of the simplicity of quicksort , this may lead to applications of parametric search that are not only efficient in theory , but in practice as well . also , we argue that cole ' s optimization of certain parametric search algorithms may be unnecessary under realistic assumptions about the input . furthermore , we present a generic , flexible , and easy to use framework that greatly simplifies the implementation of algorithms based on parametric search . we use our framework to implement an algorithm that solves the <unk> distance problem . the implementation based on parametric search is faster than the binary search approach that is often suggested as a practical replacement for the parametric search technique .\n",
      "['parametric search', 'implementation <eos>']\n",
      "['parallel sorting', 'sorting']\n",
      "['quicksort', 'parametric search', 'parallel algorithms']\n",
      "2 3 5\n",
      "additional scores from absent units to present :  ['parametric search']\n",
      "\n",
      "\n",
      "minimizing end to end delay in high speed networks with a simple coordinated schedule . we study the problem of providing end to end delay guarantees in connection oriented networks . in this environment , multiple hop sessions coexist and interfere with one another . parekh and gallager showed that the weighted fair queueing ( wfq ) scheduling discipline provides a worst case delay guarantee comparable to ( <digit> i ) ki for a session with rate i and ki hops . such delays can occur since a session i packet can wait for time <digit> i at every hop . we describe a randomized work conserving scheme that guarantees , with high probability , an additive delay bound of approximately <digit> i ki . this bound is smaller than the multiplicative bound ( <digit> i ) ki of wfq , especially when the hop count ki is large . we call our scheme coordinated earliest deadline first ( cedf ) since it uses an earliest deadline first approach in which simple coordination is applied to the deadlines for consecutive hops of a session . the key to the bound is that once a packet has passed through its first server , it can pass through all its subsequent servers quickly . we conduct simulations to compare the delays actually produced by the two scheduling disciplines . in many cases , these actual delays are comparable to their analytical worst case bounds , implying that cedf outperforms wfq .\n",
      "['scheduling', 'weighted fair queueing', 'delay bounds', 'earliest deadline first', 'packet routing <eos>']\n",
      "['high speed networks']\n",
      "['scheduling', 'connection oriented networks', 'end to end delay', 'quality of service', 'weighted fair queueing', 'end to end delay', 'high speed networks', 'performance evaluation']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['scheduling', 'weighted fair queueing']\n",
      "\n",
      "\n",
      "testing juntas . we show that a boolean valued function over n variables , where each variable ranges in an arbitrary probability space , can be tested for the property of depending on only j of them using a number of queries that depends only polynomially on j and the approximation parameter . we present several tests that require a number of queries that is polynomial in j and linear in <digit> . we <unk> non adaptive tests that has one sided error , an adaptive version of it that requires fewer queries , and a non adaptive two sided version of the test that requires the least number of queries . we also show a two sided non adaptive test that applies to functions over n boolean variables , and has a more compact analysis . we then provide a lower bound of ( j ) on the number of queries required for the nonadaptive testing of the above property a lower bound of ( log ( j <digit> ) ) for adaptive algorithms naturally follows from this . in establishing this lower bound we also prove a result about random walks on the group <unk> that may be interesting in its own right . we show that for some the distributions of the random walk at times t and t are close to each other , independently of the step distribution of the walk . we also discuss related questions . in particular , when given in advance a known j junta function h , we show how to test a function f for the property of being identical to h up to a permutation of the variables , in a number of queries that is polynomial in j and <digit> .\n",
      "['juntas', 'discrete fourier analysis', 'property testing', 'boolean functions <eos>']\n",
      "['adaptive algorithms']\n",
      "['juntas', 'random walk', 'boolean functions']\n",
      "1 3 4\n",
      "additional scores from absent units to present :  ['juntas']\n",
      "\n",
      "\n",
      "improving the static analysis of embedded languages via partial evaluation . programs in embedded languages contain invariants that are not automatically detected or enforced by their host language . we show how to use macros to easily implement partial evaluation of embedded interpreters in order to capture invariants encoded in embedded programs and render them explicit in the terms of their host language . we demonstrate the effectiveness of this technique in improving the results of a value flow analysis .\n",
      "['embedded languages', 'partial evaluation', 'macros', 'value flow analysis <eos>']\n",
      "['flow analysis', 'partial evaluation']\n",
      "['embedded languages', 'static analysis', 'embedded software', 'verification']\n",
      "2 4 6\n",
      "additional scores from absent units to present :  ['embedded languages']\n",
      "\n",
      "\n",
      "increasing internet capacity using local search . open shortest path first ( ospf ) is one of the most commonly used intra domain internet routing protocol . traffic flow is routed along shortest paths , splitting flow evenly at nodes where several outgoing links are on shortest paths to the destination . the weights of the links , and thereby the shortest path routes , can be changed by the network operator . the weights could be set proportional to the physical lengths of the links , but often the main goal is to avoid congestion , i . e . overloading of links , and the standard heuristic recommended by cisco ( a major router vendor ) is to make the weight of a link inversely proportional to its capacity . we study the problem of optimizing ospf weights for a given a set of projected demands so as to avoid congestion . we show this problem is np hard , even for approximation , and propose a local search heuristic to solve it . we also provide worst case results about the performance of ospf routing vs . an optimal multi commodity flow routing . our numerical experiments compare the results obtained with our local search heuristic to the optimal multi commodity flow routing , as well as simple and commonly used heuristics for setting the weights . experiments were done with a proposed next generation at t worldnet backbone as well as synthetic internetworks .\n",
      "['local search', 'shortest path routing', 'traffic engineering <eos>']\n",
      "['open shortest path']\n",
      "['local search', 'ospf', 'internet routing']\n",
      "1 3 4\n",
      "additional scores from absent units to present :  ['local search']\n",
      "\n",
      "\n",
      "a nonmonotonic observation logic . a variant of reiter ' s default logic is proposed as a logic for reasoning with ( defeasible ) observations . traditionally , default rules are assumed to represent generic information and the facts are assumed to represent specific information about the situation , but in this paper , the specific information derives from defeasible observations represented by ( normal free ) default rules , and the facts represent ( hard ) background knowledge . whenever the evidence underlying some observation is more refined than the evidence underlying another observation , this is modelled by means of a priority between the default rules representing the observations . we thus arrive at an interpretation of prioritized normal free default logic as an observation logic , and we propose a semantics for this observation logic . finally , we discuss how the proposed observation logic relates to the multiple extension problem and the problem of sensor fusion .\n",
      "['defeasible observations', 'free default logic', 'multiple extension problem', 'sensor fusion', 'nonmonotonic logic', 'prioritized default logic <eos>']\n",
      "['default logic', 'observation logic']\n",
      "['sensor fusion', 'default logic', 'nonmonotonic observation', 'nonmonotonic reasoning']\n",
      "2 4 6\n",
      "additional scores from absent units to present :  ['sensor fusion']\n",
      "\n",
      "\n",
      "self stabilizing clock synchronization in the presence of byzantine faults . we initiate a study of bounded clock synchronization under a more severe fault model than that proposed by lamport and melliar smith <digit> . realistic aspects of the problem of synchronizing clocks in the presence of faults are considered . one aspect is that clock synchronization is an on going task , thus the assumption that some of the processors never fail is too optimistic . to cope with this reality , we suggest self stabilizing protocols that stabilize in any ( long enough ) period in which less than a third of the processors are faulty . another aspect is that the clock value of each processor is bounded . a single transient fault may cause the clock to reach the upper bound . therefore , we suggest a bounded clock that wraps around when appropriate . we present two randomized self stabilizing protocols for synchronizing bounded clocks in the presence of byzantine processor failures . the first protocol assumes that processors have a common pulse , while the second protocol does not . a new type of distributed counter based on the chinese remainder theorem is used as part of the first protocol .\n",
      "['self stabilization', 'clock synchronization', 'byzantine failures <eos>']\n",
      "['Present predictions:']\n",
      "['byzantine faults', 'chinese remainder theorem', 'clock synchronization', 'self stabilization', 'byzantine fault tolerance', 'self stabilization', 'fault tolerance', 'fault tolerance', 'distributed systems']\n",
      "1 9 9\n",
      "additional scores from absent units to present :  ['clock synchronization']\n",
      "\n",
      "\n",
      "plugging haskell in . extension languages enable users to expand the functionality of an application without touching its source code . commonly , these languages are dynamically typed languages , such as lisp , python , or domain specific languages , which support runtime plugins via dynamic loading of components . we show that haskell can be comfortably used as a statically typed extension language for both haskell and foreign language applications supported by the haskell ffi , and that it can perform type safe dynamic loading of plugins using dynamic types . moreover , we discuss how plugin support is especially useful to applications where haskell is used as an embedded domain specific language ( edsl ) . we explain how to realise type safe plugins using dynamic types , runtime compilation , and dynamic linking , exploiting infrastructure provided by the glasgow haskell compiler . we demonstrate the practicability of our approach with several applications that serve as running examples .\n",
      "['extension languages', 'dynamic typing', 'plugins', 'dynamic loading', 'staged type inference', 'functional programming <eos>']\n",
      "['dynamic types']\n",
      "['languages', 'haskell', 'languages', 'domain specific languages', 'design', 'dynamic loading']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['dynamic loading']\n",
      "\n",
      "\n",
      "an event detection algebra for reactive systems . in reactive systems , execution is driven by external events to which the system should respond with appropriate actions . such events can be simple , but systems are often supposed to react to sophisticated situations involving a number of simpler events occurring in accordance with some pattern . a systematic approach to handle this type of systems is to separate the mechanism for detecting composite events from the rest of the application logic . in this paper , we present an event algebra for composite event detection . we show a number of algebraic laws that facilitate formal reasoning , and justify the algebra semantics by showing to what extent the operators comply with intuition . finally , we present an implementation of the algebra , and identify a large subset of expressions for which detection can be performed with bounded resources .\n",
      "['event detection', 'reactive systems', 'event algebra', 'resource efficiency <eos>']\n",
      "['formal reasoning', 'composite event algebra']\n",
      "['reactive systems', 'algebra', 'detection', 'event detection', 'reactive systems', 'formal methods']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['reactive systems', 'event detection', 'reactive systems']\n",
      "\n",
      "\n",
      "a methodology for generating verified combinatorial circuits . high level programming languages offer significant expressivity but provide little or no guarantees about resource use . resource bounded languages such as hardware description languages provide strong guarantees about the runtime behavior of computations but often lack mechanisms that allow programmers to write more structured , modular , and reusable programs . to overcome this basic tension in language design , recent work advocated the use of resource aware programming ( rap ) languages , which take into account the natural distinction between the development platform and the deployment platform for resource constrained software . this paper investigates the use of rap languages for the generation of combinatorial circuits . the key challenge that we encounter is that the rap approach does not safely admit a mechanism to express a posteriori ( post generation ) optimizations . the paper proposes and studies the use of abstract interpretation to overcome this problem . the approach is illustrated using an in depth analysis of the fast fourier transform ( fft ) . the generated computations are comparable to those generated by fftw .\n",
      "['abstract interpretation', 'multi stage programming <eos>']\n",
      "['combinatorial circuits', 'fast fourier transform']\n",
      "['resource aware programming', 'abstract interpretation', 'resource constrained programming']\n",
      "2 3 5\n",
      "additional scores from absent units to present :  ['abstract interpretation']\n",
      "\n",
      "\n",
      "adaptive offloading for pervasive computing . pervasive computing lets users continuously and consistently access an application on heterogeneous devices . however , delivering complex applications on resource constrained mobile devices such as cell phones is challenging . application or system based adaptations attempt to address the problem , but often at the cost of considerable degradation to application fidelity . the solution is to dynamically partition the application and offload part of the application execution data to a powerful nearby surrogate . this allows delivery of the application in a pervasive computing environment without significant fidelity degradation or expensive application rewriting . runtime offloading must adapt to different application execution patterns and resource fluctuations in the pervasive computing environment . this offloading inference engine adaptively solves two key decision making problems in runtime offloading timely triggering of offloading and efficient partitioning of applications . both trace driven simulations and prototype experiments confirm the effectiveness of this adaptive offloading system .\n",
      "['adaptive offloading', 'pervasive computing', 'mobile device', 'dynamic partitioning', 'application execution <eos>']\n",
      "['Present predictions:']\n",
      "['pervasive computing', 'offloading', 'adaptive offloading', 'adaptive offloading']\n",
      "1 4 4\n",
      "additional scores from absent units to present :  ['pervasive computing', 'adaptive offloading', 'adaptive offloading']\n",
      "\n",
      "\n",
      "resolution for label based formulas in hierarchical representation . order sorted logic <unk> many and partially ordered sorts as a sort hierarchy . in the field of knowledge representation and reasoning , it is useful to develop reasoning systems fbr terminological knowledge , together with assertional knowledge . however , the expression of sort hierarchies can not sufficiently capture the lexical diversity of terminological knowledge . in addition to sorts , various kinds of symbols constants , functions and predicates are semantically and hierarchically associated with each other . this is because natural language words identifying these symbols can be employed in the description of terminological knowledge . in this paper , we present a label based language for consistently handling the variety of hierarchical relationships among symbol names . for this language we develop a sorted resolution system whose reasoning power is enhanced by adding hierarchical inference rules with labeled substitutions .\n",
      "['order sorted logic', 'knowledge representation', 'terminological knowledge', 'resolution system', 'label based expressions <eos>']\n",
      "['hierarchical inference']\n",
      "['resolution', 'order sorted logic', 'label based formulas', 'label based reasoning', 'knowledge representation', 'hierarchical reasoning']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['order sorted logic', 'knowledge representation']\n",
      "\n",
      "\n",
      "unit disk graph approximation . finding a good embedding of a unit disk graph given by its connectivity information is a problem of practical importance in a variety of fields . in wireless ad hoc and sensor networks , such an embedding can be used to obtain virtual coordinates . in this paper , we prove a non approximability result for the problem of embedding a given unit disk graph . particularly , we show that if non neighboring nodes are not allowed to be closer to each other than distance <digit> , then two neighbors can be as far apart as <digit> <digit> , where goes to <digit> as n goes to infinity , unless p np . we further show that finding a realization of a d quasi unit disk graph with d <digit> <digit> is np hard .\n",
      "['unit disk graph', 'embedding', 'sensor networks', 'virtual coordinates', 'ad hoc networks <eos>']\n",
      "['sensor networks', 'disk graph']\n",
      "['connectivity', 'embedding', 'unit disk graph']\n",
      "2 3 5\n",
      "additional scores from absent units to present :  ['embedding', 'unit disk graph']\n",
      "\n",
      "\n",
      "debugging larch shared language specifications . the checkability designed into the lsl ( larch shared language ) is described , and two tools that help perform the checking are discussed . lp ( the larch power ) is the principal debugging tool . its design and development have been motivated primarily by work on lsl , but it also has other uses ( e . g . reasoning about circuits and concurrent algorithms ) . because of these other uses , and because they also tend to use lp to analyze larch interface specifications , the authors have tried not to make lp too lsl specific . instead , they have chosen to build a second tool , <unk> ( the lsl checker ) , to serve as a front end to lp . <unk> checks the syntax and static semantics of lsl specifications and generates lp proof obligations from their claims . these proof obligations fall into three categories consistency ( that a specification does not contradict itself ) , theory containment ( that a specification has intended consequences ) , and relative completeness ( that a set of operators is adequately defined ) . an extended example illustrating how lp is used to debug lsl specifications is presented .\n",
      "['debugging', 'larch shared language specifications', 'checkability', 'design', 'larch power', 'development', 'concurrent algorithms', 'static semantics', 'consistency', 'theory containment', 'parallel programming', 'inference mechanisms', 'formal specification', 'program debugging <eos>']\n",
      "['shared language']\n",
      "['debugging', 'shared language', 'debugging', 'theory', 'shared language', 'verification']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['debugging', 'debugging']\n",
      "\n",
      "\n",
      "tight bounds for testing bipartiteness in general graphs . in this paper we consider the problem of testing bipartiteness of general graphs . the problem has previously been studied in two models , one most suitable for dense graphs and one most suitable for bounded degree graphs . roughly speaking , dense graphs can be tested for bipartiteness with constant complexity , while the complexity of testing bounded degree graphs is tilde theta ( sqrt n ) , where n is the number of vertices in the graph ( and tilde theta ( f ( n ) ) means theta ( f ( n ) cdot rm polylog ( f ( n ) ) ) ) . thus there is a large gap between the complexity of testing in the two cases . in this work we bridge the gap described above . in particular , we study the problem of testing bipartiteness in a model that is suitable for all densities . we present an algorithm whose complexity is tilde o ( min ( sqrt n , n <digit> m ) ) , where m is the number of edges in the graph , and we match it with an almost tight lower bound .\n",
      "['bipartiteness', 'property testing', 'randomized algorithms <eos>']\n",
      "['Present predictions:']\n",
      "['general graphs', 'bipartiteness', 'testing bipartiteness', 'bounded degree graphs', 'graph complexity']\n",
      "1 5 5\n",
      "additional scores from absent units to present :  ['bipartiteness']\n",
      "\n",
      "\n",
      "rounding algorithms for a geometric embedding of minimum multiway cut . given an undirected graph with edge costs and a subset ofk <digit> nodes <unk> , a multiway , ork way , cut is a subset of the edges whose removal disconnects each terminal from the others . the multiway cut problem is to find a minimum cost multiway cut . this problem is max snp hard . recently , calinescu et al . ( calinescu , g . , h . karloff , y . rabani . <digit> . an improved approximation algorithm for multiway cut . j . comput . system sci . <digit> ( <digit> ) <digit> <digit> ) gave a novel geometric relaxation of the problem and a rounding scheme that produced a ( <digit> <digit> <digit> k ) approximation algorithm . in this paper , we study their geometric relaxation . in particular , we study the worst case ratio between the value of the relaxation and the value of the minimum multicut ( the so called integrality gap of the relaxation ) . fork <digit> , we show the integrality gap is <digit> <digit> , giving tight upper and lower bounds . that is , we exhibit a family of graphs with integrality gaps arbitrarily close to <digit> <digit> and give an algorithm that finds a cut of value <digit> <digit> times the relaxation value . our lower bound shows that this is the best possible performance guarantee for any algorithm based purely on the value of the relaxation . our upper bound meets the lower bound and improves the factor of <digit> <digit> shown by calinescu et al . for allk , we show that there exists a rounding scheme with performance ratio equal to the integrality gap , and we give explicit constructions of polynomial time rounding schemes that lead to improved upper bounds . fork <digit> and <digit> , our best upper bounds are based on computer constructed rounding schemes ( with computer proofs of correctness ) . for <unk> we give an algorithm with performance ratio <digit> . <digit> e k . our results were discovered with the help of computational experiments that we also describe here .\n",
      "['multiway cut', 'approximation algorithm <eos>']\n",
      "['multiway cut']\n",
      "['multicut', 'geometric embedding', 'approximation algorithms', 'integrality gap', 'multiway cut', 'rounding', 'multiway cut problem']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['multiway cut', 'multiway cut']\n",
      "\n",
      "\n",
      "hard equality constrained integer knapsacks . we consider the following integer feasibility problem given positive integer <unk> , a1 , . . . , a n , with gcd ( a1 , . . . , a n does there exist a <unk> z n <unk> x a0 we prove that if the <unk> , . . . , a <unk> a certain decomposable structure , then the frobenius number associated <unk> , . . . , a n , i . e . , the largest value <unk> for <unk> x a0 does not have a nonnegative integer solution , is close to a known upper bound . in the instances we consider , we <unk> to be the frobenius number . furthermore , we show that the decomposable structure <unk> , . . . , a <unk> the solution of a lattice reformulation of our problem almost trivial , since the number of lattice hyperplanes that intersect the polytope resulting from the reformulation in the direction of the last coordinate is going to be very small . for branch and bound such instances are difficult to solve , since they are infeasible and have large values <unk> a i , <digit> i n . we illustrate our results by some computational examples .\n",
      "['frobenius number', 'lattice basis reduction', 'branching on hyperplanes <eos>']\n",
      "['branch and bound']\n",
      "['integer feasibility problem', 'frobenius number', 'lattice reformulation', 'equality constrained integer programming']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['frobenius number']\n",
      "\n",
      "\n",
      "constraint programming viewed as rule based programming . we study here a natural situation when constraint programming can be entirely reduced to rule based programming . to this end we explain first how one can compute on constraint satisfaction problems using rules represented by simple first order formulas . then we consider constraint satisfaction problems that are based on predefined , explicitly given constraints . to solve them we first derive rules from these explicitly given constraints and limit the computation process to a repeated application of these rules , combined with labeling . we consider two types of rule here . the first type , that we call equality rules , leads to a new notion of local consistency , called rule consistency that turns out to be weaker than arc consistency for constraints of arbitrary arity ( called hyper arc consistency in marriott stuckey ( <digit> ) ) . for boolean constraints rule consistency coincides with the closure under the well known propagation rules for boolean constraints . the second type of rules , that we call membership rules , yields a rule based characterization of arc consistency . to show feasibility of this rule based approach to constraint programming , we show how both types of rules can be automatically generated , as chr rules of <unk> ( <digit> ) . this yields an implementation of this approach to programming by means of constraint logic programming . we illustrate the usefulness of this approach to constraint programming by discussing various examples , including boolean constraints , two typical examples of many valued logics , constraints dealing with waltz ' s language for describing polyhedral scenes , and allen ' s qualitative approach to temporal logic .\n",
      "['constraint programming', 'rule based programming', 'finite domain <eos>']\n",
      "['boolean constraints']\n",
      "['rule based programming', 'hyper arc consistency', 'arc consistency', 'constraint programming', 'theory']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['rule based programming', 'constraint programming']\n",
      "\n",
      "\n",
      "typing constraint logic programs . we present a prescriptive type system with parametric polymorphism and subtyping for constraint logic programs . the aim of this type system is to detect programming errors statically . it introduces a type discipline for constraint logic programs and modules , while maintaining the capabilities of performing the usual coercions between constraint domains , and of typing meta programming predicates , thanks to the exibility of subtyping . the property of subject reduction expresses the consistency of a prescriptive type system w . r . t . the execution model if a program is well typed , then all derivations starting from a well typed goal are again well typed . that property is proved w . r . t . the abstract execution model of constraint programming which proceeds by accumulation of constraints only , and w . r . t . an enriched execution model with type constraints for substitutions . we describe our implementation of the system for type checking and type inference . we report our experimental results on type checking iso prolog , the ( constraint ) libraries of sicstus prolog and other prolog programs .\n",
      "['constraint logic programming', 'type systems', 'subtyping', 'prolog', 'metaprogramming <eos>']\n",
      "['parametric polymorphism', 'constraint logic programming']\n",
      "['prolog', 'subtyping', 'type systems', 'typing', 'theory', 'type theory', 'theory']\n",
      "2 7 9\n",
      "additional scores from absent units to present :  ['prolog', 'subtyping']\n",
      "\n",
      "\n",
      "a statistical analysis of the long run node spatial distribution in mobile ad hoc networks . in this paper , we analyze the node spatial distribution of mobile wireless ad hoc networks . characterizing this distribution is of fundamental importance in the analysis of many relevant properties of mobile ad hoc networks , such as connectivity , average route length , and network capacity . in particular , we have investigated under what conditions the node spatial distribution resulting after a large number of mobility steps resembles the uniform distribution . this is motivated by the fact that the existing theoretical results concerning mobile ad hoc networks are based on this assumption . in order to test this hypothesis , we performed extensive simulations using two well known mobility models the random waypoint model , which resembles intentional movement , and a brownian like model , which resembles nonintentional movement . our analysis has shown that in brownian like motion the uniformity assumption does hold , and that the intensity of the concentration of nodes in the center of the deployment region that occurs in the random waypoint model heavily depends on the choice of some mobility parameters . for extreme values of these parameters , the uniformity assumption is impaired .\n",
      "['node spatial distribution', 'mobile ad hoc networks', 'mobility modeling', 'random waypoint model <eos>']\n",
      "['mobile ad hoc networks', 'wireless ad hoc networks']\n",
      "['mobility', 'mobility', 'mobility', 'random waypoint model', 'ad hoc networks', 'mobile ad hoc networks', 'long run node spatial distribution']\n",
      "2 7 9\n",
      "additional scores from absent units to present :  ['mobile ad hoc networks', 'mobile ad hoc networks']\n",
      "\n",
      "\n",
      "adaptive dissemination of data in time critical asymmetric communication environments . the proliferation of new data intensive applications in asymmetric communication environments has led to an increasing interest in the development of push based techniques , in which the information is broadcast to a large population of clients in order to achieve the most efficient use of the limited server and communication resources . it is important to note that quite often the data that is broadcast is time critical in nature . most of the related current research focuses on a pure push based approach ( broadcast disks model ) , where the transmission of data is done without allowing explicit requests from the users . more recently , some bidirectional models incorporating a low capacity uplink channel have been proposed in order to increase the functionality of the broadcast disks model . however , the impact of integration of the uplink channel has been investigated using only static client profiles or ignoring the existence of time sensitive data . none of the existing models integrates all the characteristics needed to perform effectively in a real world , dynamic time critical asymmetric communication environment . in this paper we present an adaptive data dissemination model and the associated on line scheduling algorithms . these improve the functionality and performance of bidirectional broadcast models , maximizing the total number of satisfied users in asymmetric communication environments with dynamic client profiles and time requirements ( e . g . , mobile systems ) . this is achieved by means of dynamic adaptation of the broadcast program to the needs of the users , taking into account the bandwidth constraints inherent in asymmetric communication environments and the deadline requirements of the user requests . performance is evaluated by simulation of a real time asymmetric communication environment\n",
      "['asymmetric communication', 'push based techniques', 'scheduling', 'broadcast data dissemination', 'time critical data <eos>']\n",
      "['Present predictions:']\n",
      "['time critical communication', 'data dissemination', 'push based approach', 'asymmetric communication', 'adaptive dissemination', 'time critical communication', 'adaptive data dissemination', 'mobile computing', 'data dissemination', 'asymmetric communication']\n",
      "1 10 10\n",
      "additional scores from absent units to present :  ['asymmetric communication', 'asymmetric communication']\n",
      "\n",
      "\n",
      "towards provable security for ad hoc routing protocols . we propose a formal framework for the security analysis of on demand source routing protocols for wireless ad hoc networks . our approach is based on the well known simulation paradigm that has been proposed to prove the security of cryptographic protocols . our main contribution is the application of the simulation based approach in the context of ad hoc routing . this involves a precise definition of a real world model , which describes the real operation of the protocol , and an ideal world model , which captures what the protocol wants to achieve in terms of security . both models take into account the peculiarities of wireless communications and ad hoc routing . then , we give a formal definition of routing security in terms of indistinguishability of the two models from the point of view of honest parties . we demonstrate the usefulness of our approach by analyzing two secure ad hoc routing protocols , srp and ariadne . this analysis leads to the discovery of as yet unknown attacks against both protocols . finally , we propose a new ad hoc routing protocol and prove it to be secure in our model .\n",
      "['provable security', 'routing protocols', 'on demand source routing', 'ad hoc networks', 'simulatability <eos>']\n",
      "['wireless ad hoc networks']\n",
      "['routing protocols', 'security', 'ad hoc routing', 'provable security', 'wireless ad hoc networks']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['routing protocols', 'provable security']\n",
      "\n",
      "\n",
      "( optimal ) duplication is not elementary recursive . in <digit> , asperti and mairson proved that the cost of reducing a term using an optimal reducer ( a la <unk> ) can not be bound by any elementary function in the number of shared beta steps . we prove in this paper that an analogous result holds for lamping ' s abstract algorithm . that is , there is no elementary function in the number of shared beta steps bounding the number of duplication steps of the optimal reducer . this theorem vindicates the oracle of lamping ' s algorithm as the culprit for the negative result of asperti and mairson . the result is obtained using as a technical tool elementary affine logic .\n",
      "['elementary affine logic', 'optimal reduction', 'complexity', 'graph rewriting <eos>']\n",
      "['affine logic']\n",
      "['duplication', 'elementary affine logic', 'recursive algorithm', '<unk>']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['elementary affine logic']\n",
      "\n",
      "\n",
      "id based encryption for complex hierarchies with applications to forward security and broadcast encryption . a forward secure encryption scheme protects secret keys from exposure by evolving the keys with time . forward security has several unique requirements in hierarchical identity based encryption ( hibe ) scheme ( <digit> ) users join dynamically ( <digit> ) encryption is joining time oblivious ( <digit> ) users evolve secret keys autonomously . we present a scalable forward secure hibe ( fs hibe ) scheme satisfying the above properties . we also show how our fs hibe scheme can be used to construct a forward secure public key broadcast encryption scheme , which protects the secrecy of prior transmissions in the broadcast encryption setting . we further generalize fs hibe into a collusion resistant multiple hierarchical id based encryption scheme , which can be used for secure communications with entities having multiple roles in role based access control . the security of our schemes is based on the bilinear diffie hellman assumption in the random oracle model .\n",
      "['id based encryption', 'forward security', 'broadcast encryption <eos>']\n",
      "['access control', 'hierarchical identity based encryption']\n",
      "['broadcast encryption', 'random oracle model', 'collusion resistant', 'forward secure encryption', 'id based encryption']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['id based encryption']\n",
      "\n",
      "\n",
      "models of software development environments . a general model of software development environments that consists of structures , mechanisms , and policies is presented . the advantage of this model is that it distinguishes intuitively those aspects of an environment that are useful in comparing and contrasting software development environments . four classes of environments the individual , the family , the city . and the state are characterized by means of a sociological metaphor based on scale . the utility of the taxonomy is that it delineates the important classes of interactions among software developers and exposes the ways in which current software development environments inadequately support the development of large systems . the generality of the model is demonstrated by its application to a previously published taxonomy that categorizes environments according to how they relate to language centered , structure oriented , toolkit , and method based environments .\n",
      "['software development environments', 'sociological metaphor', 'method based environments', 'toolkit environments', 'structure oriented environments', 'language centered environments', 'programming environments <eos>']\n",
      "['Present predictions:']\n",
      "['software development environments', 'software development environments', 'software engineering']\n",
      "1 3 3\n",
      "additional scores from absent units to present :  ['software development environments', 'software development environments']\n",
      "\n",
      "\n",
      "reliable solution of special event location problems for odes . computing the solution of the initial value problem in ordinary differential equations ( odes ) may be only part of a larger task . one such task is finding where an algebraic function of the solution ( an event function ) has a root ( an event occurs ) . this is a task which is difficult both in theory and in software practice . for certain useful kinds of event functions , it is possible to avoid two fundamental difficulties . it is described how to achieve the reliable solutions of such problems in a way that allows the capability to be grafted onto popular codes for the initial value problem .\n",
      "['event location', 'root finding', 'dense output <eos>']\n",
      "['ordinary differential equations']\n",
      "['initial value problem', 'event location', 'event location', '<unk>']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['event location', 'event location']\n",
      "\n",
      "\n",
      "mixed finite element approximation of incompressible mhd problems based on weighted regularization . we introduce and analyze a new mixed finite element method for the numerical approximation of stationary incompressible magneto hydrodynamics ( mhd ) problems in polygonal and polyhedral domains . the method is based on standard inf sup stable elements for the discretization of the hydrodynamic unknowns and on nodal elements for the discretization of the magnetic variables . in order to achieve convergence in non convex domains , the magnetic bilinear form is suitably modified using the weighted regularization technique recently developed in numer . math . <digit> ( <digit> ) <digit> . we first discuss the well posedness of this approach and establish a novel existence and uniqueness result for non linear mhd problems with small data . we then derive quasi optimal error bounds for the proposed finite element method and show the convergence of the approximate solutions in non convex domains . the theoretical results are confirmed in a series of numerical experiments for a linear two dimensional oseen type mhd problem , demonstrating that weighted regularization is indispensable for the resolution of the strongest magnetic singularities .\n",
      "['weighted regularization', 'incompressible magneto hydrodynamics', 'mixed methods <eos>']\n",
      "['finite element method', 'incompressible magneto hydrodynamics']\n",
      "['weighted regularization', 'mhd', 'mixed finite element method', 'mixed finite element method', 'incompressible flow', '<unk>']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['weighted regularization']\n",
      "\n",
      "\n",
      "i o efficient dynamic planar point location . we present an i o efficient dynamic data structure for point location in a general planar subdivision . our structure uses o ( < i > n b < i > ) disk blocks of size < i > b < i > to store a subdivision of size < i > n < i > . queries can be answered in o ( log < inf > < i > b < i > < inf > < sup > <digit> < sup > < i > n < i > ) i os in the worst case , and insertions and deletions can be performed in o ( log < inf > < i > b < i > < inf > < sup > <digit> < sup > < i > n < i > ) and o ( log < inf > < i > b < i > < inf > < i > n < i > ) i os amortized , respectively . part of our data structure is based on an external version of the so called logarithmic method that allows for efficient dynamization of static external memory data structures with certain characteristics . another important part of our structure is an external data structure for vertical ray shooting among line segments in the plane with endpoints on < i > b < i > lines , developed using an external version of dynamic fractional cascading . we believe that these methods could prove helpful in the development of other dynamic external memory data structures .\n",
      "['i o efficient', 'point location', 'dynamic data structures', 'external memory <eos>']\n",
      "['Present predictions:']\n",
      "['dynamic data structures', 'point location', 'i o efficient', 'dynamic data structures', 'algorithms']\n",
      "1 5 5\n",
      "additional scores from absent units to present :  ['point location', 'i o efficient']\n",
      "\n",
      "\n",
      "efficient leader election using sense of direction . this paper presents a protocol for leader election in complete networks with a sense of direction . sense of direction provides nodes the capability of distinguishing between their incident links according to a global scheme . we propose a protocol for leader election which requires < i > o ( n ) < i > messages and < i > o < i > ( log < i > n < i > ) time . the protocol is message optimal and the time complexity is a significant improvement over currently known protocols for this problem .\n",
      "['leader election', 'complete networks', 'message complexity', 'distributed algorithm <eos>']\n",
      "['complete networks']\n",
      "['sense of direction', 'leader election', 'distributed algorithms']\n",
      "1 3 4\n",
      "additional scores from absent units to present :  ['leader election']\n",
      "\n",
      "\n",
      "communication lower bounds for distributed memory matrix multiplication . we present lower bounds on the amount of communication that matrix multiplication algorithms must perform on a distributed memory parallel computer . we denote the number of processors by < i > p < i > and the dimension of square matrices by < i > n < i > . we show that the most widely used class of algorithms , the so called two dimensional ( 2d ) algorithms , are optimal , in the sense that in any algorithm that only uses < i > o < i > ( < i > n < i > < sup > <digit> < sup > < i > p < i > ) words of memory per processor , at least one processor must send or receive ( < i > n < i > < sup > <digit> < sup > < i > p < i > < sup > <digit> <digit> < sup > ) words . we also show that algorithms from another class , the so called three dimensional ( 3d ) algorithms , are also optimal . these algorithms use replication to reduce communication . we show that in any algorithm that uses < i > o < i > ( < i > n < i > < sup > <digit> < sup > < i > p < i > < sup > <digit> <digit> < sup > ) words of memory per processor , at least one processor must send or receive ( < i > n < i > < sup > <digit> < sup > < i > p < i > < sup > <digit> <digit> < sup > ) words . furthermore , we show a continuous tradeoff between the size of local memories and the amount of communication that must be performed . the 2d and 3d bounds are essentially instantiations of this tradeoff . we also show that if the input is distributed across the local memories of multiple nodes without replication , then ( < i > n < i > < sup > <digit> < sup > ) words must cross any bisection cut of the machine . all our bounds apply only to conventional ( < i > n < i > < sup > <digit> < sup > ) algorithms . they do not apply to strassen ' s algorithm or other < i > o < i > ( < i > n < i > < sup > <digit> < sup > ) algorithms .\n",
      "['communication', 'lower bounds', 'distributed memory', 'matrix multiplication <eos>']\n",
      "['distributed memory']\n",
      "['replication', 'lower bounds', 'matrix multiplication', 'communication complexity', 'parallel algorithms', 'distributed memory algorithms']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['lower bounds']\n",
      "\n",
      "\n",
      "approximation algorithms for partial covering problems . we study a generalization of covering problems called partial covering . here we wish to cover only a desired number of elements , rather than covering all elements as in standard covering problems . for example , in < i > k < i > partial set cover , we wish to choose a minimum number of sets to cover at least < i > k < i > elements . for < i > k < i > partial set cover , if each element occurs in at most < i > f < i > sets , then we derive a primal dual < i > f < i > approximation algorithm ( thus implying a <digit> approximation for < i > k < i > partial vertex cover ) in polynomial time . without making any assumption about the number of sets an element is in , for instances where each set has cardinality at most three , we obtain an approximation of <digit> <digit> . we also present better than <digit> approximation algorithms for < i > k < i > partial vertex cover on bounded degree graphs , and for vertex cover on expanders of bounded average degree . we obtain a polynomial time approximation scheme for < i > k < i > partial vertex cover on planar graphs , and for covering < i > k < i > points in < i > r < sup > d < sup > < i > by disks .\n",
      "['approximation algorithms', 'partial covering', 'set cover', 'vertex cover', 'randomized rounding', 'primal dual methods <eos>']\n",
      "['planar graph', 'partial covering problems']\n",
      "['expanders', 'vertex cover', 'covering problems', 'approximation algorithms']\n",
      "2 4 6\n",
      "additional scores from absent units to present :  ['vertex cover', 'approximation algorithms']\n",
      "\n",
      "\n",
      "crossing number , pair crossing number , and expansion . the < i > crossing number < i > cr ( < i > g < i > ) of a graph < i > g < i > is the minimum possible number of edge crossings in a drawing of < i > g < i > in the plane , while the < i > pair crossing number < i > pcr ( < i > g < i > ) is the smallest number of pairs of edges that cross in a drawing of < i > g < i > in the plane . while cr ( < i > g < i > ) pcr ( < i > g < i > ) holds trivially , it is not known whether a strict inequality can ever occur ( this question was raised by mohar and pach and tth ) . we aim at bounding cr ( < i > g < i > ) in terms of pcr ( < i > g < i > ) . using the methods of leighton and rao , bhatt and leighton , and even , guha and schieber , we prove that one of the main steps is an analogy of the well known lower bound cr ( < i > g < i > ) ( < i > b < i > ( < i > g < i > ) < sup > <digit> < sup > ) < i > o < i > ( <unk> ( < i > g < i > ) ) , where < i > b ( g ) < i > is the < i > bisection width < i > of < i > g < i > , that is , the smallest number of edges that have to be removed so that no component of the resulting graph has more than <digit> <digit> < i > n < i > vertices . we show that we also prove by similar methods that a graph < i > g < i > with crossing number log < sup > <digit> < sup > < i > n < i > has a nonplanar subgraph on at most < i > o < i > ( < i > nm < i > log < sup > <digit> < sup > < i > n < i > < i > k < i > ) vertices , where < i > m < i > is the number of edges , is the maximum degree in < i > g < i > , and < i > c < i > is a suitable sufficiently large constant .\n",
      "['crossing number', 'pair crossing number', 'expansion', 'graph drawing <eos>']\n",
      "['pair crossing number']\n",
      "['pair crossing number', 'crossing number', 'graph drawing']\n",
      "1 3 4\n",
      "additional scores from absent units to present :  ['pair crossing number', 'pair crossing number', 'crossing number']\n",
      "\n",
      "\n",
      "scenario based comparison of source tracing and dynamic source routing protocols for ad hoc networks . we present source tracing as a new viable approach to routing in ad hoc networks in which routers communicate the second to last hop and distance in preferred paths to destinations . we introduce a table driven protocol ( best ) in which routers maintain routing information for all destinations , and an on demand routing protocol ( dst ) in which routers maintain routing information for only those destinations to whom they need to forward data . simulation experiments are used to compare these protocols with dsr , which has been shown to incur less control overhead that other on demand routing protocols . the simulations show that dst requires far less control packets to achieve comparable or better average delays and percentage of packet delivered than dsr , and that best achieves comparable results to dsr while maintaining routing information for all destinations .\n",
      "['ad hoc networks', 'on demand routing', 'wireless routing <eos>']\n",
      "['dynamic source tracing']\n",
      "['routing protocols', 'source tracing', 'ad hoc networks', 'wireless communication', 'performance evaluation']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['ad hoc networks']\n",
      "\n",
      "\n",
      "privacy preserving data mining . data mining is under attack from privacy advocates because of a misunderstanding about what it actually is and a valid concern about how it ' s generally done . this article shows how technology from the security community can change data mining for the better , providing all its benefits while still maintaining privacy .\n",
      "['privacy', 'data mining <eos>']\n",
      "['privacy', 'security']\n",
      "['data mining', 'privacy', 'security policy', 'privacy preserving data mining', 'privacy preserving data mining']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['privacy', 'privacy']\n",
      "\n",
      "\n",
      "selection of views to materialize in a data warehouse . a data warehouse stores materialized views of data from one or more sources , with the purpose of efficiently implementing decision support or olap queries . one of the most important decisions in designing a data warehouse is the selection of materialized views to be maintained at the warehouse . the goal is to select an appropriate set of views that minimizes total query response time and the cost of maintaining the selected views , given a limited amount of resource , e . g . , materialization time , storage space , etc . in this article , we have developed a theoretical framework for the general problem of selection of views in a data warehouse . we present polynomial time heuristics for a selection of views to optimize total query response time under a disk space constraint , for some important special cases of the general data warehouse scenario , viz . <digit> ) an and view graph , where each query view has a unique evaluation , e . g . , when a multiple query optimizer can be used to general a global evaluation plan for the queries , and <digit> ) an or view graph , in which any view can be computed from any one of its related views , e . g . , data cubes . we present proofs showing that the algorithms are guaranteed to provide a solution that is fairly close to ( within a constant factor ratio of ) the optimal solution . we extend our heuristic to the general and or view graphs . finally , we address in detail the view selection problem under the maintenance cost constraint and present provably competitive heuristics .\n",
      "['materialization', 'data warehouse', 'view selection', 'index terms views <eos>']\n",
      "['Present predictions:']\n",
      "['selection of views', 'data warehouse', 'design', 'data warehouse selection', 'query processing', 'algorithms']\n",
      "1 6 6\n",
      "additional scores from absent units to present :  ['data warehouse']\n",
      "\n",
      "\n",
      "a tight bound on approximating arbitrary metrics by tree metrics . in this paper , we show that any n point metric space can be embedded into a distribution over dominating tree metrics such that the expected stretch of any edge is o ( log n ) . this improves upon the result of bartal who gave a bound of o ( log n log log n ) . moreover , our result is existentially tight there exist metric spaces where any tree embedding must have distortion ( log n ) distortion . this problem lies at the heart of numerous approximation and online algorithms including ones for group steiner tree , metric labeling , buy at bulk network design and metrical task system . our result improves the performance guarantees for all of these problems .\n",
      "['metrics', 'tree metrics', 'embeddings <eos>']\n",
      "['dominating tree']\n",
      "['tree metrics', 'metric labeling', 'tree metric space', 'metric space', 'approximation algorithms']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['tree metrics']\n",
      "\n",
      "\n",
      "load balancing scatter operations for grid computing . we present solutions to statically load balance scatter operations in parallel codes run on grids . our load balancing strategy is based on the modification of the data distributions used in scatter operations . we study the replacement of scatter operations with parameterized scatters , allowing custom distributions of data . the paper presents ( <digit> ) a general algorithm which finds an optimal distribution of data across processors ( <digit> ) a quicker guaranteed heuristic relying on hypotheses on communications and computations ( <digit> ) a policy on the ordering of the processors . experimental results with an mpi scientific code illustrate the benefits obtained from our load balancing .\n",
      "['load balancing', 'scatter operation', 'grid computing', 'heterogeneous computing', 'parallel programming <eos>']\n",
      "['parallel codes']\n",
      "['grid computing', 'scatter operations', 'load balancing', 'grid computing', 'parallel computing']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['grid computing', 'load balancing', 'grid computing']\n",
      "\n",
      "\n",
      "spatial gossip and resource location protocols . the dynamic behavior of a network in which information is changing continuously over time requires robust and efficient mechanisms for keeping nodes updated about new information . gossip protocols are mechanisms for this task in which nodes communicate with one another according to some underlying deterministic or randomized algorithm , exchanging information in each communication step . in a variety of contexts , the use of randomization to propagate information has been found to provide better reliability and scalability than more regimented deterministic approaches . in many settings , such as a cluster of distributed computing hosts , new information is generated at individual nodes , and is most interesting to nodes that are nearby . thus , we propose distance based propagation bounds as a performance measure for gossip mechanisms a node at distance d from the origin of a new piece of information should be able to learn about this information with a delay that grows slowly with d , and is independent of the size of the network . for nodes arranged with uniform density in euclidean space , we present natural gossip mechanisms , called spatial gossip , that satisfy such a guarantee new information is spread to nodes at distance d , with high probability , in o ( log1 steps . such a bound combines the desirable qualitative features of uniform gossip , in which information is spread with a delay that is logarithmic in the full network size , and deterministic flooding , in which information is spread with a delay that is linear in the distance and independent of the network size . our mechanisms and their analysis resolve a conjecture of demers et al . <digit> . we further show an application of our gossip mechanisms to a basic resource location problem , in which nodes seek to rapidly learn the location of the nearest copy of a resource in a network . this problem , which is of considerable practical importance , can be solved by a very simple protocol using spatial gossip , whereas we can show that no protocol built on top of uniform gossip can inform nodes of their approximately nearest resource within poly logarithmic time . the analysis relies on an additional useful property of spatial gossip , namely that information travels from its source to sinks along short paths not visiting points of the network far from the two nodes .\n",
      "['gossip', 'resource location', 'decentralized algorithm <eos>']\n",
      "['Present predictions:']\n",
      "['resource location protocols', 'gossip', 'spatial gossip', 'spatial gossip', 'gossip protocols']\n",
      "1 5 5\n",
      "additional scores from absent units to present :  ['gossip']\n",
      "\n",
      "\n",
      "homotopies for intersecting solution components of polynomial systems . we show how to use numerical continuation to compute the intersection c a cap b of two algebraic sets a and b , where a , b , and c are numerically represented by witness sets . en route to this result , we first show how to find the irreducible decomposition of a system of polynomials restricted to an algebraic set . the intersection of components a and b then follows by considering the decomposition of the diagonal system of equations u restricted to u , v in a times b . an offshoot of this new approach is that one can solve a large system of equations by finding the solution components of its subsystems and then intersecting these . it also allows one to find the intersection of two components of the two polynomial systems , which is not possible with any previous numerical continuation approach .\n",
      "['polynomial system', 'generic points', 'components of solutions', 'numerical algebraic geometry', 'irreducible components', 'embedding', 'homotopy continuation <eos>']\n",
      "['algebraic set', 'witness sets']\n",
      "['polynomial systems', 'continuation', 'solution components', 'irreducible decomposition', '<unk>', 'polynomial system', 'algebraic system of polynomial systems', 'numerical continuation', '<unk>']\n",
      "2 9 11\n",
      "additional scores from absent units to present :  ['polynomial system']\n",
      "\n",
      "\n",
      "associated types with class . haskell ' s type classes allow ad hoc overloading , or type indexing , of functions . a natural generalisation is to allow type indexing of data types as well . it turns out that this idea directly supports a powerful form of abstraction called associated types , which are available in c using traits classes . associated types are useful in many applications , especially for self optimising libraries that adapt their data representations and algorithms in a type directed manner . in this paper , we introduce and motivate associated types as a rather natural generalisation of haskell ' s existing type classes . formally , we present a type system that includes a type directed translation into an explicitly typed target language akin to system f the existence of this translation ensures that the addition of associated data types to an existing haskell compiler only requires changes to the front end .\n",
      "['associated types', 'type classes', 'self optimising libraries', 'type directed translation', 'type indexed types <eos>']\n",
      "['indexing']\n",
      "['haskell', 'types', 'type classes', 'data types', 'theory', 'type systems', 'theory']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['type classes']\n",
      "\n",
      "\n",
      "ergodicity and throughput bounds of petri nets with unique consistent firing count vector . ergodicity and throughput bound characterization are addressed for a subclass of timed and stochastic petri nets , interleaving qualitative and quantitative theories . the nets considered represent an extension of the well known subclass of marked graphs , defined as having a unique consistent firing count vector , independently of the stochastic interpretation of the net model . in particular , persistent and mono t semiflow net subclasses are considered . upper and lower throughput bounds are computed using linear programming problems defined on the incidence matrix of the underlying net . the bounds proposed depend on the initial marking and the mean values of the delays but not on the probability distributions ( thus including both the deterministic and the stochastic cases ) . from a different perspective , the considered subclasses of synchronized queuing networks thus , the proposed bounds can be applied to these networks .\n",
      "['ergodicity', 'throughput bounds', 'petri nets', 'unique consistent firing count vector', 'marked graphs', 'mono t semiflow net subclasses', 'linear programming', 'incidence matrix', 'synchronized queuing networks', 'persistent nets <eos>']\n",
      "['linear programming']\n",
      "['ergodicity', 'marked graphs', 'stochastic petri nets', 'consistent firing count vector', 'throughput bounds', 'timed networks', 'stochastic programming']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['ergodicity', 'marked graphs', 'throughput bounds']\n",
      "\n",
      "\n",
      "multiple resolution segmentation of textured images . a multiple resolution algorithm is presented for segmenting images into regions with differing statistical behavior . in addition , an algorithm is developed for determining the number of statistically distinct regions in an image and estimating the parameters of those regions . both algorithms use a causal gaussian autoregressive model to describe the mean , variance , and spatial correlation of the image textures . together , the algorithms can be used to perform unsupervised texture segmentation . the multiple resolution segmentation algorithm first segments images at coarse resolution and then progresses to finer resolutions until individual pixels are classified . this method results in accurate segmentations and requires significantly less computation than some previously known methods . the field containing the classification of each pixel in the image is modeled as a markov random field . segmentation at each resolution is then performed by maximizing the a posteriori probability of this field subject to the resolution constraint . at each resolution , the a posteriori probability is maximized by a deterministic greedy algorithm which iteratively chooses the classification of individual pixels or pixel blocks . the unsupervised parameter estimation algorithm determines both the number of textures and their parameters by minimizing a global criterion based on the aic information criterion . clusters corresponding to the individual textures are formed by alternately estimating the cluster parameters and repartitioning the data into those clusters . concurrently , the number of distinct textures is estimated by combining clusters until a minimum of the criterion is reached .\n",
      "['multiple resolution segmentation', 'textured images', 'statistics', 'statistical behavior', 'causal gaussian autoregressive model', 'variance', 'spatial correlation', 'unsupervised texture segmentation', 'coarse resolution', 'classification', 'markov random field', 'posteriori probability', 'probability', 'deterministic greedy algorithm', 'parameter estimation', 'aic information criterion', 'picture processing', 'pattern recognition <eos>']\n",
      "['markov random field']\n",
      "['segmentation', 'resolution segmentation', 'multiple resolution segmentation', 'image segmentation', 'multiple resolution segmentation', 'image segmentation', 'unsupervised learning', 'causal gaussian autoregressive model']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['multiple resolution segmentation', 'multiple resolution segmentation', 'causal gaussian autoregressive model']\n",
      "\n",
      "\n",
      "piecewise surface flattening for non distorted texture mapping . this paper introduces new techniques for interactive piecewise flattening of parametric <digit> d surfaces , leading to a non distorted , hence realistic , texture mapping . cuts are allowed on the mapped texture and we make a compromise between discontinuities and distortions . these techniques are based on results from differential geometry , more precisely on the notion of geodesic curvature isoparametric curves of the surface are mapped , in a constructive way , onto curves in the texture plane with preservation of geodesic curvature at each point . as an application , we give a concrete example which is a first step towards an efficient and robust cad tool for shoe modeling .\n",
      "['piecewise surface flattening', 'non distorted texture mapping', 'differential geometry', 'geodesic curvature <eos>']\n",
      "['non distorted texture mapping']\n",
      "['surface flattening', 'texture mapping', 'piecewise surface flattening', 'piecewise surface flattening', 'surface flattening', 'non distorted texture mapping']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['non distorted texture mapping', 'piecewise surface flattening', 'piecewise surface flattening', 'non distorted texture mapping']\n",
      "\n",
      "\n",
      "refinement methods for geometric bounds in constructive solid geometry . in constructive solid geometry , geometric solids are represented as trees whose leaves are labeled by primitive solids and whose internal nodes are labeled by set theoretic operations . a bounding function in this context is an upper or lower estimate on the extent of the constituent sets such bounds are commonly used to speed up algorithms based on such trees . we introduce the class of totally consistent bounding functions , which have the desirable properties of allowing surprisingly good bounds to be built quickly . both outer and inner bounds can be refined using a set of rewrite rules , for which we give some complexity and convergence results . we have implemented the refinement rules for outer bounds within a solid modeling system , where they have proved especially useful for intersection testing in three and four dimensions . our implementations have used boxes as bounds , but different classes ( shapes ) of bounds are also explored . the rewrite rules are also applicable to relatively slow , exact operations , which we explore for their theoretical insight , and to general boolean algebras . results concerning the relationship between these bounds and active zones are also noted .\n",
      "['constructive solid geometry', 'solid modeling', 'boolean algebra', 'robotics', 'interference detection', 'collision detection', 'representation simplification <eos>']\n",
      "['rewrite rules']\n",
      "['constructive solid geometry', 'refinement', 'geometric bounds', 'solid geometry', 'refinement', 'constructive geometry']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['constructive solid geometry']\n",
      "\n",
      "\n",
      "distributed representations , simple recurrent networks , and grammatical structure . in this paper three problems for a connectionist account of language are considered <digit> . what is the nature of linguistic representations <digit> . how can complex structural relationships such as constituent structure be represented <digit> . how can the apparently open ended nature of language be accommodated by a fixed resource system using a prediction task , a simple recurrent network ( srn ) is trained on <unk> sentences which contain multiply embedded relative clauses . principal component analysis of the hidden unit activation patterns reveals that the network solves the task by developing complex distributed representations which encode the relevant grammatical relations and hierarchical constituent structure . differences between the srn state representations and the more traditional pushdown store are discussed in the final section .\n",
      "['distributed representations', 'simple recurrent networks', 'grammatical structure <eos>']\n",
      "['principal component analysis']\n",
      "['grammatical structure', 'recurrent networks', 'distributed representations', 'grammatical inference', 'distributed systems']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['distributed representations']\n",
      "\n",
      "\n",
      "embedding complete binary trees into butterfly networks . the authors present embeddings of complete binary trees into butterfly networks with or without wrap around connections . let m be an even integer and q m ( log m ) <digit> . the authors show how to embed a <digit> sup q <digit> <digit> node complete binary tree t ( q ) into a ( m <digit> ) <digit> sup m <digit> node wrap around butterfly b sub w ( m <digit> ) with a dilation of <digit> , and how to embed t ( q ) into a ( m <digit> ) <digit> sup m <digit> node wrap around butterfly b sub w ( m <digit> ) with an optimal dilation of <digit> . they also present an embedding of a wrap around butterfly b sub w ( m ) into a ( m <digit> ) <digit> sup m node no wrap around butterfly b ( m ) with a dilation of <digit> . using this embedding it is shown that t ( q ) can be embedded into a no wrap butterfly b ( m <digit> ) ( resp . b ( m <digit> ) ) with a dilation of <digit> ( resp . <digit> ) .\n",
      "['embeddings', 'complete binary trees', 'butterfly networks', 'wrap around connections', 'multiprocessor interconnection networks', 'trees mathematics <eos>']\n",
      "['butterfly']\n",
      "['butterfly networks', 'dilation', 'complete binary tree', 'embedding', 'graph embedding', 'interconnection networks']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['butterfly networks']\n",
      "\n",
      "\n",
      "a packaging system for heterogeneous execution environments . a packaging system that allows diverse software components to be easily interconnected within heterogeneous programming environments is described . interface software and stubs are generated for programmers automatically once the programmers express their application ' s geometry in a few simple rules and module interconnection language attributes . by generating custom interface code for each application , based on analysis and extraction of interfacing requirements , the system is able to produce executables whose run time performance is comparable to manually integrated applications . the system is implemented within the unix environment .\n",
      "['packaging system', 'heterogeneous execution environments', 'diverse software components', 'heterogeneous programming environments', 'programming environments', 'geometry', 'module interconnection language attributes', 'custom interface code', 'interfacing requirements', 'unix environment', 'automatic programming', 'configuration management', 'user interfaces <eos>']\n",
      "['packaging']\n",
      "['heterogeneous programming environments', 'packaging', 'heterogeneous computing', 'software engineering']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['heterogeneous programming environments']\n",
      "\n",
      "\n",
      "using program slicing in software maintenance . program slicing is applied to the software maintenance problem by extending the notion of a program slice ( that originally required both a variable and line number ) to a decomposition slice , one that captures all computation on a given variable , i . e . , is independent of line numbers . using the lattice of single variable decomposition slices ordered by set inclusion , it is shown how a slice based decomposition for programs can be formed . one can then delineate the effects of a proposed change by isolating those effects in a single component of the decomposition . this gives maintainers a straightforward technique for determining those statements and variables which may be modified in a component and those which may not . using the decomposition , a set of principles to prohibit changes which will interfere with unmodified components is provided . these semantically consistent changes can then be merged back into the original program in linear time .\n",
      "['program slice', 'program slicing', 'software maintenance', 'software maintenance problem', 'line number', 'single variable decomposition slices', 'set inclusion', 'slice based decomposition', 'unmodified components', 'semantically consistent changes', 'linear time', 'program testing <eos>']\n",
      "['Present predictions:']\n",
      "['software maintenance', 'decomposition', 'program slicing', 'software engineering', 'program analysis']\n",
      "1 5 5\n",
      "additional scores from absent units to present :  ['software maintenance', 'program slicing']\n",
      "\n",
      "\n",
      "adaptive programming . an adaptive program is one that changes its behavior base on the current state of its environment . this notion of adaptivity is formalized , and a logic for reasoning about adaptive programs is presented . the logic includes several composition operators that can be used to define an adaptive program in terms of given constituent programs programs resulting from these compositions retain the adaptive properties of their constituent programs . the authors begin by discussing adaptive sequential programs , then extend the discussion to adaptive distributed programs . the relationship between adaptivity and self stabilization is discussed . a case study for constructing an adaptive distributed program where a token is circulated in a ring of processes is presented .\n",
      "['adaptivity', 'composition operators', 'constituent programs', 'adaptive sequential programs', 'adaptive distributed programs', 'self stabilization', 'parallel programming', 'adaptive systems', 'token ring networks', 'programming theory', 'formal logic <eos>']\n",
      "['logic', 'distributed']\n",
      "['adaptivity', 'composition', 'self stabilization', 'adaptive programming', 'adaptive systems']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['adaptivity', 'self stabilization']\n",
      "\n",
      "\n",
      "performance prediction and evaluation of parallel processing on a numa multiprocessor . the efficiency of the basic operations of a numa ( nonuniform memory access ) multiprocessor determines the parallel processing performance on a numa multiprocessor . the authors present several analytical models for predicting and evaluating the overhead of interprocessor communication , process scheduling , process synchronization , and remote memory access , where network contention and memory contention are considered . performance measurements to support the models and analyses through several numerical examples have been done on the bbn gp1000 , a numa shared memory multiprocessor . analytical and experimental results give a comprehensive understanding of the various effects , which are important for the effective use of numa shared memory multiprocessor . the results presented can be used to determine optimal strategies in developing an efficient programming environment for a numa system .\n",
      "['parallel processing', 'nonuniform memory access', 'parallel processing performance', 'analytical models', 'interprocessor communication', 'process scheduling', 'scheduling', 'process synchronization', 'remote memory access', 'network contention', 'memory contention', 'bbn gp1000', 'numa shared memory multiprocessor', 'optimal strategies', 'programming environment', 'performance evaluation', 'multiprocessing systems <eos>']\n",
      "['parallel processing']\n",
      "['numa multiprocessor', 'performance prediction', 'numa multiprocessor systems', 'performance evaluation', 'parallel processing', 'performance evaluation']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['parallel processing', 'parallel processing']\n",
      "\n",
      "\n",
      "a control flow normalization algorithm and its complexity . a single method for normalizing the control flow of programs to facilitate program transformations , program analysis , and automatic parallelization is presented . while previous methods result in programs whose control flowgraphs are reducible , programs normalized by this technique satisfy a stronger condition than reducibility and are therefore simpler in their syntax and structure than with previous methods . in particular , all control flow cycles are normalized into single entry , single exit while loops and all gotos are eliminated . furthermore , the method avoids problems of code replication that are characteristic of node splitting techniques . this restructuring obviates the control dependence graph , since afterwards control dependence relations are manifest in the syntax tree of the program . transformations that effect this normalization are presented , and the complexity of the method is studied .\n",
      "['control flow normalization algorithm', 'complexity', 'automatic parallelization', 'control flowgraphs', 'control flow cycles', 'gotos', 'node splitting techniques', 'control dependence relations', 'syntax tree', 'computational complexity', 'graph theory', 'parallel algorithms', 'structured programming <eos>']\n",
      "['dependence graph', 'automatic parallelization', 'algorithm']\n",
      "['complexity', 'normalization', 'method', 'program transformation', 'automatic parallelization', 'control flow', 'program transformation']\n",
      "3 7 10\n",
      "additional scores from absent units to present :  ['automatic parallelization', 'complexity', 'automatic parallelization']\n",
      "\n",
      "\n",
      "what are race conditions . in shared memory parallel programs that use explicit synchronization , race conditions result when accesses to shared memory are not properly synchronized . race conditions are often considered to be manifestations of bugs , since their presence can cause the program to behave unexpectedly . unfortunately , there has been little agreement in the literature as to precisely what constitutes a race condition . two different notions have been implicitly considered one pertaining to programs intended to be deterministic ( which we call general races ) and the other to nondeterministic programs containing critical sections ( which we call data races ) . however , the differences between general races and data races have not yet been recognized . this paper examines these differences by characterizing races using a formal model and exploring their properties . we show that two variations of each type of race exist feasible general races and data races capture the intuitive notions desired for debugging and apparent races capture less accurate notions implicitly assumed by most dynamic race detection methods . we also show that locating feasible races is an np hard problem , implying that only the apparent races , which are approximations to feasible races , can be detected in practice . the complexity of dynamically locating apparent races depends on the type of synchronization used by the program . apparent races can be exhaustively located efficiently only for weak types of synchronization that are incapable of implementing mutual exclusion . this result has important implications since we argue that debugging general races requires exhaustive race detection and is inherently harder than debugging data races ( which requires only partial race detection ) . programs containing data races can therefore be efficiently debugged by locating certain easily identifiable races . in contrast , programs containing general races require more complex debugging techniques .\n",
      "['race conditions', 'parallel programs', 'critical sections', 'data races', 'debugging', 'nondeterminacy <eos>']\n",
      "['shared memory']\n",
      "['synchronization', 'debugging', 'data races', 'shared memory', 'race conditions', 'theory', 'parallel programming', 'verification']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['debugging', 'data races', 'race conditions']\n",
      "\n",
      "\n",
      "on workload characterization of relational database environments . a relational database workload analyzer ( <unk> ) is developed to characterize the workload in a db2 environment . this is applied to study a production db2 system where a structured query language ( sql ) trace for a two hour interval and an image copy of the database catalog were obtained . the results of the workload study are summarized . the structure and complexity of sql statements , the makeup and run time behavior of transactions queries , and the composition of relations and views are discussed . the results obtained provide the important information needed to build a benchmark workload to evaluate the alternative design tradeoffs of database systems .\n",
      "['relational databases', 'relational database workload analyzer', 'db2 environment', 'structured query language', 'query languages', 'sql', 'database catalog', 'run time behavior', 'views', 'benchmark workload', 'design tradeoffs', 'dp management', 'database theory', 'redwar', 'systems analysis <eos>']\n",
      "['structured query language', 'workload', 'database']\n",
      "['relational database', 'structured query language', 'database', 'workload characterization', 'relational database systems', 'workload characterization', 'database workload']\n",
      "3 7 10\n",
      "additional scores from absent units to present :  ['structured query language', 'structured query language']\n",
      "\n",
      "\n",
      "detecting unsafe error recovery schedules . a mechanism for modeling timing , precedence , and data consistency constraints on concurrently executing processes is presented . the model allows durations and intervals between events to be specified . an algorithm is provided to detect schedules which may be unsafe with respect to the constraints . this work , motivated by the design and validation of autonomous error recovery strategies on the galileo spacecraft , appears to be applicable to a variety of asynchronous real time systems .\n",
      "['unsafe error recovery schedules', 'scheduling', 'modeling timing', 'precedence', 'data consistency constraints', 'concurrently executing processes', 'galileo spacecraft', 'asynchronous real time systems', 'real time systems', 'fault tolerant computing', 'aerospace computing <eos>']\n",
      "['error recovery']\n",
      "['galileo', 'data consistency', 'optimality', 'model checking', 'error control', 'real time systems']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['real time systems']\n",
      "\n",
      "\n",
      "a subexponential bound for linear programming . we present a simple randomized algorithm which solves linear programs with n constraints and d variables in expected o ( nde ( d ln ( n <digit> ) ) <digit> <digit> ) time in the unit cost model ( where we count the number of arithmetic operations on the numbers in the input ) . the expectation is over the internal randomizations performed by the algorithm , and holds for any input . the algorithm is presented in an abstract framework , which facilitates its application to several other related problems . the algorithm has been presented in a previous work by the authors shw , but its analysis and the subexponential complexity bound are new .\n",
      "['linear programming', 'computational geometry', 'combinatorial optimization', 'randomized incremental algorithms <eos>']\n",
      "['randomized algorithms']\n",
      "['linear programming', 'complexity bound', 'subexponential bound', 'subexponential algorithms']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['linear programming']\n",
      "\n",
      "\n",
      "the decoupled simulation model for virtual reality systems . the virtual reality user interface style allows the user to manipulate virtual objects in a 3d environment using 3d input devices . this style is best suited to application areas where traditional two dimensional styles fall short , but the current programming effort required to produce a vr application is somewhat large . we have built a toolkit called mr , which facilitates the development of vr applications . the toolkit provides support for distributed computing , head mounted displays , room geometry , performance monitoring , hand input devices , and sound feedback . in this paper , the architecture of the toolkit is outlined , the programmer ' s view is described , and two simple applications are described .\n",
      "['virtual reality', 'interactive 3d graphics', 'user interface software <eos>']\n",
      "['virtual reality']\n",
      "['simulation', '3d virtual reality', 'virtual reality']\n",
      "1 3 4\n",
      "additional scores from absent units to present :  ['virtual reality', 'virtual reality']\n",
      "\n",
      "\n",
      "estimation and enhancement of real time software reliability through mutation analysis . a simulation based method for obtaining numerical estimates of the reliability of n version , real time software is proposed . an extended stochastic petri net is used to represent the synchronization structure of n versions of the software , where dependencies among versions are modeled through correlated sampling of module execution times . the distributions of execution times are derived from automatically generated test cases that are based on mutation testing . since these test cases are designed to reveal software faults , the associated execution times and reliability estimates are likely to be conservative . experimental results using specifications for nasa ' s planetary lander control software suggest that mutation based testing could hold greater potential for enhancing reliability than the desirable but perhaps unachievable goal of independence among n versions . nevertheless , some support for n version enhancement of high quality , mutation tested code is also offered . mutation analysis could also be valuable in the design of fault tolerant software systems .\n",
      "['real time software reliability', 'software reliability', 'mutation analysis', 'simulation', 'numerical estimates', 'stochastic petri net', 'petri nets', 'synchronization structure', 'dependencies', 'correlated sampling', 'module execution times', 'mutation testing', 'software faults', 'nasa', 'planetary lander control software', 'mutation tested code', 'fault tolerant software systems', 'fault tolerant computing', 'computational complexity <eos>']\n",
      "['petri net']\n",
      "['mutation analysis', 'reliability', 'software reliability', 'fault tolerance', 'real time software', 'real time systems', 'software reliability']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['mutation analysis', 'software reliability', 'software reliability']\n",
      "\n",
      "\n",
      "programming and verifying real time systems by means of the synchronous data flow language lustre . the benefits of using a synchronous data flow language for programming critical real time systems are investigated . these benefits concern ergonomy ( since the dataflow approach meets traditional description tools used in this domain ) and ability to support formal design and verification methods . it is shown , using a simple example , how the language lustre and its associated verification tool <unk> , can be used to design a program , to specify its critical properties , and to verify these properties . as the language lustre and its uses have already been discussed in several papers , emphasis is put on program verification .\n",
      "['real time systems', 'synchronous data flow language', 'data flow language lustre', 'critical real time systems', 'ergonomy', 'dataflow approach', 'traditional description tools', 'formal design', 'verification methods', 'critical properties', 'program verification', 'parallel programming', 'verification tool lesar', 'parallel languages <eos>']\n",
      "['synchronous data flow', 'formal design', 'verification']\n",
      "['lustre', 'dataflow', 'data flow language', 'data flow language', 'real time systems', 'real time systems', 'synchronous languages', 'formal methods', 'verification']\n",
      "3 9 12\n",
      "additional scores from absent units to present :  ['real time systems', 'real time systems']\n",
      "\n",
      "\n",
      "coordinating rule based software processes with esp . esp is a language for modeling rule based software processes that take place in a distributed software development environment . it is based on polis , an abstract coordination model that relies on multiple tuple spaces , i . e . , collections of tuples a la linda . polis extends linda aiming at the specification and coordination of logically distributed systems . esp ( extended shared prolog ) combines the polis mechanisms to deal with concurrency and distribution , with the logic programming language prolog , to deal with rules and deduction . such a combination of a coordination model and a logic language provides a powerful framework in which experiments about rule based software process programming can be performed and evaluated .\n",
      "['software process', 'concurrency', 'logic programming', 'software process modeling', 'rule based programming', 'multiuser programming environment <eos>']\n",
      "['logic programming']\n",
      "['esp', 'concurrency', 'software process', 'rule based software process programming', 'software process modeling', 'rule based systems', 'distributed systems']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['concurrency', 'software process']\n",
      "\n",
      "\n",
      "mutation analysis using mutant schemata . mutation analysis is a powerful technique for assessing and improving the quality of test data used to unit test software . unfortunately , current automated mutation analysis systems suffer from severe performance problems . this paper presents a new method for performing mutation analysis that uses program schemata to encode all mutants for a program into one metaprogram , which is subsequently compiled and run at speeds substantially higher than achieved by previous interpretive systems . preliminary performance improvements of over <digit> % are reported . this method has the additional advantages of being easier to implement than interpretive systems , being simpler to port across a wide range of hardware and software platforms , and using the same compiler and run time support system that is used during development and or deployment .\n",
      "['mutation analysis', 'program schemata', 'fault based testing', 'software testing <eos>']\n",
      "['Present predictions:']\n",
      "['mutant schemata', 'mutation analysis', 'software testing', 'test data generation']\n",
      "1 4 4\n",
      "additional scores from absent units to present :  ['mutation analysis']\n",
      "\n",
      "\n",
      "a weighted nearest neighbor algorithm for learning with symbolic features . in the past , nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values . in such domains , the examples can be treated as points and distance metrics can use standard definitions . in symbolic domains , a more sophisticated treatment of the feature space is required . we introduce a nearest neighbor algorithm for learning in domains with symbolic features . our algorithm calculates distance tables that allow it to produce real valued distances between instances , and attaches weights to the instances to further modify the structure of feature space . we show that this technique produces excellent classification accuracy on three problems that have been studied by machine learning researchers predicting protein secondary structure , identifying dna promoter sequences , and pronouncing english text . direct experimental comparisons with the other learning algorithms show that our nearest neighbor algorithm is comparable or superior in all three domains . in addition , our algorithm has advantages in training speed , simplicity , and perspicuity . we conclude that experimental evidence favors the use and continued development of nearest neighbor algorithms for domains such as the ones studied here .\n",
      "['nearest neighbor', 'text pronunciation', 'exemplar based learning', 'instance based learning', 'protein structure <eos>']\n",
      "['machine learning']\n",
      "['symbolic features', 'learning', 'distance metrics', 'nearest neighbor', 'symbolic computation', 'feature selection']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['nearest neighbor']\n",
      "\n",
      "\n",
      "a conceptual framework for evolving software processes . software processes are complex entities that may last for long periods of time and are carried out through the interaction of humans and computerized tools . they need to continuously evolve in order to cope with different kinds of changes or customizations both in the organization and in the technologies used to support software production activities . in recent years , many software process support technologies have been developed , and have currently been further extended and used in trial projects . moreover , some research prototypes have generated commercial products , that are marketed and currently used in industrial organizations . despite these significant efforts and results , however , there is still little conceptual characterization and assessment of the properties of software processes and related support environments . it is difficult to compare and assess existing approaches . even a common characterization of the problems to be addressed seems to be problematic and difficult to achieve . this is particularly true when we consider the process evolution problem , for which it does not seem that a common view of the issue has been established yet . this paper aims at proposing a conceptual framework to describe and assess flexible and evolving software processes . it is based on the assumption that a software process is composed of two main components a software production process to carry out software production activities , and a software meta process to improve and evolve the whole software process . the general requirements and properties of the process domain are first discussed , and the meta process concept is introduced . then , we discuss several process related concepts and , in particular , the relationship between the meta process and the rest of the software process . methods and technologies needed to support the meta process are highlighted and discussed . finally , we apply the resulting framework to an example , in order to show the potential and expected benefits of the proposed approach .\n",
      "['software process', 'process modeling', 'metaprocess', 'process evolution and improvement <eos>']\n",
      "['software process']\n",
      "['evolving software process', 'meta relationship', 'process evolution', 'software process', 'conceptual framework', 'evolving software process', 'conceptual design', 'software process improvement', 'process and process evolution']\n",
      "1 9 10\n",
      "additional scores from absent units to present :  ['software process', 'software process']\n",
      "\n",
      "\n",
      "a comparison of adaptive wormhole routing algorithms . improvement of message latency and network utilization in torus interconnection networks by increasing adaptivity in wormhole routing algorithms is studied . a recently proposed partially adaptive algorithm and four new fully adaptive routing algorithms are compared with the well known e cube algorithm for uniform , hotspot , and local traffic patterns . our simulations indicate that the partially adaptive north last algorithm , which causes unbalanced traffic in the network , performs worse than the nonadaptive e cube routing algorithm for all three traffic patterns . another result of our study is that the performance does not necessarily improve with full adaptivity . in particular , a commonly discussed fully adaptive routing algorithm , which uses 2n virtual channels per physical channel of a k ary n cube , performs worse than e cube for uniform and hotspot traffic patterns . the other three fully adaptive algorithms , which give priority to messages based on distances traveled , perform much better than the e cube and partially adaptive algorithms for all three traffic patterns . one of the conclusions of this study is that adaptivity , full or partial , is not necessarily a benefit in wormhole routing .\n",
      "['wormhole routing', 'adaptive routing', 'k ary n cubes', 'deadlocks', 'store and forward routing', 'message routing', 'multicomputer networks <eos>']\n",
      "['interconnection networks', 'wormhole routing']\n",
      "['torus', 'message latency', 'adaptive routing', 'adaptive routing']\n",
      "2 4 6\n",
      "additional scores from absent units to present :  ['adaptive routing', 'adaptive routing']\n",
      "\n",
      "\n",
      "random walks in weyl chambers and the decomposition of tensor powers . we consider a class of random walks on a lattice , introduced by gessel and zeilberger , for which the reflection principle can be used to count the number of k step walks between two points which stay within a chamber of a weyl group . we prove three independent results about such reflectable walks first , a classification of all such walks semi second , many determinant formulas for walk numbers and their generating functions semi third , an equality between the walk numbers and the multiplicities of irreducibles in the kth tensor power of certain lie group representations associated to the walk types . our results apply to the defining representations of the classical groups , as well as some spin representations of the orthogonal groups .\n",
      "['random walk', 'tensor power', 'weyl group', 'hyperbolic bessel function', 'representation of lie group <eos>']\n",
      "['reflectable walks']\n",
      "['tensor powers', 'lie group representation', 'weyl group', 'walk types', 'random walks', '<unk>', 'tensor product', 'random walk', '<unk>']\n",
      "1 9 10\n",
      "additional scores from absent units to present :  ['weyl group', 'random walk']\n",
      "\n",
      "\n",
      "a new theory of deadlock free adaptive routing in wormhole networks . the theoretical background for the design of deadlock free adaptive routing <unk> wormhole networks is developed . the author proposes some basic definitions and <unk> . these create the conditions to verify that an adaptive algorithm <unk> free , even when there are cycles in the channel dependency graph . two <unk> are also proposed . the first supplies algorithms with a high degree offreedom , without increasing the number of physical channels . the second methodology <unk> for the design of fault tolerant algorithms . some examples are given to show theapplication of the methodologies . simulations show the performance improvement thatcan be achieved by designing the routing algorithms with the new theory .\n",
      "['wormhole networks', 'adaptive algorithm', 'fault tolerant algorithms', 'performance', 'routing algorithms', 'channeldependency graph', 'graph theory', 'fault tolerant computing', 'telecommunication network routing', 'index termsdeadlock free adaptive routing', 'concurrency control', 'message passing <eos>']\n",
      "['adaptive routing']\n",
      "['wormhole networks', 'routing', 'adaptive routing', 'fault tolerance', 'deadlock free', 'wormhole switching', 'deadlock free routing', 'adaptive routing', 'interconnection networks']\n",
      "1 9 10\n",
      "additional scores from absent units to present :  ['wormhole networks']\n",
      "\n",
      "\n",
      "theory and practice of vector quantizers trained on small training sets . examines how the performance of a memoryless vector quantizer changes as a function of its training set size . specifically , the authors study how well the training set distortion predicts test distortion when the training set is a randomly drawn subset of blocks from the test or training image ( s ) . using the vapnik chervonenkis ( vc ) dimension , the authors derive formal bounds for the difference of test and training distortion of vector quantizer codebooks . the authors then describe extensive empirical simulations that test these bounds for a variety of codebook sizes and vector dimensions , and give practical suggestions for determining the training set size necessary to achieve good generalization from a codebook . the authors conclude that , by using training sets comprising only a small fraction of the available data , one can produce results that are close to the results obtainable when all available data are used .\n",
      "['small training sets', 'memoryless vector quantizer', 'training set distortion', 'test distortion', 'training image', 'formal bounds', 'vector quantizer codebooks', 'empirical simulations', 'vector quantisation', 'statistics', 'image coding', 'learning systems', 'vapnik chervonenkis dimension <eos>']\n",
      "['Present predictions:']\n",
      "['small training sets', 'distortion test', 'training set size', 'vector quantizer', 'vector quantization', 'image processing']\n",
      "1 6 6\n",
      "additional scores from absent units to present :  ['small training sets']\n",
      "\n",
      "\n",
      "the elusive atomic register . we present a construction of a single writer , multiple reader atomic register from single writer , single reader atomic registers . the complexity of our construction is asymptotically optimal o ( m2 shared single writer , single reader safe bits are required to construct a single writer , m reader , n bit atomic register .\n",
      "['atomic register', 'linearizability', 'wait free synchronization <eos>']\n",
      "['Present predictions:']\n",
      "['complexity', 'atomic register']\n",
      "1 2 2\n",
      "additional scores from absent units to present :  ['atomic register']\n",
      "\n",
      "\n",
      "precise and efficient groundness analysis for logic programs . we show how precise groundness information can be extracted from logic programs . the idea is to use abstract interpretation with boolean functions as approximations to groundness dependencies between variables . this idea is not new , and different classes of boolean functions have been used . we argue , however , that one class , the positive functions , is more suitable than others . positive boolean functions have a certain property which we ( inspired by a . langen ) call condensation . this property allows for rapid computation of groundness information .\n",
      "['groundness analysis', 'abstract interpretation', 'condensation', 'propositional logic <eos>']\n",
      "['boolean functions', 'groundness', 'analysis']\n",
      "['logic programming', 'analysis', 'use', 'call condensation', 'abstract interpretation', 'groundness analysis']\n",
      "3 6 9\n",
      "additional scores from absent units to present :  ['abstract interpretation', 'groundness analysis']\n",
      "\n",
      "\n",
      "semantics of constraint logic programs with optimization . many applications of constraint logic programming ( clp ) languages require not only testing if a set of constraints is satisfiable , but also finding the optimal solution which satisfies them . unfortunately , the standard declarative semantics for clp languages does not consider optimization but only constraint satisfaction . here we give a model theoretic semantics for optimization , which is a simple extension of the standard semantics , and a corresponding operational semantics , which may be efficiently implemented .\n",
      "['semantics', 'constraint logic programming <eos>']\n",
      "['applications', 'testing']\n",
      "['optimization', 'semantics', 'logic programming', 'applications', 'model', 'applications', 'constraint logic programming']\n",
      "2 7 9\n",
      "additional scores from absent units to present :  ['semantics']\n",
      "\n",
      "\n",
      "dynamic nurbs with geometric constraints for interactive sculpting . this article develops a dynamic generalization of the nonuniform rational b spline ( nurbs ) model . nurbs have become a defacto standard in commercial modeling systems because of their power to represent free form shapes as well as common analytic shapes . to date , however , they have been viewed as purely geometric primitives that require the user to manually adjust multiple control points and associated weights in order to design shapes . dynamic nurbs , or d nurbs , are physics based models that incorporate mass distributions , internal deformation energies , and other physical quantities into the popular nurbs geometric substrate . using d nurbs , a modeler can interactively sculpt curves and surfaces and design complex shapes to required specifications not only in the traditional indirect fashion , by adjusting control points and weights , but also through direct physical manipulation , by applying simulated forces and local and global shape constraints . d nurbs move and deform in a physically intuitive manner in response to the user ' s direct manipulations . their dynamic behavior results from the numerical integration of a set of nonlinear differential equations that automatically evolve the control points and weights in response to the applied forces and constraints . to derive these equations , we employ lagrangian mechanics and a finite element like discretization . our approach supports the trimming of d nurbs surfaces using d nurbs curves . we demonstrate d nurbs models and constraints in applications including the rounding of solids , optimal surface fitting to unstructured data , surface design from cross sections , and free form deformation . we also introduce a new technique for 2d shape metamorphosis using constrained d nurbs surfaces .\n",
      "['dynamics', 'nurbs', 'constraints', 'finite elements', 'trimming', 'free form deformation', 'shape metamorphosis', 'solid rounding', 'cross sectional shape design', 'optimal curve and surface fitting', 'deformable models', 'cagd <eos>']\n",
      "['Present predictions:']\n",
      "['interactive sculpting', 'nurbs', 'geometric constraints', 'dynamic nurbs', 'dynamic nurbs', 'algorithms']\n",
      "1 6 6\n",
      "additional scores from absent units to present :  ['nurbs']\n",
      "\n",
      "\n",
      "complexity restricted advice functions . the authors consider uniform subclasses of the nonuniform complexity classes defined by karp and lipton l ' <unk> . math . , <digit> ( <digit> ) via the notion of advice functions . these subclasses are obtained by restricting the complexity of computing correct advice . also , the effect of allowing advice functions of limited complexity to depend on the input rather than on the input ' s length is investigated . among other results , using the notions described above , new characterizations of ( a ) np np cap sparse , ( b ) np with a restricted access to an np oracle , and ( c ) the odd levels of the boolean hierarchy are given . as a consequence , it is shown that every set that is nondeterministically truth table reducible to sat in the sense of rich j . comput . system sci . , <digit> ( <digit> ) , pp . <digit> <digit> is already deterministically truth table reducible to sat . furthermore , it turns out that the np reduction classes of bounded versions of this reducibility coincide with the odd levels of the boolean hierarchy .\n",
      "['nonuniform complexity classes', 'boolean hierarchy', 'truth table reducibility', 'optimization functions', 'restricted oracle access', 'relativization', 'sparse np sets', 'advice classes <eos>']\n",
      "['boolean hierarchy', 'advice functions', 'restricted advice']\n",
      "['complexity', 'complexity classes', 'np oracle', 'nonuniform complexity classes', 'np completeness']\n",
      "3 5 8\n",
      "additional scores from absent units to present :  ['nonuniform complexity classes']\n",
      "\n",
      "\n",
      "generating linear extensions fast . one of the most important sets associated with a poset cal p is its set of linear extensions , e ( cal p ) . this paper presents an algorithm to generate all of the linear extensions of a poset in constant amortized time , that is , in time o ( e ( cp ) ) , where e ( cp ) . the fastest previously known algorithm for generating the linear extensions of a poset runs in time o ( n cdot e ( cp ) ) , where n is the number of elements of the poset . the algorithm presented here is the first constant amortized time algorithm for generating a naturally defined class of combinatorial objects for which the corresponding counting problem is p complete . furthermore , it is shown that linear extensions can be generated in constant amortized time where each extension differs from its predecessor by one or two adjacent transpositions . the algorithm is practical and can be modified to count linear extensions efficiently and to compute p ( x < y ) , for all pairs x , y , in time o ( n <digit> e ( cal p ) ) .\n",
      "['linear extension', 'poset', 'transposition', 'combinatorial gray code <eos>']\n",
      "['Present predictions:']\n",
      "['poset', 'linear extensions', 'amortized time algorithms']\n",
      "1 3 3\n",
      "additional scores from absent units to present :  ['poset']\n",
      "\n",
      "\n",
      "computational complexity of sparse rational interpolation . the authors analyze the computational complexity of sparse rational interpolation , and give the first deterministic algorithm for this problem with singly exponential bounds on the number of arithmetic operations .\n",
      "['computational complexity', 'interpolation', 'arithmetic operations', 'sparse rational functions <eos>']\n",
      "['rational interpolation']\n",
      "['sparse rational interpolation', 'computational complexity', 'sparse interpolation', 'rational interpolation', '<unk>']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['computational complexity']\n",
      "\n",
      "\n",
      "tight upper and lower bounds on the path length of binary trees . the external path length of a tree t is the sum of the lengths of the paths from the root to each external node . the maximal path length difference , delta , is the difference between the lengths of the longest and shortest such paths . tight lower and upper bounds are proved on the external path length of binary trees with n external nodes and maximal path length difference delta is prescribed . in particular , an upper bound is given that , for each value of delta , can be exactly achieved for infinitely many values of n . this improves on the previously known upper bound that could only be achieved up to a factor proportional to n . an elementary proof of the known upper bound is also presented as a preliminary result . moreover , a lower bound is proved that can be exactly achieved for each value of n and delta leq n <digit> .\n",
      "['path length', 'binary search trees <eos>']\n",
      "['Present predictions:']\n",
      "['binary tree', 'path length', 'path length']\n",
      "1 3 3\n",
      "additional scores from absent units to present :  ['path length', 'path length']\n",
      "\n",
      "\n",
      "piecewise linear interpolation between polygonal slices . in this paper we present a new technique for piecewise linear surface reconstruction from a series of parallel polygonal cross sections . this is an important problem in medical imaging , surface reconstruction from topographic data , and other applications . we reduce the problem , as in most previous works , to a series of problems of piecewise linear interpolation between each pair of successive slices . our algorithm uses a partial curve matching technique for matching parts of the contours , an optimal triangulation of <digit> d polygons for resolving the unmatched parts , and a minimum spanning tree heuristic for interpolating between non simply connected regions . unlike previous attempts at solving this problem , our algorithm seems to handle successfully any kind of data . it allows multiple contours in each slice , with any hierarchy of contour nesting , and avoids the introduction of counter intuitive bridges between contours , proposed in some earlier papers to handle interpolation between multiply connected regions . experimental results on various complex examples , involving actual medical imaging data , are presented , and show the good and robust performance of our algorithm .\n",
      "['surface reconstruction', 'curve matching', 'triangulation', 'dynamic programming', 'polyhedra', 'slice interpolation', 'surface fitting', 'branching surfaces', 'tiling', 'geometric hashing <eos>']\n",
      "['partial curve matching']\n",
      "['polygonal slices', 'interpolation', 'minimum spanning tree', 'surface reconstruction', 'piecewise linear interpolation', 'polygonal approximation', 'piecewise linear interpolation', 'parallel computing']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['surface reconstruction']\n",
      "\n",
      "\n",
      "improvements to graph coloring register allocation . we describe two improvements to chaitin style graph coloring register allocators . the first , optimistic coloring , uses a stronger heuristic to find a k coloring for the interference graph . the second extends chaitin ' s treatment of rematerialization to handle a larger class of values . these techniques are complementary . optimistic coloring decreases the number of procedures that require spill code and reduces the amount of spill code when spilling is unavoidable . rematerialization lowers the cost of spilling some values . this paper describes both of the techniques and our experience building and using register allocators that incorporate them . it provides a detailed description of optimistic coloring and rematerialization . it presents experimental data to show the performance of several versions of the register allocator on a suite of fortran programs . it discusses several insights that we discovered only after repeated implementation of these allocators .\n",
      "['graph coloring', 'register allocation', 'code generation <eos>']\n",
      "['register allocation']\n",
      "['optimistic coloring', 'register allocation', 'graph coloring', 'algorithms']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['register allocation', 'register allocation', 'graph coloring']\n",
      "\n",
      "\n",
      "model checking and modular verification . we describe a framework for compositional verification of finite state processes . the framework is based on two ideas a subset of the logic ctl for which satisfaction is preserved under composition , and a preorder on structures which captures the relation between a component and a system containing the component . satisfaction of a formula in the logic corresponds to being below a particular structure ( a tableau for the formula ) in the preorder . we show how to do assume guarantee style reasoning within this framework . additionally , we demonstrate efficient methods for model checking in the logic and for checking the preorder in several special cases . we have implemented a system based on these methods , and we use it to give a compositional verification of a cpu controller .\n",
      "['model checking', 'ctl', 'formal verification', 'moore machines', 'temporal logics', 'computer aided verification <eos>']\n",
      "['Present predictions:']\n",
      "['compositional verification', 'verification', 'model checking', 'finite state automata', 'formal methods', 'verification']\n",
      "1 6 6\n",
      "additional scores from absent units to present :  ['model checking']\n",
      "\n",
      "\n",
      "faster approximation algorithms for the unit capacity concurrent flow problem with applications to routing and finding sparse cuts . this paper describes new algorithms for approximately solving the concurrent multicommodity flow problem with uniform capacities . these algorithms are much faster than algorithms discovered previously . besides being an important problem in its own right , the uniform capacity concurrent flow problem has many interesting applications . leighton and rao used uniform capacity concurrent flow to find an approximately sparsest cut in a graph and thereby approximately solve a wide variety of graph problems , including minimum feedback arc set , minimum cut linear arrangement , and minimum area layout . however , their method appeared to be impractical as it required solving a large linear program . this paper shows that their method might be practical by giving an o ( m <digit> log m ) expected time randomized algorithm for their concurrent flow problem on an m edge graph . raghavan and thompson used uniform capacity concurrent flow to solve approximately a channel width minimization problem in very large scale integration . an randomized algorithm and an o ( k min n , k ( m n log n ) log k ) deterministic algorithm is given for this problem when the channel width is omega ( log n ) , where k denotes the number of wires to be routed in an n node , m edge network .\n",
      "['approximation', 'concurrent flow', 'multicommodity flow', 'vlsi routing', 'graph separators <eos>']\n",
      "['concurrent flow', 'multicommodity flow']\n",
      "['sparse cuts', 'routing', 'channel width', 'unit capacity', 'concurrent flow', 'capacity minimization']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['concurrent flow', 'concurrent flow']\n",
      "\n",
      "\n",
      "using genetic algorithms for concept learning . in this article , we explore the use of genetic algorithms ( gas ) as a key element in the design and implementation of robust concept learning systems . we describe and evaluate a ga based system called gabil that continually learns and refines concept classification rules from its interaction with the environment . the use of gas is motivated by recent studies showing the effects of various forms of bias built into different concept learning systems , resulting in systems that perform well on certain concept classes ( generally , those well matched to the biases ) and poorly on others . by incorporating a ga as the underlying adaptive search mechanism , we are able to construct a concept learning system that has a simple , unified architecture with several important features . first , the system is surprisingly robust even with minimal bias . second , the system can be easily extended to incorporate traditional forms of bias found in other concept learning systems . finally , the architecture of the system encourages explicit representation of such biases and , as a result , provides for an important additional feature the ability to dynamically adjust system bias . the viability of this approach is illustrated by comparing the performance of gabil with that of four other more traditional concept learners ( <unk> , c4 . <digit> , <unk> , and <unk> ) on a variety of target concepts . we conclude with some observations about the merits of this approach and about possible extensions .\n",
      "['genetic algorithms', 'concept learning', 'bias adjustment <eos>']\n",
      "['adaptive search']\n",
      "['concept learning', 'genetic algorithms', 'evolutionary computation']\n",
      "1 3 4\n",
      "additional scores from absent units to present :  ['concept learning', 'genetic algorithms']\n",
      "\n",
      "\n",
      "factorization of matrix polynomials with symmetries . an n times n matrix polynomial l ( lambda ) ( with real or complex coefficients ) is called self adjoint if factorizations of selfadjoint and symmetric matrix polynomials of the form are studied , where d is a constant matrix and m ( lambda ) is a matrix polynomial . in particular , the minimal possible size of d is described in terms of the elementary divisors of l ( lambda ) and ( sometimes ) signature of the hermitian values of l ( lambda ) .\n",
      "['factorization', 'matrix polynomials', 'symmetries <eos>']\n",
      "['symmetric matrix']\n",
      "['symmetries', 'factorization', 'self adjoint factorizations', 'matrix polynomial', '<unk>', '<unk>', 'matrix polynomial factorization', 'hermitian matrix', '<unk>']\n",
      "1 9 10\n",
      "additional scores from absent units to present :  ['factorization']\n",
      "\n",
      "\n",
      "cache interference phenomena . the impact of cache interferences on program performance ( particularly numerical codes , which heavily use the memory hierarchy ) remains unknown . the general knowledge is that cache interferences are highly irregular , in terms of occurrence and intensity . in this paper , the different types of cache interferences that can occur in numerical loop nests are identified . an analytical method is developed for detecting the occurrence of interferences and , more important , for computing the number of cache misses due to interferences . simulations and experiments on real machines show that the model is generally accurate and that most interference phenomena are captured . experiments also show that cache interferences can be intense and frequent . certain parameters such as array base addresses or dimensions can have a strong impact on the occurrence of interferences . modifying these parameters only can induce global execution time variations of <digit> % and more . applications of these modeling techniques are numerous and range from performance evaluation and prediction to enhancement of data locality optimizations techniques .\n",
      "['numerical codes', 'modeling', 'performance evaluation', 'data locality', 'cache interferences or conflicts <eos>']\n",
      "['interference phenomena']\n",
      "['program performance', 'cache interferences', 'cache memories', 'performance evaluation']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['performance evaluation']\n",
      "\n",
      "\n",
      "a framework for expressing the relationships between multiple views in requirements specification . composite systems are generally comprised of heterogeneous components whose specifications are developed by many development participants . the requirements of such systems are invariably elicited from multiple perspectives that overlap , complement , and contradict each other . furthermore , these requirements are generally developed and specified using multiple methods and notations , respectively . it is therefore necessary to express and check the relationships between the resultant specification fragments . we deploy multiple viewpoints that hold partial requirements specifications , described and developed using different representation schemes and development strategies . we discuss the notion of inter viewpoint communication in the context of this viewpoints framework , and propose a general model for viewpoint interaction and integration . we elaborate on some of the requirements for expressing and enacting inter viewpoint relationships the vehicles for consistency checking and inconsistency management . finally , though we use simple fragments of the requirements specification method core to illustrate various components of our work , we also outline a number of larger case studies that we have used to validate our framework . our computer based viewpoints support environment , the viewer , is also briefly described .\n",
      "['multiple views', 'requirements specification', 'heterogeneous components', 'multiple viewpoints', 'partial requirements specifications', 'inter viewpoint communication', 'viewpoints framework', 'consistency checking', 'inconsistency management', 'requirements specification method', 'core', 'computer based viewpoints support', 'the viewer', 'formal specification <eos>']\n",
      "['requirements']\n",
      "['requirements specification', 'inter viewpoint communication', 'inconsistency management', 'multiple views', 'requirements engineering']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['requirements specification', 'inter viewpoint communication', 'inconsistency management', 'multiple views']\n",
      "\n",
      "\n",
      "prototyping a process monitoring experiment . features are often the basic unit of development for a very large software system and represent long term efforts , spanning up to several years from inception to actual use . developing an experiment to monitor ( by means of sampling ) such lengthy processes requires a great deal of care in order to minimize casts and to maximize benefits . just as prototyping is often a necessary auxiliary step in a large scale , long term development effort , so , too , is prototyping a necessary step in the development of a large scale , long term process monitoring experiment . therefore , we have prototyped our experiment using a representative process and reconstructed data from a large and rich feature development . this approach has yielded three interesting sets of results . first , we reconstructed a <digit> month time diary for the lead engineer of a feature composed of both hardware and software . these data represent the daily state ( where the lead engineer spent the majority of his time ) for a complete cycle of the development process . second , we found that we needed to modify our experimental design . our initial set of states did not represent the data as well as we had hoped . this is exemplified by the fact that the other category is too large . finally , the data provide evidence for both a waterfall view and an interactive , cyclic view of software development . we conclude that the prototyping effort is a necessary part of developing and installing any large scale process monitoring experiment .\n",
      "['prototyping', 'process monitoring experiment', 'very large software system', 'software', 'long term process monitoring', 'hardware', 'experimental design', 'waterfall view', 'software development', 'large scale process monitoring experiment', 'computerised monitoring', 'process computer control', 'interactive cyclic view', 'software prototyping', 'large scale long term development <eos>']\n",
      "['Present predictions:']\n",
      "['prototyping', 'process monitoring', 'process monitoring', 'software process monitoring', 'feature extraction']\n",
      "1 5 5\n",
      "additional scores from absent units to present :  ['prototyping']\n",
      "\n",
      "\n",
      "serial array time slot interchangers and optical implementations . we consider time slot interchangers ( tsis ) which are built from <digit> spl times <digit> exchange switches and delays and which are useful for time division multiplexed ( tdm ) systems in telecommunications and pipelined systems such as time multiplexed optical multiprocessors . we formulate a general method for constructing tsis based on multistage interconnection networks in the space domain via space to time mapping . two types of tsis , time slot <unk> and time slot sorters , are considered . we review the time slot permuter based on the benes network , and obtain the spl <unk> tilde time slot permuter based on the bit controlled , self routing spl lambda permutation network . the time slot sorter , s sub n , is obtained from the batcher spatial sorting network . the generalized lambda time slot permuter spl <unk> <unk> q is obtained , in an algorithmic approach , by combining the idea of the spl <unk> tilde time slot permuter and q way bitonic decomposition ( q <digit> sup q ) . the numbers of switches , control complexities , and frame delays of these architectures are compared , and the problem of crosstalk in optical implementation is discussed . it is shown that control complexity can be traded against the number of switches .\n",
      "['serial array time slot interchangers', 'optical implementations', 'exchange switches', 'time division multiplexing', 'telecommunications', 'pipelined systems', 'time multiplexed optical multiprocessors', 'multistage interconnection networks', 'space to time mapping', 'time slot sorters', 'time slot permuters', 'benes network', 'self routing spl lambda permutation network', 'batcher spatial sorting network', 'bitonic decomposition', 'control complexity', 'crosstalk', 'optical information processing', 'logic arrays', 'time division multiplexed systems', 'optical interconnections', 'multiprocessor interconnection networks <eos>']\n",
      "['benes network', 'optical multiprocessors']\n",
      "['crosstalk', 'time slot interchangers', 'multistage interconnection networks', 'time slot permuter', 'time slot permuter', 'optical communication']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['crosstalk', 'multistage interconnection networks']\n",
      "\n",
      "\n",
      "request combining in multiprocessors with arbitrary interconnection networks . several techniques have been proposed to allow parallel access to a shared <unk> by combining requests . they have one or more of the following attributes requirements for a priori knowledge of the request to combine , restrictions on the <unk> messages in the network , or the use of sophisticated interconnection network nodes . we present a new method of combining requests that does not have the <unk> . we obtain this new method for request combining by developing <unk> scheme for the existing methods of request combining . this <unk> is facilitated by separating the request combining process into a two <unk> determining the combining set , which is the set of requests that participate ina combined access and distributing the results of the combined access to the <unk> the combining set . the classification of combining strategies is based upon <unk> component , processor elements , or interconnection network performs each ofthese tasks . our approach , which uses the interconnection network to establish <unk> set and the processor elements to distribute the results , lies in an <unk> of the design space . we also present simulation results to assess the benefits of theproposed approach .\n",
      "['multiprocessors', 'arbitrary interconnection networks', 'parallel access', 'combining set', 'combining strategies', 'processor elements', 'design space', 'classification scheme', 'index termsmultiprocessor interconnection networks', 'simulationresults', 'parallel architectures', 'virtual machines', 'shared memory systems', 'shared memory location', 'hot spots', 'message routing', 'message passing <eos>']\n",
      "['interconnection networks']\n",
      "['multiprocessors', 'request combining', 'parallel computing', 'interconnection networks', 'distributed systems']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['multiprocessors']\n",
      "\n",
      "\n",
      "fault tolerant routing in mesh architectures . it is important for a distributed computing system to be able to route messages <unk> faulty links or nodes may be present . we present a fault tolerant routingalgorithm that assures the delivery of every message as long as there is a path <unk> source and destination . the algorithm works on many common mesh <unk> as the torus and hexagonal mesh . the proposed scheme can also detect <unk> of a path between a pair of nodes in a finite amount of time . moreover , <unk> requires each node in the system to know only the state ( faulty or not ) of eachof its own links . the performance of the routing scheme is simulated for both square <unk> meshes while varying the physical distribution of faulty components . it <unk> that a shortest path between the source and destination of each message <unk> with a high probability , and , if a path exists , it is usually found very quickly .\n",
      "['fault tolerant routing', 'mesh architectures', 'source', 'destination', 'torus', 'hexagonal mesh', 'hexagonal meshes', 'high probability', 'index termsmessage passing', 'parallel architectures', 'distributedcomputing system', 'network routing', 'software reliability', 'fault tolerant computing', 'message routing', 'fault tolerant routing algorithm', 'square meshes', 'parallelalgorithms', 'routing scheme performance <eos>']\n",
      "['hexagonal mesh', 'torus', 'distributed computing']\n",
      "['mesh architecture', 'routing', 'fault tolerance', 'mesh routing', 'fault tolerant routing', 'distributed computing']\n",
      "3 6 9\n",
      "additional scores from absent units to present :  ['fault tolerant routing']\n",
      "\n",
      "\n",
      "improving generalization with active learning . active learning differs from learning from examples in that the learning algorithm assumes at least some control over what part of the input domain it receives information about . in some situations , active learning is provably more powerful than learning from examples alone , giving better generalization for a fixed number of training examples . in this article , we consider the problem of learning a binary concept in the absence of noise . we describe a formalism for active concept learning called selective sampling and show how it may be approximately implemented by a neural network . in selective sampling , a learner receives distribution information from the environment and queries an oracle on parts of the domain it considers useful . we test our implementation , called an sg network , on three domains and observe significant improvement in generalization .\n",
      "['generalization', 'active learning', 'neural networks', 'queries', 'version space <eos>']\n",
      "['selective sampling', 'neural networks']\n",
      "['generalization', 'active learning', 'learning from examples', 'learning from examples']\n",
      "2 4 6\n",
      "additional scores from absent units to present :  ['generalization', 'active learning']\n",
      "\n",
      "\n",
      "index transformation algorithms in a linear algebra framework . we present a linear algebraic formulation for a class of index transformations such <unk> code encoding and decoding , matrix transpose , bit reversal , vector reversal , shuffles , and other index or dimension permutations . this formulation unifies , simplifies , and can be used to derive algorithms for hypercube multiprocessors . we show how all the widely known properties of gray codes , and some not so well known properties as well , can be derived using this framework . using this framework , we relate hypercube communications algorithms to gauss jordan elimination on a matrix of <digit> ' s and <digit> ' s .\n",
      "['linear algebra framework', 'encoding', 'decoding', 'matrix transpose', 'bit reversal', 'vector reversal', 'shuffles', 'hypercube multiprocessors', 'gray codes', 'hypercube communications algorithms', 'gauss jordan elimination', 'indextransformation algorithms', 'index termslinear algebra', 'hypercube networks', 'gray code encoding <eos>']\n",
      "['vector reversal shuffles']\n",
      "['linear algebra', 'hypercube', 'gauss jordan elimination', 'index transformation', 'matrix transpose']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['gauss jordan elimination', 'matrix transpose']\n",
      "\n",
      "\n",
      "building information systems for mobile environments . it is expected that in the near future , tens of millions of users will have access to distributed information systems through wireless connections . the technical characteristics of the wireless medium and the resulting mobility of both data resources and data consumers raise new challenging questions regarding the development of information systems appropriate for mobile environments . in this paper , we report on the development of such a system . first , we describe the general architecture of the information system and the main considerations of our design . then , based on these considerations , we present our system support for maintaining the consistency of replicated data and for providing transaction schemas that account for the frequent but predictable disconnections , the mobility , and the vulnerability of the wireless environment .\n",
      "['information systems', 'consistency', 'transaction management', 'new applications', 'mobile computing <eos>']\n",
      "['Present predictions:']\n",
      "['mobile environments', 'information systems', 'mobile computing', 'mobile computing', 'wireless networks']\n",
      "1 5 5\n",
      "additional scores from absent units to present :  ['information systems']\n",
      "\n",
      "\n",
      "a framework for the analysis of error in global illumination algorithms . in this paper we identify sources of error in global illumination algorithms and derive bounds for each distinct category . errors arise from three sources inaccuracies in the boundary data , discretization , and computation . boundary data consists of surface geometry , reflectance functions , and emission functions , all of which may be perturbed by errors in measurement or simulation , or by simplifications made for computational efficiency . discretization error is introduced by replacing the continuous radiative transfer equation with a finite dimensional linear system , usually by means of boundary elements and a corresponding projection method . finally , computational errors perturb the finite dimensional linear system through imprecise form factors , inner products , visibility , etc . , as well as by halting iterative solvers after a finite number of steps . using the error taxonomy introduced in the paper we examine existing global illumination algorithms and suggest new avenues of research .\n",
      "['global illumination', 'discretization', 'reflectance functions', 'boundary elements', 'projection methods', 'radiosity', 'linear operators', 'error bounds <eos>']\n",
      "['Present predictions:']\n",
      "['global illumination', 'global illumination', 'global illumination', 'finite dimensional linear systems', 'error analysis', 'error analysis']\n",
      "1 6 6\n",
      "additional scores from absent units to present :  ['global illumination', 'global illumination', 'global illumination']\n",
      "\n",
      "\n",
      "spreadsheets for images . we describe a data visualization system based on spreadsheets . cells in our spreadsheet contain graphical objects such as images , volumes , or movies . cells may also contain widgets such as buttons , sliders , or curve editors . objects are displayed in miniature inside each cell . formulas for cells are written in a general purpose programming language ( tcl ) augmented with operators for array manipulation , image processing , and rendering . compared to flow chart visualization systems , spreadsheets are more expressive , morescalable , and easier to program . compared to conventional numerical spreadsheets , spreadsheets for images pose several unique design problems larger formulas , longer computation times , and more complicated intercelldependencies . in response to these problems , we have extended the spreadsheet paradigm in three ways formulas can display their results anywhere in the spreadsheet , cells can be selectively disabled , and multiple cells can be edited at once . we discuss these extensions and their implications , and we also point out some unexpected uses for our spreadsheets as a visual database browser , as a graphical user interface builder , as a smart clipboard for the desktop , and as a presentation tool .\n",
      "['spreadsheets', 'data visualization', 'flow charts', 'user interfaces', 'visual programming languages <eos>']\n",
      "['flow visualization', 'visualization']\n",
      "['spreadsheets', 'spreadsheet', 'image processing', 'design']\n",
      "2 4 6\n",
      "additional scores from absent units to present :  ['spreadsheets']\n",
      "\n",
      "\n",
      "generalization of lambert ' ' s reflectance model . lambert ' s model for body reflection is widely used in computer graphics . it is used extensively by rendering techniques such as radiosity and ray tracing . for several real world objects , however , lambert ' s model can prove to be a very inaccurate approximation to the body reflectance . while the brightness of a lambertian surface is independent of viewing direction , that of a rough surface increases as the viewing direction approaches the light source direction . in this paper , a comprehensive model is developed that predicts body reflectance from rough surfaces . the surface is modeled as a collection of lambertian facets . it is shown that such a surface is inherently non lambertian due to the foreshortening of the surface facets . further , the model accounts for complex geometric and radiometric phenomena such as masking , shadowing , and interreflections between facets . several experiments have been conducted on samples of rough diffuse surfaces , such as , plaster , sand , clay , and cloth . all these surface demonstrate significant deviation from lambertian behavior . the reflectance measurements obtained are in strong agreement with the reflectance predicted by the model .\n",
      "['reflection models', \"lambert ' s model\", 'rough surfaces', 'brdf', 'moon reflectance <eos>']\n",
      "['reflectance model']\n",
      "['reflectance', \"lambert ' s model\", 'ray tracing', 'surface rendering']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  [\"lambert ' s model\"]\n",
      "\n",
      "\n",
      "multiresolution curves . we describe a multiresolution curve representation , based on wavelets , that conveniently supports a variety of operations smoothing a curve editing the overall form of a curve while preserving its details and approximating a curve within any given error tolerance for scan conversion . we present methods to support continuous levels of smoothing as well as direct manipulation of an arbitrary portion of the curve the control points , as well as the discrete nature of the underlying hierarchical representation , can be hidden from the user . the multiresolution representation requires no extra storage beyond that of the original control points , and the algorithms using the representation are both simple and fast .\n",
      "['wavelets', 'curve editing', 'scan conversion', 'direct manipulation', 'curve fitting', 'curve compression', 'curve smoothing <eos>']\n",
      "['curve editing']\n",
      "['wavelets', 'multiresolution curve editing', 'multiresolution analysis', 'curve editing', 'curve editing']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['curve editing', 'wavelets', 'curve editing', 'curve editing']\n",
      "\n",
      "\n",
      "a graphical interval logic for specifying concurrent systems . this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems . experience has shown that most software engineers find standard temporal logics difficult to understand and use . the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use . to illustrate the use of the graphical logic , the article provides some specifications for an elevator system and proves several properties of the specifications . the article also describes the tool set and the implementation .\n",
      "['graphical interval logic', 'concurrent systems', 'formal specifications', 'temporal logic', 'automated proof checking', 'timing diagrams', 'visual languages <eos>']\n",
      "['temporal logic']\n",
      "['concurrent systems', 'verification', 'graphical interval logic', 'concurrent systems', 'graphical models', 'software engineering', 'formal specification', 'verification']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['concurrent systems', 'graphical interval logic', 'concurrent systems']\n",
      "\n",
      "\n",
      "extending a graphical toolkit for two handed interaction . multimodal interaction combines input from multiple sensors such as pointing devices or speech recognition systems , in order to achieve more fluid and natural interaction . two handed interaction has been used recently to enrich graphical interaction . building applications that use such combined interaction requires new software techniques and frameworks . using additional devices means that user interface toolkits must be more flexible with regard to input devices and event types . the possibility of parallel interactions must also be taken into account , with consequences on the structure of toolkits . finally , frameworks must be provided for the combination of events and status of several devices . this paper reports on the extensions we made to the direct manipulation interface toolkit whizz in order to experiment two handed interaction . these extensions range from structural adaptations of the toolkit to new techniques for specifying the time dependent fusion of events .\n",
      "['graphical toolkit', 'two handed interaction', 'multimodal interaction', 'direct manipulation', 'interaction styles <eos>']\n",
      "['speech recognition', 'multimodal interaction', 'two handed interaction']\n",
      "['graphical toolkit', 'human computer interaction', 'interaction design', 'multimodal interaction']\n",
      "3 4 7\n",
      "additional scores from absent units to present :  ['multimodal interaction', 'graphical toolkit', 'multimodal interaction']\n",
      "\n",
      "\n",
      "reducing memory traffic with cregs . array and pointer references are often ambiguous in that compile time analysis can not always determine if distinct references are to the same object . ambiguously aliased objects are not allocated to registers by conventional compilers due to the cost of the loads and stores required to keep register copies consistent with memory and each other . there are several hardware and software strategies that can be used to solve the ambiguous alias problem we have implemented one such scheme called cregs in a compiler and instruction level simulator . we present a modification to briggs ' optimistic coloring algorithm that allows us to allocate local and parameter arrays to cregs . the cregs register file operation and instruction set modifications required to implement this scheme are discussed . underlying hardware issues such as pipeline impact and chip area are briefly discussed . several benchmarks are compared in terms of dynamic instructions executed for two creg set sizes . the measured reduction in memory operations is significant , averaging <digit> % for the benchmarks shown .\n",
      "['cregs', 'ambiguous alias', 'graph coloring', 'register allocation', 'live range <eos>']\n",
      "['Present predictions:']\n",
      "['cregs', 'analysis', 'instruction', 'memory traffic', 'memory traffic', 'performance']\n",
      "1 6 6\n",
      "additional scores from absent units to present :  ['cregs']\n",
      "\n",
      "\n",
      "data relocation and prefetching for programs with large data sets . numerical applications frequently contain nested loop structures that process large arrays of data . the execution of these loop structures often produces memory reference patterns that poorly utilize data caches . limited associativity and cache capacity result in cache conflict misses . also , non unit stride access patterns can cause low utilization of cache lines . data copying has been proposed and investigated in order to reduce cache conflict misses , but this technique has a high execution overhead since it performs the copy operations entirely in software . we propose a combined hardware and software technique called data relocation and prefetching which eliminates much of the overhead of data copying through the use of special hardware . furthermore , by relocating the data while performing software prefetching , the overhead of copying the data can be reduced further . experimental results for data relocation and prefetching are encouraging and show a large improvement in cache performance .\n",
      "['data relocation', 'cache conflicts', 'data copying', 'software prefetching', 'program optimization <eos>']\n",
      "['software prefetching', 'cache conflict']\n",
      "['large data sets', 'prefetching', 'data relocation', 'data copying', 'cache memories']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['data relocation', 'data copying']\n",
      "\n",
      "\n",
      "an interaction engine for rich hypertexts . in semantically rich hypertexts it is attractive to enable presentation of a network of nodes and link at different levels of abstraction . it is also important that the user can interact with the hypertext using a command repertoire that reflects the chosen abstraction level . based on a characterization of rich hypertext we introduce the concept of an interaction engine that governs the separation between internal hypertext representation and external screen presentation . this separation is the key principle of the hyperpro system . the hyperpro interaction engine is based on simple rules for presentation , interpretation of events , and menu set up . much of the power of the interaction engine framework comes from the organization of these rules relative to the type of hierarchy of nodes and links , and relative to a hierarchy of so called interaction schemes . the primary application domain discussed in the paper is program development and program documentation .\n",
      "['interaction engine', 'program development', 'tailorability', 'event control', 'aggregated views <eos>']\n",
      "['hypertext', 'documentation']\n",
      "['rich hypertexts', 'hypertext', 'user', 'program documentation', 'interaction engine', 'user interface']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['interaction engine']\n",
      "\n",
      "\n",
      "tight bounds on oblivious chaining . the chaining problem is defined as follows . given values the chaining problem appears as a subproblem in many contexts . there are known algorithms that solve the chaining problem on crcw prams in o ( alpha ( n ) ) time , where alpha ( n ) is the inverse of ackerman ' s function , and is a very slowly growing function . the author studies a class of algorithms ( called oblivious algorithms ) for this problem . a simple oblivious chaining algorithm running in o ( alpha ( n ) ) time is presented . more importantly , the optimality of the algorithm is demonstrated by showing a matching lower bound for oblivious algorithms using n processors . the first steps toward a lower bound for all chaining algorithms are also provided by showing that any chaining algorithm that runs in two steps must use a superlinear number of processors . the proofs use prefix graphs and weak superconcentrators . an interesting connection between the two is demonstrated and this idea is used to obtain improved bounds on the size of prefix graphs .\n",
      "['chaining', \"ackerman ' s function\", 'lower bound', 'prefix graphs', 'superconcentrators', 'parallel <eos>']\n",
      "['prefix graph', 'crcw prams', 'chaining']\n",
      "['oblivious chaining', 'chaining', 'oblivious algorithms', 'computational complexity']\n",
      "3 4 7\n",
      "additional scores from absent units to present :  ['chaining', 'chaining']\n",
      "\n",
      "\n",
      "trap driven simulation with tapeworm ii . tapeworm ii is a software based simulation tool that evaluates the cache and tlb performance of multiple task and operating system intensive workloads . tapeworm resides in an os kernel and causes a host machine ' s hardware to drive simulations with kernel traps instead of with address traces , as is conventionally done . this allows tapeworm to quickly and accurately capture complete memory referencing behavior with a limited degradation in overall system performance . this paper compares trap driven simulation , as implemented in tapeworm , with the more common technique of trace driven memory simulation with respect to speed , accuracy , portability and flexibility .\n",
      "['trap driven simulation', 'cache', 'tlb', 'trace driven simulation', 'memory system <eos>']\n",
      "['software', 'cache', 'performance']\n",
      "['tapeworm ii', 'simulation', 'performance', 'trap driven simulation', 'performance evaluation']\n",
      "3 5 8\n",
      "additional scores from absent units to present :  ['trap driven simulation']\n",
      "\n",
      "\n",
      "reducing branch costs via branch alignment . several researchers have proposed algorithms for basic block reordering . we call these branch alignment algorithms . the primary emphasis of these algorithms has been on improving instruction cache locality , and the few studies concerned with branch prediction reported small or minimal improvements . as wide issue architectures become increasingly popular the importance of reducing branch costs will increase , and branch alignment is one mechanism which can effectively reduce these costs . in this paper , we propose an improved branch alignment algorithm that takes into consideration the architectural cost model and the branch prediction architecture when performing the basic block reordering . we show that branch alignment algorithms can improve a broad range of static and dynamic branch prediction architectures . we also show that a program performance can be improved by approximately <digit> % even when using recently proposed , highly accurate branch prediction architectures . the programs are compiled by any existing compiler and then transformed via binary transformations . when implementing these algorithms on a alpha axp <digit> up to a <digit> % reduction in total execution time is achieved .\n",
      "['branch prediction', 'profile based optimization', 'trace scheduling', 'branch target buffers <eos>']\n",
      "['Present predictions:']\n",
      "['branch alignment', 'instruction cache', 'branch prediction']\n",
      "1 3 3\n",
      "additional scores from absent units to present :  ['branch prediction']\n",
      "\n",
      "\n",
      "inverse kinematics positioning using nonlinear programming for highly articulated figures . an articulated figure is often modeled as a set of rigid segments connected with joints . its configuration can be altered by varying the joint angles . although it is straight forward to compute figure configurations given joint angles ( forward kinematics ) , it is more difficult to find the joint angles for a desired configuration ( inverse kinematics ) . since the inverse kinematics problem is of special importance to an animator wishing to set a figure to a posture satisfying a set of positioning constraints , researchers have proposed several different approaches . however , when we try to follow these approaches in an interactive animation system where the object on which to operate is as highly articulated as a realistic human figure , they fail in either generality or performance . so , we approach this problem through nonlinear programming techniques . it has been successfully used since <digit> in the spatial constraint system within jack , a human figure simulation system developed at the university of pennsylvania , and proves to be satisfactorily efficient , controllable , and robust . a spatial constraint in our system involves two parts one constraint on the figure , the end effector , and one on the spatial environment , the goal . these two parts are dealt with separately , so that we can achieve a neat modular implementation . constraints can be added one at a time with appropriate weights designating the importance of this constraint relative to the others and are always solved as a group . if physical limits prevent satisfaction of all the constraints , the system stops with the ( possibly local ) optimal solution for the given weights . also , the rigidity of each joint angle can be controlled , which is useful for redundant degrees of freedom .\n",
      "['inverse kinematics', 'nonlinear programming', 'articulated figures <eos>']\n",
      "['Present predictions:']\n",
      "['nonlinear programming', 'interactive animation', 'articulated figure', 'inverse kinematics']\n",
      "1 4 4\n",
      "additional scores from absent units to present :  ['nonlinear programming', 'inverse kinematics']\n",
      "\n",
      "\n",
      "solution differentiability for nonlinear parametric control problems . perturbed nonlinear control problems with data depending on a vector parameter are considered . using second order sufficient optimality conditions , it is shown that the optimal solution and the adjoint multipliers are differentiable functions of the parameter . the proof exploits the close connections between solutions of a riccati differential equation and shooting methods for solving the associated boundary value problem . solution differentiability provides a firm theoretical basis for numerical feedback schemes that have been developed for computing neighbouring extremals . the results are illustrated by an example that admits two extremal solutions . second order sufficient conditions single out one optimal solution for which a sensitivity analysis is carried out .\n",
      "['solution differentiability', 'parametric control problems', 'shooting methods', 'second order sufficient conditions', 'feedback controls <eos>']\n",
      "['riccati differential equation']\n",
      "['nonlinear control problems', 'shooting method', 'sensitivity analysis', 'parametric control', 'solution differentiability', '<unk>', 'nonlinear control problems', '<unk>', 'perturbed nonlinear control problems', '<unk>']\n",
      "1 10 11\n",
      "additional scores from absent units to present :  ['solution differentiability']\n",
      "\n",
      "\n",
      "robust indirect adaptive control of time varying plants with unmodeled dynamics and disturbances . it is shown that indirect pole zero placement adaptive controllers are robust for systems with time varying parameters as well as unmodeled dynamics and disturbances . a parameter estimator with projection is used . no special signal normalization is employed to ensure robustness . the nominal system parameters need only be bounded , and their variations need only be small in an average sense . this allows them to vary slowly with time , as well as to take large jumps occasionally . the adaptive controllers can also simultaneously tolerate small unmodeled dynamics , as well as bounded disturbances , with no restriction on the magnitude of the bound .\n",
      "['robustness', 'unmodeled dynamics', 'disturbances', 'time varyingplants', 'adaptive systems', 'robust performance', 'stability', 'indirect adaptivecontrol <eos>']\n",
      "['adaptive control', 'time varying plants']\n",
      "['disturbances', 'time varying plants', 'robust control', 'adaptive control', 'pole placement', 'time varying systems']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['disturbances']\n",
      "\n",
      "\n",
      "randomized algorithms for multiprocessor page migration . the page migration problem is to manage a globally addressed shared memory in a multiprocessor system . each physical page of memory is located at a given processor , and memory references to that page by other processors incur a cost proportional to the network distance . at times the page may migrate between processors at cost proportional to the distance times d , a page size factor . the problem is to schedule movements on line so that the total cost of memory references is within a constant factor c of the best off line schedule . an algorithm that does so is called c competitive . black and sleator gave <digit> competitive deterministic on line algorithms for uniform networks ( complete graphs with unit edge lengths ) and for trees with arbitrary edge lengths . no good deterministic algorithm is known for general networks with arbitrary edge lengths . randomized algorithms are presented for the migration problem that are both simple and better than <digit> competitive against an oblivious adversary . an algorithm for uniform graphs is given . it is approximately <digit> . <digit> competitive as d grows large . a second , more powerful algorithm that works on graphs with arbitrary edge distances is also given . this algorithm is approximately <digit> . <digit> competitive ( or , <digit> plus the golden ratio ) for large d . both these algorithms use random bits only during an initialization phase , and from then on run deterministically . the competitiveness of a very simple coin flipping algorithm is also examined .\n",
      "['multiprocessors', 'page migration', 'on line algorithms', 'competitive analysis', 'memory management <eos>']\n",
      "['Present predictions:']\n",
      "['multiprocessor systems', 'page migration', 'randomized algorithm', 'multiprocessor scheduling', 'competitive analysis']\n",
      "1 5 5\n",
      "additional scores from absent units to present :  ['page migration']\n",
      "\n",
      "\n",
      "distributed network computing over local atm networks . communication between processors has long been the bottleneck of distributed network computing . however , recent progress in switch based high speed local area networks ( lans ) may be changing this situation . asynchronous transfer mode ( atm ) is one of the most widely accepted and emerging high speed network standards which can potentially satisfy the communication needs of distributed network computing . in this paper , we investigate distributed network computing over local atm networks . we first study the performance characteristics involving end to end communication in an environment that includes several types of workstations interconnected via a fore systems ' asx <digit> atm switch . we then compare the communication performance of four different application programming interfaces ( apis ) . the four apis were fore systems atm api , bsd socket programming interface , sun ' s remote procedure call ( rpc ) , and the parallel virtual machine ( pvm ) message passing library . each api represents distributed programming at a different communication protocol layer . we evaluate parallel matrix multiplication over the local atm network . the experimental results show that network computing is promising over local atm networks .\n",
      "['distributed network computing', 'asynchronous transfer mode', 'application programming interface', 'performance measurement <eos>']\n",
      "['parallel virtual machine', 'asynchronous transfer mode']\n",
      "['local atm networks', 'atm', 'network computing', 'distributed network computing', 'distributed computing', 'parallel computing']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['distributed network computing']\n",
      "\n",
      "\n",
      "invariants of six points and projective reconstruction from three uncalibrated images . <unk> are three projective invariants of a set of six points in general position in space . it is well known that these invariants can not be recovered from one image , however an invariant relationship does exist between space invariants and image invariants . this invariant relationship is first derived for a single image . then this invariant relationship is used to derive the space invariants , when multiple images are available . this paper establishes that the minimum number of images for computing these invariants is three , and the computation of invariants of six points from three images can have as many as three solutions . algorithms are presented for computing these invariants in closed form . the accuracy and stability with respect to image noise , selection of the triplets of images and distance between viewing positions are studied both through real and simulated images . applications of these invariants are also presented . both the results of faugeras <digit> and hartley et al . <digit> for projective reconstruction and <unk> method <digit> for epipolar geometry determination from two uncalibrated images with at least seven points are extended to the case of three uncalibrated images with only six points .\n",
      "['projective reconstruction', 'uncalibrated images', 'this invariant', 'epipolar geometry', 'projective geometry', 'self calibration <eos>']\n",
      "['epipolar geometry', 'uncalibrated images']\n",
      "['invariants', 'projective reconstruction', 'image invariants', 'six points', 'image invariants', 'three dimensional reconstruction']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['projective reconstruction']\n",
      "\n",
      "\n",
      "efficient image processing algorithms on the scan line array processor . <unk> develop efficient algorithms for low and intermediate level image processing on the scan line array processor , a simd machine consisting of a linear array of cells that processes images in a scan line fashion . for low level processing , we present algorithms for block dft , block dct , convolution , template matching , shrinking , and expanding which run in real time . by real time , we mean that , if the required processing is based on neighborhoods of size mm , then the output lines are generated at a rate of o ( m ) operations per line and a latency of o ( m ) scan lines , which is the best that can be achieved on this model . we also develop an algorithm for median filtering which runs in almost real time at a cost of o ( m rm log m ) time per scan line and a latency of bf lfloor underline m , , <digit> rfloor scan lines . for intermediate level processing , we present optimal algorithms for translation , histogram computation , scaling , and rotation . we also develop efficient algorithms for labelling the connected components and determining the convex hulls of multiple figures which run in o ( n rm log n ) and o ( n rm log 2n ) time , respectively . the latter algorithms are significantly simpler and easier to implement than those already reported in the literature for linear arrays .\n",
      "['image processing', 'scan line array processors', 'linear array', 'parallel algorithms', 'video procesor', 'simd algorithms <eos>']\n",
      "['median filtering']\n",
      "['scan line array processor', 'simd', 'block dct', 'image processing', 'scan line array processor', 'image processing', 'image processing']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['image processing', 'image processing', 'image processing']\n",
      "\n",
      "\n",
      "a fortran <digit> environment for research and prototyping of enclosure algorithms for nonlinear equations and global optimization . an environment for general research into and prototyping of algorithms for reliable constrained and unconstrained global nonlinear optimization and reliable enclosure of all roots of nonlinear systems of equations , with or without inequality constraints , is being developed . this environment should be portable , easy to learn , use , and maintain , and sufficiently fast for some production work . the motivation , design principles , uses , and capabilities for this environment are outlined . the environment includes an interval data type , a symbolic form of automatic differentiation to obtain an internal representation for functions , a special technique to allow conditional branches with operator overloading and interval computations , and generic routines to give interval and noninterval function and derivative information . some of these generic routines use a special version of the backward mode of automatic differentiation . the package also includes dynamic data structures for exhaustive search algorithms .\n",
      "['fortran <digit>', 'global optimization', 'automatic differentiation', 'nonlinear algebraic systems', 'symbolic computation <eos>']\n",
      "['nonlinear systems of equations', 'automatic differentiation']\n",
      "['global optimization', 'prototyping of algorithms', 'enclosure algorithms', 'fortran', 'nonlinear systems of equations']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['global optimization']\n",
      "\n",
      "\n",
      "effective cache prefetching on bus based multiprocessors . compiler directed cache prefetching has the potential to hide much of the high memory latency seen by current and future high performance processors . however , prefetching is not without costs , particularly on a shared memory multiprocessor . prefetching can negatively affect bus utilization , overall cache miss rates , memory latencies and data sharing . we simulate the effects of a compiler directed prefetching algorithm , running on a range of bus based multiprocessors . we show that , despite a high memory latency , this architecture does not necessarily support prefetching well , in some cases actually causing performance degradations . we pinpoint several problems with prefetching on a shared memory architecture ( additional conflict misses , no reduction in the data sharing traffic and associated latencies , a multiprocessor ' s greater sensitivity to memory utilization and the sensitivity of the cache hit rate to prefetch distance ) and measure their effect on performance . we then solve those problems through architectural techniques and heuristics for prefetching that could be easily incorporated into a compiler ( <digit> ) victim caching , which eliminates most of the cache conflict misses caused by prefetching in a direct mapped cache , ( <digit> ) special prefetch algorithms for shared data , which significantly improve the ability of our basic prefetching algorithm to prefetch individual misses , and ( <digit> ) compiler based shared data restructuring , which eliminates many of the invalidation misses the basic prefetching algorithm does not predict . the combined effect of these improvements is to make prefetching effective over a much wider range of memory architectures .\n",
      "['cache prefetching', 'bus based multiprocessors', 'false sharing', 'memory latency hiding <eos>']\n",
      "['shared memory', 'cache prefetching']\n",
      "['multiprocessor', 'prefetching', 'bus based multiprocessors', 'performance', 'experimentation', 'cache prefetching', 'performance evaluation']\n",
      "2 7 9\n",
      "additional scores from absent units to present :  ['cache prefetching', 'bus based multiprocessors', 'cache prefetching']\n",
      "\n",
      "\n",
      "conjoining specifications . we show how to specify components of concurrent systems . the specification of a system is the conjunction of its components ' specifications . properties of the system are proved by reasoning about its components . we consider both the decomposition of a given system into parts , and the composition of given parts to form a system .\n",
      "['decomposition', 'composition', 'temporal logic', 'modular specification', 'safety properties', 'liveness properties', 'concurrent programming <eos>']\n",
      "['concurrent systems', 'reasoning']\n",
      "['decomposition', 'concurrent systems', 'formal specification', 'verification']\n",
      "2 4 6\n",
      "additional scores from absent units to present :  ['decomposition']\n",
      "\n",
      "\n",
      "matrix powers in finite precision arithmetic . if a is a square matrix with spectral radius less than <digit> then a k to <digit> as k to infty , but the powers computed in finite precision arithmetic may or may not converge . we derive a sufficient condition for fl ( a k ) to <digit> as k to infty and a bound on norm fl ( a k ) , both expressed in terms of the jordan canonical form of a . examples show that the results can be sharp . we show that the sufficient condition can be rephrased in terms of a pseudospectrum of a when a is diagonalizable , under certain assumptions . our analysis leads to the rule of thumb that convergence or divergence of the computed powers of a can be expected according as the spectral radius computed by any backward stable algorithm is less than or greater than <digit> .\n",
      "['matrix powers', 'jordan canonical form', 'pseudospectrum', 'rounding errors', 'nonnormal matrices <eos>']\n",
      "['spectral radius']\n",
      "['finite precision arithmetic', 'square matrix', 'matrix powers', 'finite precision arithmetic', 'matrix factorization', 'spectral radius', '<unk>']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['matrix powers']\n",
      "\n",
      "\n",
      "a domain specific software architecture for adaptive intelligent systems . a good software architecture facilitates application system development , promotes achievement of functional requirements , and supports system reconfiguration . we present a domain specific software architecture ( dssa ) that we have developed for a large application domain of adaptive intelligent systems ( ais ' s ) . the dssa provides a ) an ais reference architecture designed to meet the functional requirements shared by applications in this domain , b ) principles for decomposing expertise into highly reusable components , and c ) an application configuration method for selecting relevant components from a library and automatically configuring instances of those components in an instance of the architecture . the ais reference architecture incorporates features of layered , pipe and filter , and blackboard style architectures . we describe three studies demonstrating the utility of our architecture in the subdomain of mobile office robots and identify software engineering principles embodied in the architecture .\n",
      "['domain specific software architectures', 'software architecture', 'mobile robots', 'software reuse', 'intelligent agents <eos>']\n",
      "['mobile robots', 'intelligent systems']\n",
      "['adaptive intelligent systems', 'software architecture', 'domain specific software architecture', 'adaptive systems', 'domain specific architecture']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['software architecture']\n",
      "\n",
      "\n",
      "analysis of costate discretizations in parameter estimation for linear evolution equations . a widely used approach to parameter identification is the output least squares formulation . numerical methods for solving the resulting minimization problem almost invariably require the computation of the gradient of the output least squares functional . when the identification problem involves time dependent distributed parameter systems ( or approximations thereof ) , numerical evaluation of the gradient can be extremely time consuming . the costate method can greatly reduce the cost of computing these gradients . however , questions have been raised concerning the accuracy and convergence of costate approximations , even when the numerical methods being used are known to converge rapidly on the forward problem . in this paper it is shown that the use of time marching schemes that yield high order accuracy on the forward problem does not necessarily lead to high order accurate costate approximations . in fact , in some cases these approximations do not converge at all . however , under certain circumstances , rapidly converging gradient approximations do result because of rapid weak star type convergence of the costate approximations . these issues are treated both theoretically and numerically .\n",
      "['parameter estimation', 'evolution equations', 'costate method <eos>']\n",
      "['Present predictions:']\n",
      "['linear evolution equations', 'parameter estimation', 'costate method', '<unk>', 'costate methods', '<unk>']\n",
      "1 6 6\n",
      "additional scores from absent units to present :  ['parameter estimation']\n",
      "\n",
      "\n",
      "tighter lower bounds on the exact complexity of string matching . this paper considers the exact number of character comparisons needed to find all occurrences of a pattern of length m in a text of length n using on line and general algorithms . for on line algorithms , a lower bound of about ( <digit> frac <digit> <digit> ( m <digit> ) ) cdot n character comparisons is obtained . for general algorithms , a lower bound of about ( <digit> frac <digit> m <digit> ) cdot n character comparisons is obtained . these lower bounds complement an on line upper bound of about ( <digit> frac <digit> <digit> ( m <digit> ) ) cdot n comparisons obtained recently by cole and hariharan . the lower bounds are obtained by finding patterns with interesting combinatorial properties . it is also shown that for some patterns off line algorithms can be more efficient than on line algorithms .\n",
      "['lower bounds', 'complexity', 'string matching', 'comparisons', 'pattern matching <eos>']\n",
      "['on line algorithms']\n",
      "['string matching', 'exact complexity', 'lower bounds']\n",
      "1 3 4\n",
      "additional scores from absent units to present :  ['string matching', 'lower bounds']\n",
      "\n",
      "\n",
      "planar strong connectivity helps in parallel depth first search . this paper proves that for a strongly connected planar directed graph of size n , a depth first search tree rooted at a specified vertex can be computed in o ( log <digit> n ) time with n log n processors . previously , for planar directed graphs that may not be strongly connected , the best depth first search algorithm runs in o ( log <digit> n ) time with n processors . both algorithms run on a parallel random access machine that allows concurrent reads and concurrent writes in its shared memory , and in case of a write conflict , permits an arbitrary processor to succeed .\n",
      "['strong connectivity', 'depth first search', 'planar directed graphs', 'bubblegraphs', 'graph separators', 'linear processor nc algorithms', 's t graphs <eos>']\n",
      "['directed graphs', 'algorithm']\n",
      "['parallel', 'connectivity', 'strong connectivity', 'search', 'graph', 'depth first search', 'planar directed graph', 'parallel algorithms']\n",
      "2 8 10\n",
      "additional scores from absent units to present :  ['strong connectivity', 'depth first search']\n",
      "\n",
      "\n",
      "a new approach to formal language theory by kolmogorov complexity . we present a new approach to formal language theory using kolmogorov complexity . the main results presented here are an alternative for pumping lemma ( s ) , a new characterization for regular languages , and a new method to separate deterministic context free languages and nondeterministic context free languages . the use of the new incompressibility arguments is illustrated by many examples . the approach is also successful at the high end of the chomsky hierarchy since one can quantify <unk> in terms of kolmogorov complexity .\n",
      "['formal language theory', 'kolmogorov complexity', 'pumping lemmas', 'regular languages', 'deterministic context free languages', 'finite automata <eos>']\n",
      "['regular languages']\n",
      "['kolmogorov complexity', 'language theory', 'pumping lemma', 'context free languages', 'formal language theory', 'formal languages', 'formal languages', '<unk>']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['kolmogorov complexity', 'formal language theory']\n",
      "\n",
      "\n",
      "stability of linear equations solvers in interior point methods . primal dual interior point methods for linear complementarity and linear programming problems solve a linear system of equations to obtain a modified newton step at each iteration . these linear systems become increasingly ill conditioned in the later stages of the algorithm , but the computed steps are often sufficiently accurate to be useful . we use error analysis techniques tailored to the special structure of these linear systems to explain this observation and examine how theoretically superlinear convergence of a path following algorithm is affected by the roundoff errors .\n",
      "['stability', 'primal dual interior point methods', 'error analysis <eos>']\n",
      "['Present predictions:']\n",
      "['interior point methods', 'stability', 'path following algorithm', 'linear complementarity', '<unk>', 'interior point methods', 'linear complementarity problems', '<unk>', 'primal dual method', 'newton method', '<unk>']\n",
      "1 11 11\n",
      "additional scores from absent units to present :  ['stability']\n",
      "\n",
      "\n",
      "a case study in simulating pcs networks using time warp . there has been rapid growth in the demand for mobile communications over the past few years . this has led to intensive research and development efforts for complex pcs ( personal communication service ) networks . capacity planning and performance modeling is necessary to maintain a high quality of service to the mobile subscriber while minimizing cost to the pcs provider . the need for flexible analysis tools and the high computational requirements of large pcs network simulations make it an excellent candidate for parallel simulation . here , we describe our experiences in developing two pcs simulation models on a general purpose distributed simulation platform based on the time warp mechanism . these models utilize two widely used approaches to simulating pcs networks ( i ) the call initiated and ( ii ) the portable initiated models . we discuss design decisions that were made in mapping these models to the time warp executive , and characterize the workloads resulting from these models in terms of factors such as communication locality and computation granularity . we then present performance measurements for their execution on a network of workstations . these measurements indicate that the call initiated model generally outperforms the portable initiated model , but is not able to capture phenomenon such as the busy line effect . moreover , these studies indicate that the high locality in large scale pcs network simulations make them well suited for execution on general purpose parallel and distributed simulation platforms .\n",
      "['pcs networks', 'time warp', 'mobile communications', 'mobile communication', 'personal communication service', 'flexible analysis tools', 'parallel', 'distributed', 'simulation platforms', 'performance measurements', 'discrete event simulation', 'time warp simulation', 'telecommunication computing <eos>']\n",
      "['performance', 'distributed simulation']\n",
      "['time warp', 'pcs networks', 'capacity planning', 'time warp', 'mobile computing', 'network simulation', 'distributed systems', 'performance evaluation']\n",
      "2 8 10\n",
      "additional scores from absent units to present :  ['time warp', 'pcs networks', 'time warp']\n",
      "\n",
      "\n",
      "probabilistic adaptive direct optimism control in time warp . in a distributed memory environment the communication overhead of time warp as induced by the rollback procedure due to overoptimistic progression of the simulation is the dominating performance factor . to limit optimism to an extent that can be justified from the inherent model parallelism , an optimism control mechanism is proposed , which by maintaining a history record of virtual time differences from the time stamps carried by arriving messages , and forecasting the timestamps of forthcoming messages , probabilistically delays the execution of scheduled events to avoid potential rollback and associated communication overhead ( antimessages ) . after investigating statistical forecast methods which express only the central tendency of the arrival process , we demonstrate that arrival processes in the context of time warp simulations of timed petri nets have certain predictable and consistent arima characteristics , which encourage the use of sophisticated and recursive forecast procedures based on those models . adaptiveness is achieved in two respects the synchronization behavior of logical processes automatically progressing and conservatively blocking , that is the most adequate for ( i ) the specific simulation model and ( ii ) the communication computation speed characteristics of the underlying execution platform .\n",
      "['optimism control', 'time warp', 'petri nets', 'forecast models', 'pvm', 'rs6000 cluster <eos>']\n",
      "['timed petri nets', 'distributed memory']\n",
      "['time warp', 'direct optimism control', 'model parallelism', 'direct optimism control', 'time warp', 'probabilistic adaptive direct optimism control', 'distributed memory systems', 'performance']\n",
      "2 8 10\n",
      "additional scores from absent units to present :  ['time warp', 'time warp']\n",
      "\n",
      "\n",
      "buffer management in shared memory time warp systems . mechanisms for managing message buffers in time warp parallel simulations executing on cache coherent shared memory multiprocessors are studied . two simple buffer management strategies called the sender pool and receiver pool mechanisms are examined with respect to their efficiency , and in particular , their interaction with multiprocessor cache coherence protocols . measurements of implementations on a kendall square research ksr <digit> machine using both synthetic workloads and benchmark applications demonstrate that sender pools offer significant performance advantages over receiver pools . however , it is also observed that both schemes , especially the sender pool mechanism , are prone to severe performance degradations due to poor locality of reference in large simulations using substantial amounts of message buffer memory . a third strategy called the partitioned buffer pool approach is proposed that exploits the advantages of sender pools , but exhibits much better locality . measurements of this approach indicate that the partitioned pool mechanism yields substantially better performance than both the sender and receiver pool schemes for large scale , small granularity parallel simulation applications . the central conclusions from this study are ( <digit> ) buffer management strategies play an important role in determining the overall efficiency of multiprocessor based parallel simulators , and ( <digit> ) the partitioned buffer pool organization offers significantly better performance than the sender and receiver pool schemes . these studies demonstrate that poor performance may result if proper attention is not paid to realizing an efficient buffer management mechanism .\n",
      "['buffer management', 'shared memory time warp systems', 'message buffers', 'cache coherent shared memory multiprocessors', 'sender pool', 'receiver pool', 'multiprocessor cache coherence protocols', 'kendall square research ksr <digit> machine', 'severe performance degradations', 'message buffer memory', 'partitioned buffer pool approach', 'partitioned pool mechanism', 'multiprocessor based parallel simulators', 'storage management', 'buffer storage', 'mall granularity parallel simulation applications', 'multiprocessing programs', 'shared memory systems', 'time warp simulation', 'discrete event simulation', 'message passing <eos>']\n",
      "['parallel simulation', 'cache coherence protocols']\n",
      "['shared memory multiprocessors', 'message memory', 'time warp', 'buffer management', 'shared memory multiprocessors', 'parallel computing', 'cache coherence protocols', 'performance evaluation']\n",
      "2 8 10\n",
      "additional scores from absent units to present :  ['buffer management']\n",
      "\n",
      "\n",
      "production and playback of human figure motion for visual simulation . we describe a system for off line production and real time playback of motion for articulated human figures in 3d virtual environments . the key notion are ( <digit> ) the logical storage of full body motion in posture graphs , which provides a simple motion access method for playback , and ( <digit> ) mapping the motions of high dof figures to lower dof figures using slaving to provide human models at several levels of detail , both in geometry and articulation , for later playback . we present our system in the context of a simple problem animating human figures in a distributed simulation , using dis protocols for communicating the human state information . we also discuss several related techniques for real time animation of articulated figures in visual simulation .\n",
      "['visual simulation', 'posture graphs', 'animation', 'real time animation', 'multiresolution motion <eos>']\n",
      "['distributed simulation']\n",
      "['visual simulation', 'playback', 'human figure motion', 'visual simulation', 'human motion', '3d virtual environments', 'motion capture', 'virtual reality']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['visual simulation', 'visual simulation']\n",
      "\n",
      "\n",
      "active camera calibration for a head eye platform using the variable state dimension filter . <unk> correspondence presents a new technique for calibrating a camera mounted on a controllable head eye platform . it uses the trajectories of an arbitrary number of tracked corner features to improve the calibration parameter estimates over time , utilizing a novel variable state dimension form of recursive filter . no special visual stimuli are required and no assumptions are made about the structure of the scene , other than that it is stationary relative to the head . the algorithm runs at <digit> frames per second on a single inmos t805 transputer , and is fully integrated into a real time active vision system . updated calibration parameters are regularly passed to the vision modules that require them . although the algorithm requires an initial estimate of camera focal length , results are presented from real experiments demonstrating that convergence is achieved for initial errors up to <digit> % .\n",
      "['camera calibration', 'recursive filter', 'active vision', 'real time vision <eos>']\n",
      "['recursive filter']\n",
      "['variable state dimension filter', 'camera calibration', 'head eye platform', 'active vision', 'active vision', 'eye tracking', 'camera calibration']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['camera calibration', 'active vision', 'active vision', 'camera calibration']\n",
      "\n",
      "\n",
      "genetic algorithms , operators , and dna fragment assembly . we study different genetic algorithm operators for one permutation problem associated with the human genome <unk> assembly of dna sequence fragments from a parent clone whose sequence is unknown into a consensus sequence corresponding to the parent sequence . the sorted order representation , which does not require specialized operators , is compared with a more traditional permutation representation , which does require specialized operators . the two representations and their associated operators are compared on problems ranging from 2k to 34k base pairs ( kb ) . edge recombination crossover used in conjunction with several specialized operators is found to perform best in these experiments semi these operators solved a 10kb sequence , consisting of <digit> fragments , with no manual intervention . natural building blocks in the problem are exploited at progressively higher levels through macro operators . this significantly improves performance .\n",
      "['genetic algorithms', 'dna fragment assembly', 'edge recombination crossover', 'building blocks', 'ordering problems', 'human genome project <eos>']\n",
      "['Present predictions:']\n",
      "['dna fragment assembly', 'genetic algorithms', 'dna assembly', 'evolutionary computation']\n",
      "1 4 4\n",
      "additional scores from absent units to present :  ['dna fragment assembly', 'genetic algorithms']\n",
      "\n",
      "\n",
      "unsupervised learning of multiple motifs in biopolymers using expectation maximization . the meme algorithm extends the expectation maximization ( em ) algorithm for identifying motifs in unaligned biopolymer sequences . the aim of meme is to discover new motifs in a set of biopolymer sequences where little or nothing is known in advance about any motifs that may be present . meme innovations expand the range of problems which can be solved using em and increase the chance of finding good solutions . first , subsequences which actually occur in the biopolymer sequences are used as starting points for the em algorithm to increase the probability of finding globally optimal motifs . second , the assumption that each sequence contains exactly one occurrence of the shared motif is removed . this allows multiple appearances of a motif to occur in any sequence and permits the algorithm to ignore sequences with no appearance of the shared motif , increasing its resistance to noisy data . third , a method for probabilistically erasing shared motifs after they are found is incorporated so that several distinct motifs can be found in the same set of sequences , both when different motifs appear in different sequences and when a single sequence may contain multiple motifs . experiments show that meme can discover both the crp and lexa binding sites from a set of sequences which contain one or both sites , and that meme can discover both the <digit> and <digit> promoter regions in a set of e . coli sequences .\n",
      "['unsupervised learning', 'motif', 'biopolymer', 'expectation maximization', 'binding site', 'promoter', 'dna', 'sequence analysis', 'consensus sequence', 'protein <eos>']\n",
      "['em algorithm', 'multiple motifs']\n",
      "['unsupervised learning', 'unsupervised learning', 'em algorithm', 'motif discovery']\n",
      "2 4 6\n",
      "additional scores from absent units to present :  ['unsupervised learning', 'unsupervised learning']\n",
      "\n",
      "\n",
      "processor mapping techniques toward efficient data redistribution . <unk> time data redistribution can enhance algorithm performance in distributed memory machines . explicit redistribution of data can be performed between algorithm phases when a different data decomposition is expected to deliver increased performance for a subsequent phase of computation . redistribution , however , represents increased program overhead as algorithm computation is discontinued while data are exchanged among processor memories . in this paper , we present a technique that minimizes the amount of data exchange for block to cyclic ( c ) ( or vice versa ) redistributions of arbitrary number of dimensions . preserving the semantics of the target ( destination ) distribution pattern , the technique manipulates the data to logical processor mapping of the target pattern . when implemented on an ibm sp , the mapping technique demonstrates redistribution performance improvements of approximately <digit> % over traditional data to processor mapping . relative to the traditional mapping technique , the proposed method affords greater flexibility in specifying precisely which data elements are redistributed and which elements remain on processor .\n",
      "['processor mapping', 'data redistribution', 'data decomposition', 'distributed memory architectures', 'high performance fortran', 'data parallel programming <eos>']\n",
      "['distributed memory machines']\n",
      "['data redistribution', 'mapping', 'processor mapping', 'data redistribution', 'parallel processing', 'distributed memory systems']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['data redistribution', 'processor mapping', 'data redistribution']\n",
      "\n",
      "\n",
      "a comparison of id3 and backpropagation for english text to speech mapping . the performance of the error backpropagation ( bp ) and id3 learning algorithms was compared on the task of mapping english text to phonemes and stresses . under the distributed output code developed by sejnowski and rosenberg , it is shown that bp consistently out performs id3 on this task by several percentage points . three hypotheses explaining this difference were explored ( a ) id3 is overfitting the training data , ( b ) bp is able to share hidden units across several output units and hence can learn the output units better , and ( c ) bp captures statistical information that id3 does not . we conclude that only hypothesis ( c ) is correct . by augmenting id3 with a simple statistical learning procedure , the performance of bp can be closely matched . more complex statistical procedures can improve the performance of both bp and id3 substantially in this domain .\n",
      "['backpropagation', 'text to speech', 'experimental comparisons <eos>']\n",
      "['text to speech mapping', 'id3']\n",
      "['speech mapping', 'backpropagation', 'error backpropagation', 'speech recognition', 'text to speech']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['backpropagation', 'text to speech']\n",
      "\n",
      "\n",
      "weakly hard problems . a weak completeness phenomenon is investigated in the complexity class rm e rm dtime ( <digit> rm linear ) . according to standard terminology , a language h is leq rm p m hard for e if the set rm p m ( h ) , consisting of all languages a leq rm p m h , contains the entire class e . a language c is leq rm p m complete for e if it is leq rm p m hard for e and is also an element of e . generalizing this , a language h is weakly leq rm p m hard for e if the set rm p m ( h ) does not have measure <digit> in e . a language c is weakly leq rm p m complete for e if it is weakly leq rm p m hard for e and is also an element of e . the main result of this paper is the construction of a language that is weakly leq rm p m complete , but not leq rm p m complete , for e . the existence of such languages implies that previously known strong lower bounds on the complexity of weakly leq rm p m hard problems for e ( given by work of lutz , mayordomo , and juedes ) are indeed more general than the corresponding bounds for leq rm p m hard problems for e . the proof of this result introduces a new diagonalization method , called martingale diagonalization . using this method , one simultaneously develops an infinite family of polynomial time computable martingales ( betting strategies ) and a corresponding family of languages that defeat these martingales ( prevent them from winning too much money ) while also pursuing another agenda . martingale diagonalization may be useful for a variety of applications .\n",
      "['weak completeness', 'complexity classes', 'resource bounded measure', 'computational complexity', 'complete problems <eos>']\n",
      "['hard problems', 'martingale']\n",
      "['complexity', 'strong completeness', 'weak completeness', 'weakly p complete', 'weakly p complete', 'hard problems']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['weak completeness']\n",
      "\n",
      "\n",
      "efficient algorithms for atmospheric correction of remotely sensed data . remotely sensed imagery has been used for developing and validating various studies regarding land cover dynamics . however , the large amounts of imagery collected by the satellites are largely contaminated by the effects of atmospheric particles . the objective of atmospheric correction is to retrieve the surface reflectance from remotely sensed imagery by removing the atmospheric effects . we introduce a number of computational techniques that lead to a substantial speedup of an atmospheric correction algorithm based on using look up tables . excluding i o time , the previous known implementation processes one pixel at a time and requires about <digit> . <digit> seconds per pixel on a sparc <digit> machine , while our implementation is based on processing the whole image and takes about <digit> <digit> microseconds per pixel on the same machine . we also develop a parallel version of our algorithm that is scalable in terms of both computation and i o . experimental results obtained show that a thematic mapper ( tm ) image ( <digit> mb per band , <digit> bands need to be corrected ) can be handled in less than <digit> . <digit> minutes on a <digit> node cm <digit> machine , including i o time .\n",
      "['atmospheric correction', 'remote sensing', 'scalable parallel processing', 'high performance computing', 'avhrr', 'parallel i o <eos>']\n",
      "['Present predictions:']\n",
      "['remotely sensed imagery', 'atmospheric correction', 'atmospheric tomography', 'parallel computing', 'image processing', 'land cover']\n",
      "1 6 6\n",
      "additional scores from absent units to present :  ['atmospheric correction']\n",
      "\n",
      "\n",
      "towards modeling the performance of a fast connected components algorithm on parallel machines . we present and analyze a portable , high performance algorithm for finding connected components on modern distributed memory multiprocessors . the algorithm is a hybrid of the classic dfs on the subgraph local to each processor and a variant of the shiloach vishkin pram algorithm on the global collection of subgraphs . we implement the algorithm in split c and measure performance on the the cray t3d , the meiko cs <digit> , and the thinking machines cm <digit> using a class of graphs derived from cluster dynamics methods in computational physics . on a <digit> processor cray t3d , the implementation outperforms all previous solutions by an order of magnitude . a characterization of graph parameters allows us to select graphs that highlight key performance features . we study the effects of these parameters and machine characteristics on the balance of time between the local and global phases of the algorithm and find that edge density , surface to volume ratio , and relative communication cost dominate performance . by understanding the effect of machine characteristics on performance , the study sheds light on the impact of improvements in computational and or communication performance on this challenging problem .\n",
      "['connected components', 'parallel machines', 'distributed memory', 'performance modeling modelling', 'hybrid algorithm <eos>']\n",
      "['connected components']\n",
      "['parallel machines', 'cluster dynamics', 'connected components', 'distributed memory multiprocessors', 'parallel computing', 'distributed memory multiprocessors', 'connected components modeling', 'graph algorithms', 'performance evaluation']\n",
      "1 9 10\n",
      "additional scores from absent units to present :  ['connected components', 'parallel machines', 'connected components']\n",
      "\n",
      "\n",
      "message passing versus distributed shared memory on networks of workstations . the message passing programs are executed with the parallel virtual machine ( pvm ) library and the shared memory programs are executed using treadmarks . the programs are water and barnes hut from the splash benchmark suite <digit> d fft , integer sort ( is ) and embarrassingly parallel ( ep ) from the nas benchmarks ilink , a widely used genetic linkage analysis program and successive over relaxation ( sor ) , traveling salesman ( tsp ) , and quicksort ( qsort ) . two different input data sets were used for water ( water <digit> and water <digit> ) , is ( is small and is large ) , and sor ( sor zero and sor nonzero ) . our execution environment is a set of eight hp735 workstations connected by a 100mbits per second fddi network . for water <digit> , ep , ilink , sor zero , and sor nonzero , the performance of treadmarks is within <digit> % of pvm . for is small , water <digit> , barnes hut , <digit> d fft , tsp , and qsort , differences are on the order of <digit> % to <digit> % . finally , for is large , pvm performs two times better than treadmarks . more messages and more data are sent in treadmarks , explaining the performance differences . this extra communication is caused by <digit> ) the separation of synchronization and data transfer , <digit> ) extra messages to request updates for data by the invalidate protocol used in treadmarks , accumulation for migratory data in treadmarks .\n",
      "['message passing', 'distributed shared memory', 'pvm', 'treadmarks', 'lazy release consistency <eos>']\n",
      "['genetic linkage analysis', 'distributed shared memory']\n",
      "['traveling salesman', 'parallel virtual machine', 'shared memory', 'message passing', 'parallel computing', 'distributed computing']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['message passing']\n",
      "\n",
      "\n",
      "distributing a chemical process optimization application over a gigabit network . we evaluate the impact of a gigabit network on the implementation of a distributed chemical process optimization application . the optimization problem is formulated as a stochastic linear assignment problem and was solved using the thinking machines cm <digit> ( simd ) and the cray c <digit> ( vector ) computers at psc , and the intel iwarp ( mimd ) system at cmu , connected by the gigabit nectar testbed . we report our experience distributing the application across this heterogeneous set of systems and present measurements that show how the communication requirements of the application depend on the structure of the application . we use detailed traces to build an application performance model that can be used to estimate the elapsed time of the application for different computer system and network combinations . our results show that the application benefits from the high speed network , and that the need for high network throughput is increasing as computer systems get faster . we also observed that supporting high burst rates is critical , although structuring the application so that communication is overlapped with computation relaxes the bandwidth requirements .\n",
      "['chemical process optimization', 'gigabit networks', 'stochastic linear assignment problem', 'heterogeneous computing', 'optimal resource allocation', 'distributed computing <eos>']\n",
      "['stochastic linear assignment']\n",
      "['gigabit network', 'process optimization', 'chemical process optimization', 'chemical process optimization', 'stochastic optimization', 'network optimization', 'distributed computing', 'performance evaluation']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['chemical process optimization', 'chemical process optimization']\n",
      "\n",
      "\n",
      "relative debugging and its application to the development of large numerical models . because large scientific codes are rarely static objects , developers are often faced with the tedious task of accounting for discrepancies between new and old versions . in this paper , we describe a new technique called relative debugging that addresses this problem by automating the process of comparing a modified code against a correct reference code . we examine the utility of the relative debugging technique by applying a relative debugger called guard to a range of debugging problems in a large atmospheric circulation model . our experience confirms the effectiveness of the approach . using guard , we are able to validate a new sequential version of the atmospheric model , and to identify the source of a significant discrepancy in a parallel version in a short period of time .\n",
      "['relative debugging', 'debugging', 'guard', 'parallelism', 'meteorology', 'scientific computing', 'tools <eos>']\n",
      "['atmospheric model']\n",
      "['debugging', 'relative debugging', 'large atmospheric model', 'parallel programming', 'atmospheric models']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['debugging', 'relative debugging']\n",
      "\n",
      "\n",
      "lazy release consistency for hardware coherent multiprocessors . release consistency is a widely accepted memory model for distributed shared memory systems . eager release consistency represents the state of the art in release consistent protocols for hardware coherent multiprocessors , while lazy release consistency has been shown to provide better performance for software distributed shared memory ( dsm ) . several of the optimizations performed by lazy protocols have the potential to improve the performance of hardware coherent multiprocessors as well , but their complexity has precluded a hardware implementation . with the advent of programmable protocol processors it may become possible to use them after all . we present and evaluate a lazy release consistent protocol suitable for machines with dedicated protocol processors . this protocol admits multiple concurrent writers , sends write notices concurrently with computation , and delays invalidations until acquire operations . we also consider a lazier protocol that delays sending write notices until release operations . our results indicate that the first protocol outperforms eager release consistency by as much as <digit> % across a variety of applications . the lazier protocol , on the other hand , is unable to recoup its high synchronization overhead . this represents a qualitative shift from the dsm world , where lazier protocols always yield performance improvements . based on our results , we conclude that machines with flexible hardware support for coherence should use protocols based on lazy release consistency , but in a less ' ' aggressively lazy ' ' form than is appropriate for dsm .\n",
      "['lazy release consistency', 'shared memory', 'protocol processors', 'cache coherence <eos>']\n",
      "['performance', 'distributed shared memory']\n",
      "['hardware coherent multiprocessors', 'release consistency', 'lazy release consistency', 'hardware coherent multiprocessors', 'lazy release consistency', 'distributed shared memory systems']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['lazy release consistency', 'lazy release consistency']\n",
      "\n",
      "\n",
      "an hpf compiler for the ibm sp2 . we describe phpf , an research prototype hpf compiler for the ibm sp series parallel machines . the compiler accepts as input fortran <digit> and fortran <digit> programs , augmented with hpf directives sequential loops are automatically parallelized . the compiler supports symbolic analysis of expressions . this allows parameters such as the number of processors to be unknown at compile time without significantly affecting performance . communication schedules and computation guards are generated in a parameterized form at compile time . several novel optimizations and improved versions of well known optimizations have been implemented in phpf to exploit parallelism and reduce communication costs . these optimizations include elimination of redundant communication using data availability analysis using collective communication new techniques for mapping scalar variables coarse grain wavefronting and communication reduction in multi dimensional shift communications . we present experimental results for some well known benchmark routines . the results show the effectiveness of the compiler in generating efficient code for hpf programs .\n",
      "['hpf', 'compiler', 'ibm sp2', 'fortran', 'automatic parallelization', 'communication optimization', 'distributed memory compilation', 'optimizing compiler <eos>']\n",
      "['Present predictions:']\n",
      "['ibm sp2', 'hpf', 'parallel programming', 'performance evaluation']\n",
      "1 4 4\n",
      "additional scores from absent units to present :  ['ibm sp2', 'hpf']\n",
      "\n",
      "\n",
      "real time multi tasking in software synthesis for information processing systems . abstract software synthesis is a new approach which focuses on the support of embedded systems without the use of operating systems . compared to traditional design practices , a better utilization of the available time and hardware resources can be achieved , because the static information provided by the system specification is fully exploited and an application specific solution is automatically generated . on going research on a software synthesis approach for real time information processing systems is presented which starts from a concurrent process system specification and tries to automate the mapping of this description to a single processor . an internal representation model which is well suited for the support of concurrency and timing constraints is proposed , together with flexible execution models for multi tasking with real time constraints . the method is illustrated on a personal terminal receiver demodulator for mobile satellite communication .\n",
      "['real time multi tasking', 'software synthesis', 'information processing systems', 'embedded systems', 'static information', 'concurrent process system specification', 'internal representation model', 'timing constraints', 'flexible execution models', 'personal terminal receiver demodulator', 'mobile satellite communication', 'multiprocessing programs', 'processor scheduling', 'time utilization', 'real time systems', 'automatically generated application specific solution', 'automatic processor mapping', 'concurrency control', 'computer aided software engineering', 'hardware resource utilization <eos>']\n",
      "['software synthesis', 'multi tasking']\n",
      "['information processing systems', 'real time', 'real time systems', 'software engineering', 'multi tasking systems']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['information processing systems']\n",
      "\n",
      "\n",
      "time constrained code compaction for dsps . abstract dsp algorithms are , in most cases , subject to hard real time constraints . in the case of programmable dsps , meeting those constraints must be ensured by appropriate code generation techniques . for processors offering instruction level parallelism , the task of code generation includes code compaction . the exact timing behavior of a dsp program is only known after compaction . therefore , real time constraints should be taken into account during the compaction phase . while most known dsp code generators rely on rigid heuristics for that phase , this paper proposes a novel approach to local code compaction based on an integer programming model , which obeys exact timing constraints . due to a general problem formulation , the model also obeys encoding restrictions and possible side effects .\n",
      "['time constrained code compaction', 'timing', 'hard real time constraints', 'programmable dsp', 'code generation techniques', 'instruction level parallelism', 'exact timing behavior', 'rigid heuristics', 'local code compaction', 'integer programming', 'integer programming model', 'encoding restrictions', 'side effects', 'digital signal processing chips', 'real time systems', 'digital signal processing algorithms', 'automatic programming', 'source coding <eos>']\n",
      "['integer programming']\n",
      "['code compaction', 'instruction level parallelism']\n",
      "1 2 3\n",
      "additional scores from absent units to present :  ['instruction level parallelism']\n",
      "\n",
      "\n",
      "synthesis of pipelined dsp accelerators with dynamic scheduling . abstract to construct complete systems on silicon , application specific dsp accelerators are needed to speed up the execution of high throughput dsp algorithms . in this paper , a methodology is presented to synthesize high throughput dsp functions into accelerator processors containing a datapath of highly pipelined , bit parallel hardware units . emphasis is put on the definition of a controller architecture that allows efficient run time schedules of these dsp algorithms on such highly pipelined data paths . the methodology is illustrated by means of an fft butterfly accelerator block .\n",
      "['dynamic scheduling', 'scheduling', 'silicon', 'application specific dsp accelerators', 'dsp algorithms', 'datapath', 'controller architecture', 'run time schedules', 'highly pipelined data paths', 'fft butterfly accelerator block', 'digital signal processing chips', 'network synthesis', 'pipelined bit parallel hardware', 'pipelined dsp accelerator synthesis', 'pipeline processing', 'circuit cad', 'parallel architectures', 'application specific integrated circuits <eos>']\n",
      "['Present predictions:']\n",
      "['dynamic scheduling', 'dsp', 'high performance computing', 'pipelined dsp', 'parallel computing', 'hardware software codesign']\n",
      "1 6 6\n",
      "additional scores from absent units to present :  ['dynamic scheduling']\n",
      "\n",
      "\n",
      "an exact methodology for scheduling in a 3d design space . abstract this paper describes an exact solution methodology , implemented in rensselaer ' s voyager design space exploration system , for solving the scheduling problem in a <digit> dimensional ( 3d ) design space the usual 2d design space ( which trades off area and schedule length ) , plus a third dimension representing clock length . unlike design space exploration methodologies which rely on bounds or estimates , this methodology is guaranteed to find the globally optimal solution to the 3d scheduling problem . furthermore , this methodology efficiently prunes the search space , eliminating provably inferior design points through a careful selection of candidate clock lengths and tight bounds on the number of functional units of each type or on the schedule length .\n",
      "['scheduling', '3d design space', 'voyager design space exploration system', '2d design space', 'schedule length', 'clock length', 'clocks', 'globally optimal solution', '3d scheduling problem', 'candidate clock lengths', 'tight bounds', 'optimisation', 'high level synthesis', 'search space pruning', 'three dimensional design space', 'three dimensional scheduling', 'network synthesis', 'two dimensional design space', 'search problems <eos>']\n",
      "['Present predictions:']\n",
      "['3d design space exploration', 'scheduling', 'design space exploration', '3d design space exploration']\n",
      "1 4 4\n",
      "additional scores from absent units to present :  ['scheduling']\n",
      "\n",
      "\n",
      "power analysis and low power scheduling techniques for embedded dsp software . abstract this paper describes the application of a measurement based power analysis technique for an embedded dsp processor . an instruction level power model for the processor has been developed using this technique . significant points of difference have been observed between this model and the ones developed earlier for some general purpose commercial microprocessors . in particular , the effect of circuit state on the power cost of an instruction stream is more marked in the case of this dsp processor . in addition , the dsp processor has a special architectural feature that allows instructions to be packed into pairs . the energy reduction possible through the use of this feature is studied . the on chip booth multiplier on the processor is a major source of energy consumption for dsp programs . a micro architectural power model for the multiplier is developed and analyzed for further energy minimization . a scheduling algorithm incorporating these new techniques is proposed to reduce the energy consumed by dsp software . energy reductions varying from <digit> % to <digit> % have been observed for several example programs . these energy savings are real and have been verified through physical measurement .\n",
      "['power analysis', 'low power scheduling', 'scheduling', 'embedded dsp software', 'measurement based power analysis', 'dsp processor', 'instruction level power model', 'general purpose commercial microprocessors', 'circuit state', 'energy reduction', 'on chip booth multiplier', 'energy consumption', 'micro architectural power model', 'energy minimization', 'scheduling algorithm', 'instruction sets', 'real time systems', 'circuit cad', 'digital signal processing chips', 'application specific integrated circuits <eos>']\n",
      "['booth multiplier', 'embedded dsp']\n",
      "['scheduling', 'low power', 'instruction level power model', 'power analysis', 'low power design', 'power analysis', 'embedded systems']\n",
      "2 7 9\n",
      "additional scores from absent units to present :  ['scheduling', 'instruction level power model', 'power analysis', 'power analysis']\n",
      "\n",
      "\n",
      "bounded skew clock and steiner routing under elmore delay . we study the minimum cost bounded skew routing tree problem under the elmore delay model . we present two approaches to construct bounded skew routing trees ( i ) the boundary merging and embedding ( bme ) method which utilizes merging points that are restricted to the boundaries of merging regions , and ( ii ) the interior merging and embedding ( ime ) algorithm which employs a sampling strategy and dynamic programming to consider merging points that are interior to , rather than on the boundary of , the merging regions . our new algorithms allow accurate control of elmore delay skew , and show the utility of merging points inside merging regions .\n",
      "['bounded skew', 'elmore delay', 'routing trees', 'clock routing', 'vlsi', 'global routing', 'pathlength delay', 'zero skew <eos>']\n",
      "['dynamic programming']\n",
      "['elmore delay', 'routing', 'steiner routing', 'boundary merging', 'bounded skew routing']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['elmore delay']\n",
      "\n",
      "\n",
      "hybrid decision diagrams . abstract functions that map boolean vectors into the integers are important for the design and verification of arithmetic circuits . mtbdds and bmds have been proposed for representing this class of functions . we discuss the relationship between these methods and describe a generalization called hybrid decision diagrams which is often much more concise . we show how to implement arithmetic operations efficiently for hybrid decision diagrams . in practice , this is one of the main limitations of bmds since performing arithmetic operations on functions expressed in this notation can be very expensive . in order to extend symbolic model checking algorithms to handle arithmetic properties , it is essential to be able to compute the bdd for the set of variable assignments that satisfy an arithmetic relation . in our paper , we give an efficient algorithm for this purpose . moreover , we prove that for the class of linear expressions , the time complexity of our algorithm is linear in the number of variables .\n",
      "['hybrid decision diagrams', 'boolean vectors', 'integers', 'mtbdds', 'symbolic model checking algorithms', 'linear expressions', 'time complexity', 'computational complexity', 'circuit analysis computing', 'digital arithmetic', 'binary decision diagrams', 'multi terminal binary decision diagrams', 'arithmetic circuits verification <eos>']\n",
      "['symbolic model checking', 'decision diagrams']\n",
      "['verification', 'model checking', 'arithmetic circuits', 'hybrid decision diagrams', 'hybrid decision diagrams']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['hybrid decision diagrams', 'hybrid decision diagrams']\n",
      "\n",
      "\n",
      "design verification via simulation and automatic test pattern generation . we present a simulation based method for combinational design verification that aims at complete coverage of specified design errors using conventional atpg tools . the error models used in prior research are examined and reduced to four types gate substitution errors ( gses ) , gate count errors ( gces ) , input count errors ( ices ) , and wrong input errors ( wies ) . conditions are derived for a gate to be completely testable for gses these conditions lead to small test sets for gses . near minimal test sets are also derived for gces . we analyze redundancy in design errors and relate this to single stuck line ( ssl ) redundancy . we show how to map all the foregoing error types into ssl faults , and describe an extensive set of experiments to evaluate the proposed method . our experiments demonstrate that high coverage of the modeled design errors can be achieved with small test sets .\n",
      "['design verification', 'error models', 'test generation', 'logic simulation <eos>']\n",
      "['Present predictions:']\n",
      "['simulation', 'gate substitution errors', 'test pattern generation', 'design verification', 'design for testability']\n",
      "1 5 5\n",
      "additional scores from absent units to present :  ['design verification']\n",
      "\n",
      "\n",
      "diagnosis of realistic bridging faults with single stuck at information . abstract precise failure analysis requires accurate fault diagnosis . a previously proposed method for diagnosing bridging faults using single stuck at dictionaries was applied only to small circuits , produced large and imprecise diagnoses , and did not take into account the byzantine generals problem for bridging faults . we analyze the original technique and improve it by introducing the concepts of match restriction , match requirement , and failure recovery . our new technique , which requires no information other than that used by standard stuck at methods , produces diagnoses that are an order of magnitude smaller than those produced by the original technique and produces many fewer misleading diagnoses than that of traditional stuck at diagnosis .\n",
      "['single stuck at information', 'failure analysis', 'fault diagnosis', 'single stuck at dictionaries', 'match restriction', 'match requirement', 'failure recovery', 'stuck at methods', 'stuck at diagnosis', 'fault location', 'logic testing', 'realistic bridging faults diagnosis <eos>']\n",
      "['single stuck at diagnosis']\n",
      "['diagnosis', 'bridging faults', 'fault diagnosis', 'stuck at diagnosis', 'fault diagnosis', 'fault diagnosis', 'verification']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['fault diagnosis', 'stuck at diagnosis', 'fault diagnosis', 'fault diagnosis']\n",
      "\n",
      "\n",
      "instruction selection using binate covering for code size optimization . we address the problem of instruction selection in code generation for embedded dsp microprocessors . such processors have highly irregular data paths , and conventional code generation methods typically result in inefficient code . instruction selection can be formulated as directed acyclic graph ( dag ) covering . conventional methods for instruction selection use heuristics that break up the dag into a forest of trees and then cover them independently . this breakup can result in suboptimal solutions for the original dag . alternatively , the dag covering problem can be formulated as a binate covering problem , and solved exactly or heuristically using branch and bound methods . we show that optimal instruction selection on a dag in the case of accumulator based architectures requires a partial scheduling of nodes in the dag , and we augment the binate covering formulation to minimize spills and reloads . we show how the irregular data transfer costs of typical dsp data paths can be modeled in the binate covering formulation .\n",
      "['instruction selection', 'code generation', 'digital signal processors <eos>']\n",
      "['branch and bound']\n",
      "['code generation', 'binate covering', 'directed acyclic graph', 'instruction selection']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['code generation', 'instruction selection']\n",
      "\n",
      "\n",
      "optimal wiresizing for interconnects with multiple sources . the optimal wiresizing problem for nets with multiple sources is studied under the distributed elmore delay model . we decompose such a net into a source subtree ( sst ) and a set of loading subtrees ( lsts ) , and show the optimal wiresizing solution satisfies a number of interesting properties , including the lst separability , the lst monotone property , the sst local monotone property and the general dominance property . furthermore , we study the optimal wiresizing problem using a variable grid and reveal the bundled refinement property . these properties lead to efficient algorithms to compute the lower and upper bounds of the optimal solutions . experiment results on nets from an intel processor layout show an interconnect delay reduction of up to <digit> . <digit> % when compared to the minimum width solution . in addition , the algorithm based on a variable grid yields a speedup of two orders of magnitude without loss of accuracy , when compared with the fixed grid based methods .\n",
      "['optimal wiresizing', 'vlsi routing', 'performance driven layout', 'interconnect optimization <eos>']\n",
      "['multiple sources']\n",
      "['interconnect', 'wiresizing', 'source subtree', 'optimal wiresizing', 'elmore delay', 'distributed algorithms']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['optimal wiresizing']\n",
      "\n",
      "\n",
      "iteration abstraction in sather . sather extends the notion of an iterator in a powerful new way . we argue that iteration abstractions belong in class interfaces on an equal footing with routines . sather iterators were derived from clu iterators but are much more flexible and better suited for object oriented programming . we retain the property that iterators are structured , i . e . , strictly bound to a controlling structured statement . we motivate and describe the construct along with several simple examples . we compare it with iteration based on clu iterators , cursors , riders , streams , series , generators , coroutines , blocks , closures , and lambda expressions . finally , we describe experiences with iterators in the sather compiler and libraries .\n",
      "['iteration abstraction', 'sather', 'general control structures <eos>']\n",
      "['object oriented programming', 'iterator', 'iterators']\n",
      "['sather', 'abstraction', 'interfaces', 'class', 'property', 'examples', 'iteration abstraction', 'languages', 'design']\n",
      "3 9 12\n",
      "additional scores from absent units to present :  ['sather', 'iteration abstraction']\n",
      "\n",
      "\n",
      "the block distributed memory model . <unk> introduce a computation model for developing and analyzing parallel algorithms on distributed memory machines . the model allows the design of algorithms using a single address space and does not assume any particular interconnection topology . we capture performance by incorporating a cost measure for interprocessor communication induced by remote memory accesses . the cost measure includes parameters reflecting memory latency , communication bandwidth , and spatial locality . our model allows the initial placement of the input data and pipelined prefetching . we use our model to develop parallel algorithms for various data rearrangement problems , load balancing , sorting , fft , and matrix multiplication . we show that most of these algorithms achieve optimal or near optimal communication complexity while simultaneously guaranteeing an optimal speed up in computational complexity . ongoing experimental work in testing and evaluating these algorithms has thus far shown very promising results .\n",
      "['parallel algorithms', 'load balancing', 'sorting', 'matrix multiplication', 'parallel model', 'fast fourier transform', 'broadcasting', 'personalized communication <eos>']\n",
      "['parallel algorithms', 'distributed memory machines']\n",
      "['prefetching', 'matrix multiplication', 'parallel algorithms', 'distributed memory systems', 'performance evaluation']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['parallel algorithms', 'matrix multiplication', 'parallel algorithms']\n",
      "\n",
      "\n",
      "a necessary and sufficient condition for deadlock free routing in cut through and store and forward networks . <unk> paper develops the theoretical background for the design of deadlock free adaptive routing algorithms for virtual cut through and store and forward switching . this theory is valid for networks using either central buffers or edge buffers . some basic definitions and three theorems are proposed , developing conditions to verify that an adaptive algorithm is deadlock free , even when there are cyclic dependencies between routing resources . moreover , we propose a necessary and sufficient condition for deadlock free routing . also , a design methodology is proposed . it supplies fully adaptive , minimal and non minimal routing algorithms , guaranteeing that they are deadlock free . the theory proposed in this paper extends the necessary and sufficient condition for wormhole switching previously proposed by us . the resulting routing algorithms are more flexible than the ones for wormhole switching . also , the design methodology is much easier to apply because it automatically supplies deadlock free routing algorithms .\n",
      "['store and forward', 'adaptive routing', 'virtual cut through', 'design methodologies', 'interconnection networks', 'deadlock avoidance <eos>']\n",
      "['adaptive routing', 'deadlock free routing', 'virtual cut']\n",
      "['deadlock free routing', 'routing', 'adaptive routing', 'deadlock free routing']\n",
      "3 4 7\n",
      "additional scores from absent units to present :  ['adaptive routing', 'adaptive routing']\n",
      "\n",
      "\n",
      "localizing failures in distributed synchronization . <unk> fault tolerance of distributed algorithms is investigated in asynchronous message passing systems with undetectable process failures . two specific synchronization problems are considered , the dining philosophers problem and the binary committee coordination problem . the abstraction of a bounded doorway is introduced as a general mechanism for achieving individual progress and good failure locality . using it as a building block , optimal fault tolerant algorithms are constructed for the two problems .\n",
      "['synchronization', 'fault tolerance', 'distributed algorithms', 'lower bounds', 'concurrency <eos>']\n",
      "['undetectable process failures', 'asynchronous message passing', 'coordination']\n",
      "['synchronization', 'dining philosophers problem', 'fault tolerance', 'distributed algorithms', 'distributed algorithms', 'fault tolerance', 'asynchronous systems']\n",
      "3 7 10\n",
      "additional scores from absent units to present :  ['synchronization', 'fault tolerance', 'distributed algorithms', 'distributed algorithms', 'fault tolerance']\n",
      "\n",
      "\n",
      "static assignment of stochastic tasks using majorization . <unk> consider the problem of statically assigning many tasks to a ( smaller ) system of homogeneous processors , where a task ' s structure is modeled as a branching process , all tasks are assumed to have identical behavior , and the tasks may synchronize frequently . we show how the theory of majorization can be used to obtain a partial order among possible task assignments . we show that if the vector of numbers of tasks assigned to each processor under one mapping is majorized by that of another mapping , then the former mapping is better than the latter with respect to a large number of objective functions . in particular , we show how the metrics of finishing time , the space time product , and reliability are all captured . we also apply majorization to the problem of partitioning a pool of processors for distribution among parallelizable tasks . limitations of the approach , which include the static nature of the assignment , are also discussed .\n",
      "['majorization', 'task assignment', 'performance of parallel systems', 'processor allocation', 'task allocation', 'resource allocation', 'load balancing <eos>']\n",
      "['reliability']\n",
      "['majorization', 'assignment', 'stochastic tasks', 'static assignment', 'stochastic programming', 'task assignment', 'scheduling']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['majorization', 'task assignment']\n",
      "\n",
      "\n",
      "an analysis of the average message overhead in replica control protocols . <unk> of replicated data has received considerable attention in the last few years . several replica control schemes have been proposed which work in the presence of both node and communication link failures . however , this resiliency to failure inflicts a performance penalty in terms of the communication overhead incurred . though the issue of performance of these schemes from the standpoint of availability of the system has been well addressed , the issue of message overhead has been limited to the analysis of worst case and best case message bounds . in this paper we derive expressions for computing the average message overhead of several well known replica control protocols and provide a comparative study of the different protocols with respect to both average message overhead and system availabilities .\n",
      "['message overhead', 'replica control', 'availability', 'quorum consensus', 'replicated databases', 'update synchronization <eos>']\n",
      "['communication', 'performance']\n",
      "['replica control', 'message overhead', 'presence', 'average message overhead']\n",
      "2 4 6\n",
      "additional scores from absent units to present :  ['replica control', 'message overhead']\n",
      "\n",
      "\n",
      "parallel divide and conquer on meshes . <unk> address the problem of mapping divide and conquer programs to mesh connected multicomputers with wormhole or store and forward routing . we propose the binomial tree as an efficient model of parallel divide and conquer and present two mappings of the binomial tree to the 2d mesh . our mappings exploit regularity in the communication structure of the divide and conquer computation and are also sensitive to the underlying flow control scheme of the target architecture . we evaluate these mappings using new metrics which are extensions of the classical notions of dilation and contention . we introduce the notion of communication slowdown as a measure of the total communication overhead incurred by a parallel computation . we conclude that significant performance gains can be realized when the mapping is sensitive to the flow control scheme of the target architecture .\n",
      "['mapping', 'store and forward routing', 'routing', 'binomial tree', 'dilation', 'contention', 'divide and conquer algorithms', 'embedding', 'mesh connected machines', 'wormhole routing <eos>']\n",
      "['binomial tree']\n",
      "['mesh', 'dilation', 'mesh connected multicomputers', 'binomial tree', 'flow control', 'divide and conquer', 'parallel divide and conquer', 'mesh connected multicomputers', 'parallel computing', 'parallel computing']\n",
      "1 10 11\n",
      "additional scores from absent units to present :  ['binomial tree', 'dilation', 'binomial tree']\n",
      "\n",
      "\n",
      "file access characteristics of parallel scientific workloads . <unk> improvements in the computational performance of multiprocessors have not been matched by comparable gains in i o system performance . this imbalance has resulted in i o becoming a significant bottleneck for many scientific applications . one key to overcoming this bottleneck is improving the performance of multiprocessor file systems . the design of a high performance multiprocessor file system requires a comprehensive understanding of the expected workload . unfortunately , until recently , no general workload studies of multiprocessor file systems have been conducted . the goal of the charisma project was to remedy this problem by characterizing the behavior of several production workloads , on different machines , at the level of individual reads and writes . the first set of results from the charisma project describe the workloads observed on an intel ipsc <digit> and a thinking machines cm <digit> . this paper is intended to compare and contrast these two workloads for an understanding of their essential similarities and differences , isolating common trends and platform dependent variances . using this comparison , we are able to gain more insight into the general principles that should guide multiprocessor file system design .\n",
      "['multiprocessor', 'parallel i o', 'parallel file system', 'scientific computing', 'workload characterization <eos>']\n",
      "['multiprocessor file systems']\n",
      "['multiprocessor', 'scientific applications', 'file systems', 'parallel computing', 'file system performance', 'scientific computing', 'multiprocessor systems', 'performance evaluation']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['multiprocessor']\n",
      "\n",
      "\n",
      "parallelized direct execution simulation of message passing parallel programs . <unk> massively parallel computers proliferate , there is growing interest in finding ways by which performance of massively parallel codes can be efficiently predicted . this problem arises in diverse contexts such as parallelizing compilers , parallel performance monitoring , and parallel algorithm development . in this paper , we describe one solution where one directly executes the application code , but uses a discrete event simulator to model details of the presumed parallel machine , such as operating system and communication network behavior . because this approach is computationally expensive , we are interested in its own parallelization , specifically the parallelization of the discrete event simulator . we describe methods suitable for parallelized direct execution simulation of message passing parallel programs , and report on the performance of such a system , lapse ( large application parallel simulation environment ) , we have built on the intel paragon . on all codes measured to date , lapse predicts performance well , typically within <digit> % relative error . depending on the nature of the application code , we have observed low slowdowns ( relative to natively executing code ) and high relative speedups using up to <digit> processors .\n",
      "['direct execution simulation', 'parallel simulation', 'contention', 'message passing programs', 'synchronization', 'mimd', 'architectural simulation <eos>']\n",
      "['parallel computing', 'discrete event simulation']\n",
      "['parallel programs', 'message passing', 'direct execution simulation', 'parallel computing', 'parallel computing']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['direct execution simulation']\n",
      "\n",
      "\n",
      "on tridiagonalizing and diagonalizing symmetric matrices with repeated eigenvalues . we describe a divide and conquer tridiagonalization approach for matrices with repeated eigenvalues . our algorithm hinges on the fact that , under easily constructively verifiable conditions , a symmetric matrix with band width b and k distinct eigenvalues must be block diagonal with diagonal blocks of size at most b k . a slight modification of the usual orthogonal band reduction algorithm allows us to reveal this structure , which then leads to potential parallelism in the form of independent diagonal blocks . compared to the usual householder reduction algorithm , the new approach exhibits improved data locality , significantly more scope for parallelism , and the potential to reduce arithmetic complexity by close to <digit> % for matrices that have only two numerically distinct eigenvalues . the actual improvement depends to a large extent on the number of distinct eigenvalues and a good estimate thereof . however , at worst the algorithms behave like a successive band reduction approach to tridiagonalization . moreover , we provide a numerically reliable and effective algorithm for computing the eigenvalue decomposition of a symmetric matrix with two numerically distinct eigenvalues . such matrices arise , for example , in invariant subspace decomposition approaches to the symmetric eigenvalue problem .\n",
      "['tridiagonalization', 'repeated eigenvalues', 'eigenvalue decomposition <eos>']\n",
      "['eigenvalue decomposition']\n",
      "['repeated eigenvalues', 'tridiagonalizing', 'symmetric matrix', 'divide and conquer', 'matrix decomposition', '<unk>']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['repeated eigenvalues']\n",
      "\n",
      "\n",
      "group invariance and convex matrix analysis . certain interesting classes of functions on a real inner product space are invariant under an associated group of orthogonal linear transformations . this invariance can be made explicit via a simple decomposition . for example , rotationally invariant functions on bf r <digit> are just even functions of the euclidean norm , and functions on the hermitian matrices ( with trace inner product ) which are invariant under unitary similarity transformations are just symmetric functions of the eigenvalues . we develop a framework for answering geometric and analytic ( both classical and nonsmooth ) questions about such a function by answering the corresponding question for the ( much simpler ) function appearing in the decomposition . the aim is to understand and extend the foundations of eigenvalue optimization , matrix approximation , and semidefinite programming .\n",
      "['group invariance', 'convexity', 'eigenvalue optimization', 'semidefinite program', 'fenchel conjugate', 'nonsmooth analysis', 'unitarily invariant norm', 'subdifferential', 'extreme point', \"von neumann ' s lemma\", 'spectral function', 'schur convex <eos>']\n",
      "['hermitian matrices', 'eigenvalue optimization', 'orthogonal linear transformations']\n",
      "['semidefinite programming', 'matrix approximation', 'hermitian matrices', 'group invariance', '<unk>', 'convex optimization', 'group theory', '<unk>']\n",
      "3 8 11\n",
      "additional scores from absent units to present :  ['group invariance']\n",
      "\n",
      "\n",
      "a persistent rescheduled page cache for low overhead object code compatibility in vliw architectures . object code compatibility between processor generations is an open issue for vliw architectures . a potential solution is a technique termed dynamic rescheduling , which performs run time software rescheduling at the first time page faults . the time required for rescheduling the pages constitutes a large portion of the overhead of this method . a disk caching scheme that uses a persistent rescheduled page cache ( prc ) is presented . the scheme reduces the overhead associated with dynamic rescheduling by saving rescheduled pages on disk , across program executions . operating system support is required for dynamic rescheduling and management of the prc . the implementation details for the prc are discussed . results of simulations used to gauge the effectiveness of prc indicate that the prc is effective in reducing the overhead of dynamic rescheduling and due to different overhead requirements of programs , a split prc organization performs better than a unified prc . the unified prc was studied for two different page replacement policies lru and overhead based replacement . it was found that with lru replacement , all the programs consistently perform better with increasing prc sizes , but the high overhead programs take a consistent performance hit compared to the low overhead programs . with overhead based replacement , the performance of high overhead programs improves substantially , while the low overhead programs perform only slightly worse than in the case of the lru replacement .\n",
      "['persistent rescheduled page cache', 'low overhead object code compatibility', 'vliw architectures', 'dynamic rescheduling', 'run time software rescheduling', 'first time page faults', 'disk caching scheme', 'program executions', 'operating system support', 'simulations', 'page replacement policies', 'overhead based replacement', 'lru replacement', 'high overhead programs', 'program performance', 'cache storage <eos>']\n",
      "['object code compatibility']\n",
      "['vliw architectures', 'rescheduling', 'object code compatibility']\n",
      "1 3 4\n",
      "additional scores from absent units to present :  ['vliw architectures']\n",
      "\n",
      "\n",
      "modulo scheduling of loops in control intensive non numeric programs . much of the previous work on modulo scheduling has targeted numeric programs , in which , often , the majority of the loops are well behaved loop counter based loops without early exits . in control intensive non numeric programs , the loops frequently have characteristics that make it more difficult to effectively apply modulo scheduling . these characteristics include multiple control flow paths , loops that are not based on a loop counter , and multiple exits . in these loops , the presence of unimportant paths with high resource usage or long dependence chains can penalize the important paths . a path that contains a hazard such as another nested loop can prohibit modulo scheduling of the loop . control dependences can severely restrict the overlap of the blocks within and across iterations . this paper describes a set of methods that allow effective modulo scheduling of loops with multiple exits . the techniques include removal of control dependences to enable speculation , extensions to modulo variable expansion , and a new epilogue generation scheme . these methods can be used with superblock and hyperblock techniques to allow modulo scheduling of the selected paths of loops with arbitrary control flow . a case study is presented to show how these methods , combined with superblock techniques , enable modulo scheduling to be effectively applied to control intensive non numeric programs . performance results for several spec cint92 benchmarks and unix utility programs are reported and demonstrate the applicability of modulo scheduling to this class of programs .\n",
      "['modulo scheduling', 'control intensive', 'speculation', 'modulo variable expansion', 'instruction level parallelism', 'software pipelining <eos>']\n",
      "['non numeric programs']\n",
      "['scheduling', 'control flow', 'modulo scheduling', 'loop counter', 'non numeric programs']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['modulo scheduling']\n",
      "\n",
      "\n",
      "assigning confidence to conditional branch predictions . many high performance processors predict conditional branches and consume processor resources based on the prediction . in some situations , resource allocation can be better optimized if a confidence level is assigned to a branch prediction i . e . if the quantity of resources allocated is a function of the confidence level . to support such optimizations , we consider hardware mechanisms that partition conditional branch predictions into two sets those which are accurate a relatively high percentage of the time , and those which are accurate a relatively low percentage of the time . the objective is to concentrate as many of the mispredictions as practical into a relatively small set of low confidence dynamic branches . we first study an ideal method that profiles branch predictions and sorts static branches into high and low confidence sets , depending on the accuracy with which they are dynamically predicted . we find that about <digit> percent of the mispredictions can be localized to a set of static branches that account for <digit> percent of the dynamic branches . we then study idealized dynamic confidence methods using both one and two levels of branch correctness history . we find that the single level method performs at least as well as the more complex two level method and is able to isolate <digit> percent of the mispredictions into a set containing <digit> percent of the dynamic branches . finally , we study practical , less expensive implementations and find that they achieve most of the performance of the idealized methods .\n",
      "['conditional branch predictions', 'processor resources', 'resource allocation', 'dynamic branches', 'static branches', 'branch correctness <eos>']\n",
      "['branch prediction']\n",
      "['conditional branch predictions', 'confidence', 'resource allocation', 'conditional prediction', 'branch and bound', 'performance evaluation']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['conditional branch predictions', 'resource allocation']\n",
      "\n",
      "\n",
      "instruction fetch mechanisms for vliw architectures with compressed encodings . vliw architectures use very wide instruction words in conjunction with high bandwidth to the instruction cache to achieve multiple instruction issue . this report uses the tinker experimental testbed to examine instruction fetch and instruction cache mechanisms for vliws . a compressed instruction encoding for vliws is defined and a classification scheme for i fetch hardware for such an encoding is introduced . several interesting cache and i fetch organizations are described and evaluated through trace driven simulations . a new i fetch mechanism using a silo cache is found to have the best performance .\n",
      "['instruction fetch mechanisms', 'vliw architectures', 'compressed encodings', 'instruction words', 'instruction cache', 'multiple instruction issue', 'tinker experimental testbed', 'compressed instruction encoding', 'i fetch hardware', 'trace driven simulations', 'silo cache', 'parallel architectures <eos>']\n",
      "['cache', 'tinker']\n",
      "['compressed encodings', 'vliw', 'vliw architectures', 'use', 'instruction fetch', 'instruction cache']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['compressed encodings', 'vliw architectures', 'instruction cache']\n",
      "\n",
      "\n",
      "estimating optical flow in segmented images using variable order parametric models with local deformations . <unk> paper presents a new model for estimating optical flow based on the motion of planar regions plus local deformations . the approach exploits brightness information to organize and constrain the interpretation of the motion by using segmented regions of piecewise smooth brightness to hypothesize planar regions in the scene . parametric flow models are estimated in these regions in a two step process which first computes a coarse fit and estimates the appropriate parameterization of the motion of the region ( two , six , or eight parameters ) . the initial fit is refined using a generalization of the standard area based regression approaches . since the assumption of planarity is likely to be violated , we allow local deformations from the planar assumption in the same spirit as physically based approaches which model shape using coarse parametric models plus local deformations . this parametric deformation model exploits the strong constraints of parametric approaches while retaining the adaptive nature of regularization approaches . experimental results on a variety of images indicate that the parametric deformation model produces accurate flow estimates while the incorporation of brightness segmentation provides precise localization of motion boundaries .\n",
      "['optical flow', 'segmentation', 'local deformation', 'parameterized flow models', 'robust regression <eos>']\n",
      "['parametric models']\n",
      "['local deformations', 'variable order parametric models', 'optical flow', 'optical flow', 'parametric deformation model', 'image segmentation']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['optical flow', 'optical flow']\n",
      "\n",
      "\n",
      "scale space properties of quadratic feature detectors . <unk> detectors using a quadratic nonlinearity in the filtering stage are known to have some advantages over linear detectors here , we consider their scale space properties . in particular , we investigate whether , like linear detectors , quadratic feature detectors permit a scale selection scheme with the causality property , which guarantees that features are never created as scale is coarsened . we concentrate on the design most common in practice , i . e . , one dimensional detectors with two constituent filters , with scale selection implemented as convolution with a scaling function . we consider two special cases of interest constituent filter pairs related by the hilbert transform , and by the first spatial derivative . we show that , under reasonable assumptions , hilbert pair quadratic detectors can not have the causality property . in the case of derivative pair detectors , we describe a family of scaling functions related to fractional derivatives of the gaussian that are necessary and sufficient for causality . in addition , we report experiments that show the effects of these properties in practice . thus we show that at least one class of quadratic feature detectors has the same desirable scaling property as the more familiar detectors based on linear filtering .\n",
      "['scale space', 'causality', 'energy filters', 'edge detection', 'feature detection', 'nonlinear filtering', 'quadratic filters <eos>']\n",
      "['hilbert transform']\n",
      "['quadratic nonlinearity', 'causality', 'scale selection', 'quadratic filtering', 'scale space']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['causality', 'scale space']\n",
      "\n",
      "\n",
      "a framework for resource constrained rate optimal software pipelining . <unk> rapid advances in high performance computer architecture and compilation techniques provide both challenges and opportunities to exploit the rich solution space of software pipelined loop schedules . in this paper , we develop a framework to construct a software pipelined loop schedule which runs on the given architecture ( with a fixed number of processor resources ) at the maximum possible iteration rate ( la rate optimal ) while minimizing the number of <unk> close approximation to minimizing the number of registers . the main contributions of this paper are first , we demonstrate that such problem can be described by a simple mathematical formulation with precise optimization objectives under a periodic linear scheduling framework . the mathematical formulation provides a clear picture which permits one to visualize the overall solution space ( for rate optimal schedules ) under different sets of constraints . secondly , we show that a precise mathematical formulation and its solution does make a significant performance difference . we evaluated the performance of our method against three leading contemporary heuristic methods . experimental results show that the method described in this paper performed significantly better than these methods . the techniques proposed in this paper are useful in two different ways <digit> ) as a compiler option which can be used in generating faster schedules for performance critical loops ( if the interested users are willing to trade the cost of longer compile time with faster runtime ) . <digit> ) as a framework for compiler writers to evaluate and improve other heuristics based approaches by providing quantitative information as to where and how much their heuristic methods could be further improved .\n",
      "['software pipelining', 'integer linear programming', 'superscalar and vliw architectures', 'instruction scheduling', 'instruction level parallelism <eos>']\n",
      "['software pipelining']\n",
      "['software pipelining', 'rate optimal', 'resource constrained rate optimal scheduling', 'rate optimal scheduling', 'resource constrained optimization', 'parallel computing', 'performance optimization']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['software pipelining', 'software pipelining']\n",
      "\n",
      "\n",
      "on the fully commutative elements of coxeter groups . let w be a coxeter group . we define an element w w to be fully commutative if any reduced expression for w can be obtained from any other by means of braid relations that only involve commuting generators . we give several combinatorial characterizations of this property , classify the coxeter groups with finitely many fully commutative elements , and classify the parabolic quotients whose members are all fully commutative . as applications of the latter , we classify all parabolic quotients with the property that ( <digit> ) the bruhat ordering is a lattice , ( <digit> ) the bruhat ordering is a distributive lattice , ( <digit> ) the weak ordering is a distributive lattice , and ( <digit> ) the weak ordering and bruhat ordering coincide .\n",
      "['coxeter group', 'bruhat order', 'weak order', 'reduced word <eos>']\n",
      "['parabolic quotients', 'commuting generators']\n",
      "['coxeter group', 'distributive lattice', 'bruhat ordering', 'bruhat ordering', 'fully commutative elements', 'fully commutative elements', 'parabolic lattice', 'bruhat ordering', '<unk>']\n",
      "2 9 11\n",
      "additional scores from absent units to present :  ['coxeter group']\n",
      "\n",
      "\n",
      "analytical delay models for vlsi interconnects under ramp input . elmore delay has been widely used as an analytical estimate of interconnect delays in the performance driven synthesis and layout of vlsi routing topologies . however , for typical rlc interconnections with ramp input , elmore delay can deviate by up to <digit> % or more from spice computed delay since it is independent of rise time of the input ramp signal . we develop new analytical delay models based on the first and second moments of the interconnect transfer function when the input is a ramp signal with finite rise time . delay estimates using our first moment based analytical models are within <digit> % of spice computed delay , and models based on both first and second moments are within <digit> . <digit> % of spice , across a wide range of interconnect parameter values . evaluation of our analytical models is several orders of magnitude faster than simulation using spice . we also describe extensions of our approach for estimation of source sink delays in arbitrary interconnect trees .\n",
      "['analytical delay models', 'vlsi', 'vlsi interconnects', 'ramp input', 'elmore delay', 'interconnect delays', 'performance driven synthesis', 'rlc interconnections', 'spice computed delay', 'interconnect transfer function', 'source sink delays', 'arbitrary interconnect trees', 'vlsi routing topologies layout <eos>']\n",
      "['Present predictions:']\n",
      "['ramp input', 'interconnect', 'vlsi interconnects', 'performance driven synthesis', 'analytical delay model', 'ramp input', 'interconnect modeling']\n",
      "1 7 7\n",
      "additional scores from absent units to present :  ['ramp input', 'vlsi interconnects', 'performance driven synthesis', 'ramp input']\n",
      "\n",
      "\n",
      "register transfer level estimation techniques for switching activity and power consumption . we present techniques for estimating switching activity and power consumption in register transfer level ( rtl ) circuits . previous work on this topic has ignored the presence of glitching activity at various data path and control signals , which can lead to significant underestimation of switching activity . for data path blocks that operate on word level data , we construct piecewise linear models that capture the variation of output glitching activity and power consumption with various word level parameters like mean , standard deviation , spatial and temporal correlations , and glitching activity at the block ' s inputs . for rtl blocks that operate on data that need not have an associated word level value , we present accurate bit level modeling techniques for glitching activity as well as power consumption . this allows us to perform accurate power estimation for control flow intensive circuits , where most of the power consumed is dissipated in non arithmetic components like multiplexers , registers , vector logic operators , etc . since the final implementation of the controller is not available during high level design iterations , we develop techniques that estimate glitching activity at control signals using control expressions and partial delay information . experiments on example rtl designs resulted in power estimates that were within <digit> % of those produced by an inhouse power analysis tool on the final gate level implementation .\n",
      "['register transfer level estimation', 'switching activity', 'power consumption', 'glitching', 'rtl designs', 'gate level implementation', 'logic design <eos>']\n",
      "['vector logic']\n",
      "['power consumption', 'switching activity', 'register transfer level', 'low power design', 'power estimation']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['power consumption', 'switching activity']\n",
      "\n",
      "\n",
      "an efficient approach to simultaneous transistor and interconnect sizing . in this paper , we study the simultaneous transistor and interconnect sizing ( stis ) problem . we define a class of optimization problems as ch posynomial programs and reveal a general dominance property for all ch posynomial programs . we show that the stis problems under a number of transistor delay models are ch posynomial programs and propose an efficient and near optimal stis algorithm based on the dominance property . when used to solve the simultaneous driver buffer and wire sizing problem for real designs , it reduces the maximum delay by up to <digit> . <digit> % , and more significantly , reduces the power consumption by a factor of <digit> . 63x , when compared with the original designs . when used to solve the transistor sizing problem , it achieves a smooth area delay trade off . moreover , the algorithm optimizes a clock net of <digit> drivers buffers and <digit> spl mu m long wire in <digit> seconds , and a <digit> bit adder with <digit> transistors in <digit> seconds on a sparc <digit> workstation .\n",
      "['transistor and interconnect sizing', 'stis', 'ch posynomial programs', 'driver buffer', 'wire sizing problem', 'transistor sizing', 'circuit cad <eos>']\n",
      "['Present predictions:']\n",
      "['interconnect sizing', 'posynomial', 'transistor sizing', 'simultaneous transistor and interconnect sizing', 'interconnect optimization', 'design automation']\n",
      "1 6 6\n",
      "additional scores from absent units to present :  ['transistor sizing']\n",
      "\n",
      "\n",
      "vlsi circuit partitioning by cluster removal using iterative improvement techniques . move based iterative improvement partitioning methods such as the fiduccia mattheyses ( fm ) algorithm and krishnamurthy ' s look ahead ( la ) algorithm are widely used in vlsi cad applications largely due to their time efficiency and ease of implementation . this class of algorithms is of the local improvement type . they generate relatively high quality results for small and medium size circuits . however , as vlsi circuits become larger , these algorithms are not so effective on them as direct partitioning tools . we propose new iterative improvement methods that select cells to move with a view to moving clusters that straddle the two subsets of a partition into one of the subsets . the new algorithms significantly improve partition quality while preserving the advantage of time efficiency . experimental results on <digit> medium to large size acm sigda benchmark circuits show up to <digit> % improvement over fm in cutsize , with an average of per circuit percent improvements of about <digit> % , and a total cut improvement of about <digit> % . they also outperform the recent placement based partitioning tool paraboli and the spectral partitioner melo by about <digit> % and <digit> % , respectively , with less cpu time . this demonstrates the potential of iterative improvement algorithms in dealing with the increasing complexity of modern vlsi circuitry .\n",
      "['vlsi', 'vlsi circuit partitioning', 'cluster removal', 'iterative improvement techniques', 'cad', 'partition quality', 'acm sigda benchmark circuits', 'spectral partitioner melo', 'look ahead algorithm', 'fiduccia mattheyses algorithm <eos>']\n",
      "['algorithms']\n",
      "['iterative improvement', 'cluster removal', 'vlsi circuit partitioning', 'vlsi design', 'design for testability', 'circuit optimization']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['cluster removal', 'vlsi circuit partitioning']\n",
      "\n",
      "\n",
      "implementing fail silent nodes for distributed systems . abstracta fail silent node is a self checking node that either functions correctly or stops functioning after an internal failure is detected . such a node can be constructed from a number of conventional processors . in a software implemented fail silent node , the nonfaulty processors of the node need to execute message order and comparison protocols to keep in step and check each other , respectively . in this paper , the design and implementation of efficient protocols for a two processor fail silent node are described in detail . the performance figures obtained indicate that in a wide class of applications requiring a high degree of fault tolerance , software implemented fail silent nodes constructed simply by utilizing standard off the shelf components are an attractive alternative to their hardware implemented counterparts that do require special purpose hardware components , such as fault tolerant clocks , comparator , and bus interface circuits .\n",
      "['fault tolerance', 'distributed processing', 'reliability', 'fail silence', 'replicated processing <eos>']\n",
      "['distributed systems']\n",
      "['silent node', 'fault tolerance', 'fail silent node', 'fail silent node', 'fault tolerance', 'fault tolerance', 'distributed systems']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['fault tolerance', 'fault tolerance', 'fault tolerance']\n",
      "\n",
      "\n",
      "rigidity checking of 3d point correspondences under perspective projection . <unk> algorithm is described which rapidly verifies the potential rigidity of three dimensional point correspondences from a pair of two dimensional views under perspective projection . the output of the algorithm is a simple yes or no answer to the question could these corresponding points from two views be the projection of a rigid configuration potential applications include 3d object recognition from a single previous view and correspondence matching for stereo or motion over widely separated views . the rigidity checking problem is different from the structure from motion problem because it is often the case that two views can not provide an accurate structure from motion estimate due to ambiguity and ill conditioning , whereas it is still possible to give an accurate yes no answer to the rigidity question . rigidity checking verifies point correspondences using 3d recovery equations as a matching condition . the proposed algorithm improves upon other methods that fall under this approach because it works with as few as six corresponding points under full perspective projection , handles correspondences from widely separated views , makes full use of the disparity of the correspondences , and is integrated with a linear algorithm for 3d recovery due to <unk> . results are given for experiments with synthetic and real image data . a complete implementation of this algorithm is being made publicly available .\n",
      "['rigidity checking', 'point correspondences', 'perspective projection', 'structure from motion', 'image matching', 'nonlinear parameter estimation <eos>']\n",
      "['Present predictions:']\n",
      "['perspective projection', '3d point correspondences', 'point correspondences', 'rigidity checking', '3d object recognition']\n",
      "1 5 5\n",
      "additional scores from absent units to present :  ['perspective projection', 'point correspondences', 'rigidity checking']\n",
      "\n",
      "\n",
      "real time focus range sensor . <unk> of dynamic scenes can only be recovered using a real time range sensor . depth from defocus offers an effective solution to fast and dense range estimation . however , accurate depth estimation requires theoretical and practical solutions to a variety of problems including recovery of textureless surfaces , precise blur estimation , and magnification variations caused by defocusing . both textured and textureless surfaces are recovered using an illumination pattern that is projected via the same optical path used to acquire images . the illumination pattern is optimized to maximize accuracy and spatial resolution in computed depth . the relative blurring in two images is computed using a narrow band linear operator that is designed by considering all the optical , sensing , and computational elements of the depth from defocus system . defocus invariant magnification is achieved by the use of an additional aperture in the imaging optics . a prototype focus range sensor has been developed that has a workspace of <digit> cubic foot and produces up to <digit> <digit> depth estimates at hz with an average rms error of <digit> . <digit> % . several experimental results are included to demonstrate the performance of the sensor .\n",
      "['real time range sensor', 'depth from defocus', 'depth estimation', 'image sensing', 'constant magnification defocusing', 'active illumination pattern', 'optical transfer function', 'tuned focus operator <eos>']\n",
      "['depth from defocus']\n",
      "['range sensor', 'real time', 'real time sensor', 'image processing', 'depth estimation', 'optical sensing']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['depth estimation']\n",
      "\n",
      "\n",
      "an application of petri net reduction for ada tasking deadlock analysis . <unk> part of our continuing research on using petri nets to support automated analysis of ada tasking behavior , we have investigated the application of petri net reduction for deadlock analysis . although reachability analysis is an important method to detect deadlocks , it is in general inefficient or even intractable . net reduction can aid the analysis by reducing the size of the net while preserving relevant properties . we introduce a number of reduction rules and show how they can be applied to ada nets , which are automatically generated petri net models of ada tasking . we define a reduction process and a method by which a useful description of a detected deadlock state can be obtained from the reduced net ' s information . a reduction tool and experimental results from applying the reduction process are discussed .\n",
      "['petri nets', 'net reduction', 'ada tasking', 'deadlock analysis', 'reachability analysis', 'concurrent software <eos>']\n",
      "['deadlock analysis', 'tasking']\n",
      "['ada tasking', 'deadlock', 'reachability analysis', 'petri net reduction']\n",
      "2 4 6\n",
      "additional scores from absent units to present :  ['ada tasking', 'reachability analysis']\n",
      "\n",
      "\n",
      "detection of strong unstable predicates in distributed programs . <unk> paper discusses detection of global predicates in a distributed program . a run of a distributed program results in a set of sequential traces , one for each process . these traces may be combined to form many global sequences consistent with the single run of the program . a strong global predicate is true in a run if it is true for all global sequences consistent with the run . we present algorithms which detect if the given strong global predicate became true in a run of a distributed program . our algorithms can be executed on line as well as off line . moreover , our algorithms do not assume that underlying channels satisfy fifo ordering .\n",
      "['unstable predicates', 'distributed algorithms', 'distributed debugging', 'predicate detection <eos>']\n",
      "['distributed']\n",
      "['timing', 'detection', 'unstable predicates', 'process', 'paper', 'global', 'distributed algorithms']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['unstable predicates']\n",
      "\n",
      "\n",
      "generation of high quality tests for robustly untestable path delay faults . <unk> many designs a large portion of path delay faults is not robustly testable . in this paper , we investigate testing strategies for robustly untestable faults . we show that the quality of nonrobust tests may be very poor in detecting small defects caused by manufacturing process variation . we demonstrate that better quality nonrobust tests can be obtained by including timing information into the process of test generation . a good nonrobust test can tolerate larger timing variations on the off inputs . we also show that not all <unk> untestable path delay faults may be ignored in high quality delay testing . functional sensitizable paths are <unk> untestable but , under some faulty conditions , may degrade the performance of the circuit . however , up till now , there was no strategy for generating tests for such faults . in this paper , we present algorithms for generating high quality nonrobust and functional sensitizable tests . we also devise an algorithm for generating tests for validatable nonrobust faults which have a high quality in detecting defects but are hard to be generated automatically . our experimental results show that the quality of delay testing increases if validatable and high quality nonrobust tests , as well as tests for functional sensitizable path delay faults are included .\n",
      "['path delay faults', 'nonrobust', 'delay testing', 'vlsi testing', 'timing defects', 'automatic test generation', 'robust <eos>']\n",
      "['Present predictions:']\n",
      "['test generation', 'path delay faults', 'high quality tests', 'test generation']\n",
      "1 4 4\n",
      "additional scores from absent units to present :  ['path delay faults']\n",
      "\n",
      "\n",
      "stability radii of systems with stochastic uncertainty and their optimization by output feedback . we consider linear plants controlled by dynamic output feedback which are subjected to <unk> stochastic parameter perturbations . the stability radii of these systems are characterized , and it is shown that , for real data , the real and the complex stability radii coincide . a corresponding result does not hold in the deterministic case , even for perturbations of single output feedback type . in a second part of the paper we study the problem of optimizing the stability radius by dynamic linear output feedback . necessary and sufficient conditions are derived for the existence of a compensator which achieves a suboptimal stability radius . these conditions consist of a parametrized riccati equation , a parametrized liapunov inequality , a coupling inequality , and a number of linear matrix inequalities ( one for each disturbance term ) . the corresponding problem in the deterministic case , the optimal mu synthesis problem , is still unsolved .\n",
      "['dynamic output feedback', 'stability radius', 'linear matrix inequalities', 'riccati inequalities', 'state dependent noise', 'scaling', 'stochastic systems', 'multiperturbations <eos>']\n",
      "['linear matrix inequality', 'stochastic parameter perturbations']\n",
      "['output feedback', 'optimization', 'parametrized riccati equation', 'stochastic uncertainty', 'dynamic output feedback', 'stability radius', 'stochastic systems']\n",
      "2 7 9\n",
      "additional scores from absent units to present :  ['dynamic output feedback', 'stability radius']\n",
      "\n",
      "\n",
      "finite difference preconditioning for solving orthogonal collocation equations for boundary value problems . a technique to construct a low order finite difference preconditioner for solving orthogonal collocation equations for boundary value problems is presented . it is shown numerically and theoretically that the spectral condition numbers of the preconditioned collocation matrices are bounded by constants independent of the number of mesh nodes when certain exact low order finite difference preconditionings are used . preconditioners based on incomplete lu factorization are also discussed . numerical experiments show the efficiency and robustness of the preconditioning .\n",
      "['preconditioning', 'collocation', 'boundary value problem <eos>']\n",
      "['collocation equations', 'orthogonal collocation equations']\n",
      "['boundary value problems', 'preconditioning', 'finite difference', 'krylov subspace methods', 'boundary value problems', 'finite difference method', 'orthogonal collocation method', '<unk>']\n",
      "2 8 10\n",
      "additional scores from absent units to present :  ['preconditioning']\n",
      "\n",
      "\n",
      "an evolutionary approach to constructing effective software reuse repositories . repositories for software reuse are faced with two interrelated problems ( <digit> ) acquiring the knowledge to initially construct the repository and ( <digit> ) modifying the repository to meet the evolving and dynamic needs of software development organizations . current software repository methods rely heavily on classification , which exacerbates <unk> and evolution problems by requiring costly classification and domain analysis efforts before a repository can be used effectively , this article outlines an approach that avoids these problems by choosing a retrieval method that utilizes minimal repository structure to effectively support the process of finding software <unk> . the approach is demonstrated through a pair of proof of concept prototypes peel , a tool to semiautomatically identify reusable components , and codefinder , a retrieval system that compensates for the lack of explicit knowledge structures through a spreading activation retrieval process . codefinder also allows component representations to be modified while users are searching for information . this mechanism adapts to the changing nature of the information in the repository and incrementally improves the repository while people use it . the combination of these techniques holds potential for designing software repositories that minimize up front costs , effectively support the search process , and evolve with an organization ' s changing needs .\n",
      "['software reuse', 'component repositories', 'information retrieval <eos>']\n",
      "['software reuse']\n",
      "['software reuse', 'design', 'evolutionary computation', 'software evolution', 'algorithms']\n",
      "1 5 6\n",
      "additional scores from absent units to present :  ['software reuse', 'software reuse']\n",
      "\n",
      "\n",
      "a graduated assignment algorithm for graph matching . abstracta graduated assignment algorithm for graph matching is presented which is fast and accurate even in the presence of high noise . by combining graduated nonconvexity , two way ( assignment ) constraints , and sparsity , large improvements in accuracy and speed are achieved . its low order computational complexity o ( lm ) , where l and m are the number of links in the two graphs and robustness in the presence of noise offer advantages over traditional combinatorial approaches . the algorithm , not restricted to any special class of graph , is applied to subgraph isomorphism , weighted graph matching , and attributed relational graph matching . to illustrate the performance of the algorithm , attributed relational graphs derived from objects are matched . then , results from twenty five thousand experiments conducted on <digit> node random graphs of varying types ( graphs with only zero one links , weighted graphs , and graphs with node attributes and multiple link types ) are reported . no comparable results have been reported by any other graph matching algorithm before in the research literature . twenty five hundred control experiments are conducted using a relaxation labeling algorithm and large improvements in accuracy are demonstrated .\n",
      "['graduated assignment', 'graph matching', 'weighted graphs', 'attributed relational graphs', 'relaxation labeling', 'model matching', 'softassign', 'continuation method <eos>']\n",
      "['attributed relational graph matching']\n",
      "['graph matching', 'assignment algorithm', 'graduated assignment', 'graph matching']\n",
      "1 4 5\n",
      "additional scores from absent units to present :  ['graph matching', 'graduated assignment', 'graph matching']\n",
      "\n",
      "\n",
      "stochastic scheduling with variable profile and precedence constraints . in this paper , we consider the stochastic profile scheduling problem of a partially ordered set of tasks on uniform processors . the set of available processors varies in time . the running times of the tasks are independent random variables with exponential distributions . we obtain a sufficient condition under which a list policy stochastically minimizes the makespan within the class of preemptive policies . this result allows us to obtain a simple optimal policy when the partial order is an interval order , an in forest , or an out forest .\n",
      "['stochastic scheduling', 'precedence constraint', 'profile scheduling', 'uniform processors', 'makespan', 'interval order', 'in forest', 'out forest', 'stochastic ordering <eos>']\n",
      "['Present predictions:']\n",
      "['precedence constraints', 'scheduling', 'variable profile', 'profile scheduling', 'stochastic scheduling', 'stochastic programming']\n",
      "1 6 6\n",
      "additional scores from absent units to present :  ['profile scheduling', 'stochastic scheduling']\n",
      "\n",
      "\n",
      "on bounded queries and approximation . this paper investigates the computational complexity of approximating several np optimization problems using the number of queries to an np oracle as a complexity measure . the results show a tradeoff between the closeness of the approximation and the number of queries required . for an approximation factor k ( n ) , log log k ( n ) n queries to an np oracle can be used to approximate the maximum clique size of a graph within a factor of k ( n ) . however , this approximation can not be achieved using fewer than log log k ( n ) n c queries to any oracle unless is a constant that does not depend on k . these results hold for approximation factors k ( n ) geq <digit> that belong to a class of functions which includes any integer constant function , log n , log a n , and n <digit> a . similar results are obtained for graph coloring , set cover , and other np optimization problems .\n",
      "['bounded queries', 'maximum clique', 'set cover', 'chromatic number', 'np completeness', 'approximation algorithm <eos>']\n",
      "['computational complexity', 'complexity']\n",
      "['approximation', 'queries', 'class', 'graph coloring', 'paper', 'bounded queries']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['bounded queries']\n",
      "\n",
      "\n",
      "reducing communication latency with path multiplexing in optically interconnected multiprocessor systems . <unk> communication latency , which is a performance bottleneck in optically interconnected multiprocessor systems , is of prominent importance . a conventional approach for establishing connections in multiplexed networks uses a set of independent time slots ( or virtual channels ) along a path for each connection . this approach requires the use of switching devices capable of interchanging time slots , and thus introduces latency in addition to hardware and control complexity . in this paper , we propose an approach to all optical time division multiplexed ( tdm ) communications in multiprocessor systems . the idea is to establish a connection along a path using a set of time slots ( or virtual channels ) that are dependent on each other , so that no time slot interchanging is required . we compare the proposed approach with the conventional one in terms of the overall communication latency . we found that , despite the possibility that establishing a connection may take a longer time , the proposed approach will result in lower overall communication latency as it eliminates the delays introduced by the time slot interchanging switching devices .\n",
      "['communication latency', 'time division multiplexing', 'time slot interchangers', 'fiber optical interconnects', 'switching networks <eos>']\n",
      "['independent time slots']\n",
      "['time slot', 'path multiplexing', 'communication latency', 'optically interconnected systems', 'interconnection networks', 'optical time division multiplexing']\n",
      "1 6 7\n",
      "additional scores from absent units to present :  ['communication latency']\n",
      "\n",
      "\n",
      "a general method for maximizing the error detecting ability of distributed algorithms . <unk> bound on component failures and their spatial distribution govern the fault tolerance of any candidate error detecting algorithm . for distributed memory multiprocessors , the specific algorithm and the topology of the processor interconnection network define these bounds . this paper introduces the maximal fault index , derived from the system topology and local communication patterns , to demonstrate how a maximal number of simultaneous component failures can be tolerated for a particular interconnection network and error detecting algorithm . the index is used to design a mapping of processes to processor groups such that the error detecting ability of the algorithm is preserved for certain multiple simultaneous processor failures .\n",
      "['error detection', 'fault tolerance', 'mapping', 'multicomputers', 'fault tolerant algorithms', 'architecture <eos>']\n",
      "['interconnection networks', 'distributed memory multiprocessors']\n",
      "['maximal fault index', 'fault tolerance', 'fault tolerance', 'fault tolerance', 'distributed systems']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['fault tolerance', 'fault tolerance', 'fault tolerance']\n",
      "\n",
      "\n",
      "on runtime parallel scheduling for processor load balancing . <unk> scheduling is a new approach for load balancing . in parallel scheduling , all processors cooperate to schedule work . parallel scheduling is able to accurately balance the load by using global load information at compile time or runtime . it provides high quality load balancing . this paper presents an overview of the parallel scheduling technique . scheduling algorithms for tree , hypercube , and mesh networks are presented . these algorithms can fully balance the load and maximize locality at runtime . communication costs are significantly reduced compared to other existing algorithms .\n",
      "['runtime parallel scheduling', 'load balancing', 'scheduling algorithms', 'trees', 'hypercubes', 'meshes', 'distributed memory computers <eos>']\n",
      "['mesh networks']\n",
      "['processor load balancing', 'scheduling', 'parallel scheduling', 'load balancing', 'load balancing', 'runtime', 'runtime scheduling', 'parallel processing']\n",
      "1 8 9\n",
      "additional scores from absent units to present :  ['load balancing', 'load balancing']\n",
      "\n",
      "\n",
      "an empirical evaluation of performance memory trade offs in time warp . <unk> performance of the time warp mechanism is experimentally evaluated when only a limited amount of memory is available to the parallel computation . an implementation of the cancelback protocol is used for memory management on a shared memory architecture , viz . , ksr to evaluate the performance vs . memory tradeoff . the implementation of the cancelback protocol supports canceling back more than one memory object when memory has been exhausted ( the precise number is referred to as the salvage parameter ) and incorporates a non work conserving processor scheduling technique to prevent starvation . several synthetic and benchmark programs are used that provide interesting stress cases for evaluating the limited memory behavior . the experiments are extensively monitored to determine the extent to which various factors may affect performance . several observations are made by analyzing the behavior of time warp under limited memory <digit> ) depending on the available memory and asymmetry in the workload , canceling back several memory objects at one time ( i . e . , a salvage parameter value of more than one ) improves performance significantly , by reducing certain overheads . however , performance is relatively insensitive to the salvage parameter except at extreme values . <digit> ) the speedup vs . memory curve for time warp programs has a well defined knee before which speedup increases very rapidly with memory and beyond which there is little performance gain with increased memory . performance nearly equivalent to that with large amounts of memory can be achieved with only a modest amount of additional memory beyond that required for sequential execution , if memory management overheads are small compared to the event granularity . these results indicate that contrary to the common belief , memory usage by time warp can be controlled within reasonable limits without any significant loss of performance .\n",
      "['time warp', 'memory management', 'rollback', 'parallel and distributed simulation', 'discrete event simulation', 'virtual time', 'performance evaluation', 'checkpointing <eos>']\n",
      "['parallel computation', 'performance']\n",
      "['time warp', 'performance', 'memory management', 'memory management', 'time warp', 'parallel processing', 'memory management', 'memory management', 'performance evaluation']\n",
      "2 9 11\n",
      "additional scores from absent units to present :  ['time warp', 'memory management', 'memory management', 'time warp', 'memory management', 'memory management']\n",
      "\n",
      "\n",
      "crash resilient communication in dynamic networks . <unk> end to end data delivery protocol for dynamic communication networks is presented . the protocol uses bounded sequence numbers and can tolerate both link failures and ( intermediate ) processor crashes . previous bounded end to end protocols could not tolerate crashes . we present a self stabilizing version of the algorithm that can recover from crashes of the sender and the receiver as well as of intermediate processors . starting with the network in an arbitrary state , the self stabilizing version guarantees proper transmission of messages following a finite convergence period .\n",
      "['dynamic networks', 'communication networks', 'end to end protocols', 'self stabilization', 'crash failures <eos>']\n",
      "['Present predictions:']\n",
      "['dynamic networks', 'communication', 'end to end protocols', 'self stabilization', 'crash resilient communication', 'dynamic communication networks', 'crash resilient communication', 'self stabilization', 'fault tolerance', 'end to end delay']\n",
      "1 10 10\n",
      "additional scores from absent units to present :  ['dynamic networks', 'end to end protocols']\n",
      "\n",
      "\n",
      "a graph partitioning approach to sequential diagnosis . <unk> paper describes a generalized sequential diagnosis algorithm whose analysis leads to strong diagnosability results for a variety of multiprocessor interconnection topologies . the overall complexity of this algorithm in terms of total testing and syndrome decoding time is linear in the number of edges in the interconnection graph and the total number of iterations of diagnosis and repair needed by the algorithm is bounded by the diameter of the interconnection graph . the degree of diagnosability of this algorithm for a given interconnection graph is shown to be directly related to a graph parameter which we refer to as the partition number . we approximate this graph parameter for several interconnection topologies and thereby obtain lower bounds on degree of diagnosability achieved by our algorithm on these topologies . if we let n denote total number of vertices in the interconnection graph and denote the maximum degree of any vertex in it , then our results may be summarized as follows . we show that a symmetric d dimensional grid graph is sequentially omega left ( n d over d <digit> right ) diagnosable for any fixed d . for hypercubes , <unk> log n dimensional grid graphs , it is shown that our algorithm leads to a surprising omega left ( n , rm log , log , n over log , n right ) degree of diagnosability . next we show that the degree of diagnosability of an arbitrary interconnection graph by our algorithm is omega left ( sqrt n over delta right ) . this bound translates to an omega left ( sqrt n right ) degree of diagnosability for cube connected cycles and an omega left ( sqrt n over k right ) degree of diagnosability for k ary trees . finally , we augment our algorithm with another algorithm to show that every topology is omega left ( n <digit> over <digit> right ) diagnosable .\n",
      "['graph partitioning', 'sequential diagnosis', 'degree of diagnosability', 'system level diagnosis', 'fault tolerance', 'multiprocessor systems', 'analysis of algorithms <eos>']\n",
      "['interconnection graph', 'sequential diagnosis']\n",
      "['diagnosis', 'diagnosability', 'partition', 'syndrome decoding', 'graph partitioning', 'graph algorithms', 'interconnection network', 'interconnection networks']\n",
      "2 8 10\n",
      "additional scores from absent units to present :  ['graph partitioning']\n",
      "\n",
      "\n",
      "a traffic balanced adaptive wormhole routing scheme for two dimensional meshes . <unk> this paper , we analyze several issues involved in developing low latency adaptive wormhole routing schemes for two dimensional meshes . it is observed that along with adaptivity , balanced distribution of traffic has a significant impact on the system performance . motivated by this observation , we develop a new fully adaptive routing algorithm called positive first negative first for two dimensional meshes . the algorithm uses only two virtual channels per physical channel creating two virtual networks . the messages are routed positive first in one virtual network and negative first in the other . because of this combination , the algorithm distributes the system load uniformly throughout the network and is also fully adaptive . it is shown that the proposed algorithm results in providing better performance in terms of the average network latency and throughput when compared with the previously proposed routing algorithms .\n",
      "['adaptive wormhole routing', 'two dimensional mesh', 'traffic distribution', 'region of adaptivity', 'positive first negative first algorithm <eos>']\n",
      "['wormhole routing', 'two dimensional meshes']\n",
      "['adaptive wormhole routing', 'traffic balanced', 'traffic balanced routing', 'adaptive routing', 'two dimensional meshes', 'performance evaluation']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['adaptive wormhole routing']\n",
      "\n",
      "\n",
      "control and energy consumption in communications for nomadic computing . <unk> consider the problem of communications over a wireless channel in support of data transmissions from the perspective of small portable devices that must rely on limited battery energy . we model the channel outages as statistically correlated errors . classic arq strategies are found to lead to a considerable waste of energy , due to the large number of transmissions . the use of finite energy sources in the face of dependent channel errors leads to new protocol design criteria . as an example , a simple probing scheme , which slows down the transmission rate when the channel is impaired , is shown to be more energy efficient , with a slight loss in throughput . a modified scheme that yields slightly better performance but requires some additional complexity is also studied . some references on the modeling of battery cells are discussed to highlight the fact that battery charge capacity is strongly influenced by the available relaxation time between current pulses . a formal approach that can track complex models for power sources , including dynamic charge recovery , is also developed .\n",
      "['energy consumption', 'error control', 'mobile communications', 'battery modeling', 'mobile computing', 'channel probing', 'wireless systems <eos>']\n",
      "['Present predictions:']\n",
      "['nomadic computing', 'energy consumption', 'nomadic computing', 'energy efficiency', 'wireless communications', 'wireless communications']\n",
      "1 6 6\n",
      "additional scores from absent units to present :  ['energy consumption']\n",
      "\n",
      "\n",
      "improving nfs performance over wireless links . <unk> is a widely used remote file access protocol that has been tuned to perform well on traditional lans which exhibit low error rates . users migrating to mobile hosts would like continued remote file access via nfs . however , low bandwidth and high error rates degrade performance on mobile hosts using wireless links , hindering the use of nfs . we conducted experiments to study the behavior of nfs in a wireless testbed . based on these experiments , we incorporated modifications into the mobile nfs client . this paper presents two mechanisms which improve nfs performance over wireless links an aggressive nfs client and link level retransmissions . our experiments show that these mechanisms improve response time by up to <digit> % , which brings the performance to within <digit> % of that obtained in zero error conditions .\n",
      "['nfs', 'link level retransmission', 'mobile computing', 'file systems', 'wireless lan', 'performance evaluation <eos>']\n",
      "['mobile hosts', 'wireless links']\n",
      "['nfs', 'remote file access', 'mobile computing', 'wireless networks', 'performance evaluation']\n",
      "2 5 7\n",
      "additional scores from absent units to present :  ['nfs']\n",
      "\n",
      "\n",
      "an efficient protocol for call setup and path migration in ieee <digit> . <digit> based personal communication networks . <unk> , dqdb ( ieee <digit> . <digit> ) man has been proposed as a component of personal communication networks , in which base stations of wireless infrastructures are connected by a number of <unk> which in turn are connected via bridges . we propose a protocol for call setup and path migration in a cluster of <unk> . the protocol uses a link state like routing method for path selection and a source routing based scheme for path establishment . in addition , we propose a labeling scheme that makes it possible to carry the path information needed by the source routing protocol in a single <digit> octet dqdb slot . without such a labeling scheme , source routing would be inefficient for our purpose .\n",
      "['call setup', 'path migration', 'dqdb', 'routing', 'isochronous channels <eos>']\n",
      "['Present predictions:']\n",
      "['ieee <digit>', 'dqdb', 'path migration', 'personal communication networks', 'path migration', 'call setup', 'wireless networks']\n",
      "1 7 7\n",
      "additional scores from absent units to present :  ['dqdb', 'path migration', 'path migration', 'call setup']\n",
      "\n",
      "\n",
      "a predictor corrector algorithm for a class of nonlinear saddle point problems . an interior path following algorithm is proposed for solving the nonlinear saddle point problem rm minimax c tx ph ( x ) b ty psi ( y ) y tax <unk> <unk> rm subject to ( x , y ) in x ti y su r n ti r m , <unk> where ph ( x ) and ps ( y ) are smooth convex functions and x and y are boxes ( hyperrectangles ) . this problem is closely related to the models in stochastic programming and optimal control studied by rockafellar and wets ( math . programming studies , <digit> ( <digit> ) , pp . <digit> <digit> siam j . control optim . , <digit> ( <digit> ) , pp . <digit> <digit> ) . existence and error bound results on a central path are derived . starting from an initial solution near the central path with duality gap o ( mu ) , the algorithm finds an ep optimal solution of the problem in o ( sqrt m n , log mu ep ) iterations if both ph ( x ) and ps ( y ) satisfy a scaled lipschitz condition .\n",
      "['saddle point problem', 'stochastic programming', 'optimal control', 'nonlinear complementarity problem', 'interior point methods <eos>']\n",
      "['stochastic programming']\n",
      "['nonlinear saddle point problem', 'saddle point problem', 'predictor corrector algorithm', '<unk>', 'nonlinear programming', 'minimax problem', '<unk>']\n",
      "1 7 8\n",
      "additional scores from absent units to present :  ['saddle point problem']\n",
      "\n",
      "\n",
      "recent developments in high level synthesis . we survey recent developments in high level synthesis technology for vlsi design . the need for higher level design automation tools are discussed first . we then describe some basic techniques for various subtasks of high level synthesis . techniques that have been proposed in the past few years ( since <digit> ) for various subtasks of high level synthesis are surveyed . we also survey some new synthesis objectives including testability , power efficiency , and reliability .\n",
      "['high level synthesis', 'vlsi design', 'design automation', 'design methodology <eos>']\n",
      "['survey']\n",
      "['design automation', 'high level synthesis', 'design automation']\n",
      "1 3 4\n",
      "additional scores from absent units to present :  ['design automation', 'high level synthesis', 'design automation']\n",
      "\n",
      "\n",
      "complete orthogonal decomposition for weighted least squares . this paper proposes a complete orthogonal decomposition ( cod ) algorithm for solving weighted least squares problems . in applications , the weight matrix can be highly ill conditioned , and this can cause standard methods like qr factorization to return inaccurate answers in floating point arithmetic . stewart and todd independently established a norm bound for the weighted least squares problem that is independent of the weight matrix . vavasis proposed a definition of a stable solution of weighted least squares based on this norm bound the solution computed by a stable algorithm must satisfy an accuracy bound that is not affected by ill conditioning in the weight matrix . a forward error analysis shows that the cod algorithm is stable in this sense , but it is simpler and more efficient than the algorithm proposed by vavasis . our forward error bound is contrasted to the backward error analysis of other previous works on weighted least squares .\n",
      "['weighted least squares', 'qr factorization', 'forward error analysis', 'equilibrium systems', 'numerical stability', 'interior point methods <eos>']\n",
      "['qr factorization', 'orthogonal decomposition']\n",
      "['weighted least squares', 'forward error analysis', 'weight matrix', 'least squares', 'complete orthogonal decomposition', 'weighted least squares algorithm']\n",
      "2 6 8\n",
      "additional scores from absent units to present :  ['weighted least squares', 'forward error analysis']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "greedy = '/home/thomased/work/codebase/coopsummer2023/KPDrop/experiments/trans_set_b128_n4_ep5/final_results_greedy(split_ab_pre)/kp20k_TransformerSeq2Set_seq2set_testkrapivin_predictions.txt'\n",
    "topbeam = '/home/thomased/work/codebase/coopsummer2023/KPDrop/experiments/trans_set_b128_n4_ep5/final_result_top5beam(split_ab_pre)/kp20k_TransformerSeq2Set_seq2set_testkrapivin_predictions.txt'\n",
    "with open(greedy,'r') as f:\n",
    "    lines = f.read()\n",
    "    lines = lines.split(\"\\n\\n\")[:-1]\n",
    "    for ind, line in enumerate(lines):\n",
    "        # print('ind: ', ind)\n",
    "        line = line.split('\\n')\n",
    "        text = line[0].strip()\n",
    "        target = line[1].strip().replace('Target: ','').split(' ; ')\n",
    "        preds = line[2].strip().replace('Predictions: ','').split(' ; ')\n",
    "        present_preds = line[3].strip().replace('Present predictions: ','').split(' ; ')\n",
    "        absent_preds = line[4].strip().replace('Absent predictions: ', '').split(' ; ')\n",
    "        \n",
    "        ab_from_pres = [x for x in preds if x not in text and x in present_preds and x in target]\n",
    "        pres_from_ab = [x for x in preds if x in text and x in absent_preds and x in target]\n",
    "        # if ab_from_pres:\n",
    "        #     print(text)\n",
    "        #     print(target)\n",
    "        #     print(present_preds)\n",
    "        #     print(absent_preds)\n",
    "        #     print(len(present_preds), len(absent_preds), len(preds))\n",
    "        #     print('additional scores from present units to absent : ',ab_from_pres)\n",
    "        #     print('\\n')\n",
    "        \n",
    "        if pres_from_ab:\n",
    "            print(text)\n",
    "            print(target)\n",
    "            print(present_preds)\n",
    "            print(absent_preds)\n",
    "            print(len(present_preds), len(absent_preds), len(preds))\n",
    "            print('additional scores from absent units to present : ',pres_from_ab)\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b5960a1-7d14-4c4f-a00c-4199e65a7f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "                                                                                                                                                                              \n",
    "# Source: information risk of inadvertent disclosure an analysis of file sharing risk in the financial supply chain . firms face many different types of information security risk . inadvertent disclosure of sensitive business information represents one of the largest classes of recent security breaches . we examine a specific instance of this problem inadvertent disclosures through peer to peer file sharing networks . we characterize the extent of the security risk for a group of large financial institutions using a direct analysis of leaked documents . we also characterize the threat of loss by examining search patterns in peer to peer networks . our analysis demonstrates both a substantial threat and vulnerability for large financial firms . we find a statistically significant link between leakage and leak sources including the firm employment base and the number of retail accounts . we also find a link between firm visibility and threat activity . finally , we find that firms with more leaks also experience increased threat .\n",
    "target = 'inadvertent disclosure ; file sharing ; information security ; peer to peer networks ; data breaches ; intellectual property leaks ; risk management <eos>'\n",
    "prediction = 'peer to peer networks ; security risk ; financial supply chain ; disclosure ; search patterns ; file sharing ; information security risk ; financial risk ; information security risk ; peer to peer networks ; risk management ; peer to peer ; risk ; file sharing risk ; inadvertent disclosure ; information security ; information security ; data security ; supply chain management ; information security risk ; electronic commerce ; privacy ; inadvertent disclosure ; security breaches ; financial supply chains ; disclosure risk ; vulnerability ; supply chain ; information risk ; financial disclosure ; information security policy ; peer to peer systems ; risk analysis ; risk management ; trust ; risk ; security ; search pattern ; economics ; financial services ; information security management ; peer to peer ; information security ; security policy ; internet ; peer to peer network ; file sharing ; security ; threat ; peer to peer computing ; risk assessment ; electronic markets ; privacy protection ; risk ; security policy ; financial institutions ; vulnerability ; security risk ; direct analysis ; file sharing networks ; business information ; financial risk management ; information security risk assessment ; information security policy ; disclosure policy ; leakage risk ; security policy ; supply chain ; retail accounts ; financial firms ; threat ; security breaches ; experience threat ; information security risk ; business information security ; information security breaches ; decision making ; <unk> ; financial risk analysis ; information security economics ; data envelopment analysis ; disclosure risk ; trust management ; file sharing risk ; financial disclosure ; disclosure of risk ; risk inadvertent disclosure ; employment disclosure ; public policy ; confidentiality ; financial information systems ; information privacy ; data security risk ; peer to peer security risk ; information security management ; risk analysis ; security management ; peer to peer networking ; retail risk ; security policies ; disclosure of sources ; vulnerability activity ; search ; file sharing policy ; supply chains ; file sharing ; decision support ; quality of service ; financial risk assessment ; risk management ; data disclosure ; peer to peer security ; file sharing risk ; risk assessment ; privacy preservation ; risk inadvertent disclosure ; inadvertent disclosure ; financial risk ; disclosure policy ; peer to peer networks ; information security ; business information security risk ; information sharing ; quality assurance ; information security risk management ; data mining ; supply chain security ; disclosure control ; search engines ; security risk ; peer to peer file sharing ; retail ; risk ; disclosure of sensitive business ; threat activity ; search activity ; file sharing threat ; employment base ; information economics ; public key infrastructure ; quality of services ; financial file systems ; data privacy ; social networks ; security risk ; security analysis ; business information ; disclosure of threat ; risk of financial supply chain ; peer to peer ; inadvertent disclosure risk ; file sharing risk ; quality of service risk ; financial information security ; data security policy ; security policy ; retail policy ; search patterns ; internet security ; search patterns ; information security risk ; security policy ; inadvertent disclosure ; public key cryptography ; financial file sharing ; information security risk analysis ; data security analysis ; financial risk ; search engine ; privacy policy ; web services ; inadvertent disclosure risk ; peer to peer networks ; disclosure threat ; threat loss ; search behavior ; file security risk ; employment base firm ; information security policy ; key management ; quality of risk ; information disclosure ; disclosure analysis ; security management ; security policies ; supply chains ; information security ; disclosure of inadvertent disclosure ; risk of financial institutions ; file security ; employment ; information security inadvertent disclosure ; it governance ; quality control ; financial file system ; risk analysis ; security policy ; social network analysis ; information security risk assessment ; retail risk ; search log analysis ; internet of things ; retail threat ; security breaches risk ; financial disclosure risk ; risk inadvertent disclosure ; vulnerability risk ; peer to peer network ; data security ; supply chain networks ; information sharing risk ; economics of information ; financial disclosure risk ; information risk ; design science research ; supply chain security risk ; risk mitigation ; financial services ; security economics ; privacy preserving data mining ; web file sharing ; retail leak ; search ; peer to peer ; disclosure of sensitive business information ; threat vulnerability ; file systems ; information assurance ; decision support systems ; business disclosure ; design science ; business information security ; information security economics ; risk mitigation ; security risk assessment ; security risk assessment ; social networks ; file sharing policy ; security risk inadvertent disclosure ; business information security ; threat and vulnerability ; experience leaks ; data security risk ; employment supply chain ; security risk ; decision analysis ; economics of security risk ; financial information services ; information assurance ; business value ; file sharing policy ; risk evaluation ; search engine risk ; internet security risk ; social chain ; inadvertent ; search pattern ; vulnerability activity ; file networks ; employment threat ; information assurance risk ; public administration ; quality of information ; business information security ; information risk management ; data security risk analysis ; peer to peer security policy ; security management ; financial disclosure ; security risk analysis ; security risk management ; web networks ; file sharing threat ; business information security risk ; risk of financial supply chain ; risk of inadvertent disclosure ; information risk ; data risk ; public key encryption ; business security ; information assurance risk ; social media ; information security risk analysis ; financial risk management ; search based security ; internet policy ; peer networks ; business supply chain ; risk of inadvertent disclosure ; security threat ; experience experience ; data risk ; business disclosure ; key distribution ; quality management ; financial information sharing ; information risk analysis ; business intelligence ; information security risk management ; retail security ; search engine disclosure ; security risk analysis ; peer to peer file ; security risk of financial institutions ; financial supply ; disclosure of sources threat ; experience risk ; supply chain management ; information risk of security risk ; decision trees ; economics of information security ; risk assessment ; design ; supply chains ; risk evaluation ; financial risk analysis ; electronic business ; internet disclosure ; file sharing networks ; business disclosure ; risk of information ; risk of financial supply chains ; experience leak ; file allocation ; business file sharing ; data security ; public goods ; quality of security risk ; financial information ; information risk assessment ; data disclosure risk ; supply chain disclosure ; security analysis ; retail file sharing ; search log ; trust region ; web to peer networks ; risk of financial institutions ; risk inadvertent disclosure ; financial supply chain networks ; risk of security risk ; risk of information ; experience threat activity ; business ; file security risk ; it security ; economics of information systems ; financial information security risk ; information disclosure risk ; social capital ; information disclosure ; financial information systems ; search engine security ; web disclosure ; risk of inadvertent disclosure ; inadvertent disclosure risk ; peer to peer network ; risk of financial institutions ; risk of security risk ; experience experience threat ; information security breaches ; inadvertent link ; information security inadvertent ; key agreement ; quality assessment ; information security risk ; security policy ; security risk ; business information management ; file disclosure ; retail information security ; security risk management ; privacy preserving disclosure ; supply chain networks ; risk vulnerability ; security risk of information ; disclosure of sensitive business disclosure ; vulnerability and vulnerability ; peer to peer networking ; inadvertent ; data security risk ; it risk ; business value ; information security risk risk ; design research ; business information systems ; information privacy ; retail security risk ; leakage probability ; security policy analysis ; web to peer ; inadvertent link ; security risk of inadvertent disclosure ; threat activity ; direct analysis of inadvertent disclosure ; file allocation networks ; supply chain risk ; public key management ; <unk> scheme ; financial risk risk ; security management ; data security and protection ; social networking ; file sharing ; financial risk assessment ; leakage risk analysis ; trust and reputation ; supply chain management ; file networks ; security risk of risk ; business file sharing ; vulnerability risk ; risk of financial firms ; direct leaked ; information sharing ; business supply chain ; file security ; decision support system ; quality of security ; financial file disclosure ; information assurance policy ; design of algorithms ; supply chain analysis ; security risk ; financial information security ; electronic risk ; privacy preserving ; social network ; inadvertent leakage ; security risk for risk ; financial supply chain firms ; vulnerability loss ; direct analysis of documents ; data sharing ; business information security breaches ; it outsourcing ; quality of service disclosure ; financial file networks ; security analysis ; supply chain security management ; information assurance ; disclosure of risk ; security and privacy protection ; security policy risk ; web file sharing risk ; file security risk ; business supply chains ; risk for financial supply chain ; direct method ; business information sharing ; information risk inadvertent disclosure ; key exchange ; business information systems ; file sharing risk ; data security risk assessment ; supply chain security risk management ; information assurance risk ; disclosure control risk ; security policy analysis ; trust in peer networks ; file security ; security risk in peer networks ; financial supply chain management ; disclosure of sources risk ; risk inadvertent disclosure risk ; direct methods ; file security breaches ; supply chain firms ; information security risk inadvertent disclosure ; public key services ; quality of service analysis ; sensitive disclosure ; information privacy risk ; data security risk management ; peer to peer security breaches ; security economics ; retail file sharing risk ; search log disclosure ; privacy preserving data security ; supply chain risk ; inadvertent disclosure threat ; financial supply chain risk ; disclosure of sensitive disclosure ; risk in peer networks ; direct analysis of security risk ; inadvertent disclosure risk ; business information risk ; information risk of inadvertent disclosure ; decision tree ; economics of it ; business information management ; security risk ; supply chain risk management ; file disclosure risk ; retail file systems ; leakage risk management ; privacy preserving data disclosure ; peer to firm ; risk of financial supply chain ; information security breaches ; risk of large financial institutions ; direct ; information sharing risk ; inadvertent disclosure and threat ; it investment ; financial file sharing risk ; information privacy protection ; design patterns ; peer to peer security disclosure ; file leakage ; disclosure risk analysis ; leakage regulation ; privacy preserving file sharing ; peer review ; risk of security risk ; security policy risk ; financial supply networks ; risk of financial supply chains ; risk of large firms ; inadvertent disclosure threat ; information risk of security ; quality of service security ; sensitive risk ; information risk risk ; security analysis ; information risk ; financial file sharing ; leakage risk assessment ; privacy protection risk ; web file ; inadvertent disclosure and threat ; search risk ; business information sharing ; threat and vulnerability ; file sharing threat activity ; business file sharing risk ; it security risk ; financial services discovery ; file disclosure ; data privacy risk ; social networking sites ; information disclosure risk ; retail information security risk ; leakage threat ; trust in networks ; social to peer networks ; risk inadvertent disclosure risk ; security risk for financial institutions ; business ; disclosure of threat activity ; risk of information security risk ; data disclosure ; economics of security ; file sharing policy ; data security and privacy ; business file sharing ; information risk management ; retail information systems ; electronic peer to peer networks ; security risk risk ; inadvertent leak ; information risk ; vulnerability activity activity ; direct analysis of financial institutions ; data sharing risk ; inadvertent leak ; key confirmation ; economics of information security risk ; information security ; information security management systems ; security management ; peer review ; file sharing disclosure ; retail risk management ; search based security risk ; social institutions ; risk of information ; business file sharing risk ; disclosure of inadvertent disclosure risk ; risk inadvertent ; file sharing risk inadvertent disclosure ; inadvertent leakage ; public private key infrastructure ; quality assurance risk ; information security policy ; information assurance security risk ; security economics ; supply chain risk ; security risk assessment ; disclosure risk management ; security policy risk ; trust in security ; peer to peer file networks ; security risk of financial firms ; business information security breaches ; risk in peer to peer ; peer to peer file sharing ; employment base station ; financial services disclosure ; information disclosure policy ; supply chain security policy ; file security risk ; disclosure risk assessment ; search engine security risk ; privacy preserving data envelopment analysis ; inadvertent disclosure activity ; business information sharing risk ; risk for security risk ; search patterns in financial institutions ; business information sharing risk ; key management risk ; quality of service security risk ; financial information sharing risk ; information security policy risk ; data security and privacy protection ; business to peer ; information security risk risk ; disclosure control policy ; electronic peer to peer ; privacy preserving data protection'\n",
    "present_prediction = 'peer to peer networks ; security risk ; peer to peer ; inadvertent disclosure ; security breaches ; peer to peer network ; file sharing ; security ; risk ; security policy ; supply chain ; retail accounts ; file sharing risk ; peer to peer networking ; retail risk ; security policies ; risk inadvertent disclosure ; inadvertent disclosure ; peer to peer file sharing ; retail ; risk ; search patterns ; web services ; inadvertent disclosure risk ; supply chains ; retail threat ; security breaches risk ; web file sharing ; retail leak ; search ; social networks ; file sharing policy ; security risk inadvertent disclosure ; social chain ; inadvertent ; search pattern ; web networks ; file sharing threat ; peer networks ; peer to peer file ; security risk of financial institutions ; file sharing networks ; web to peer networks ; risk of financial institutions ; risk inadvertent disclosure ; web disclosure ; risk of inadvertent disclosure ; inadvertent disclosure risk ; supply chain networks ; risk vulnerability ; security risk of information ; web to peer ; inadvertent link ; security risk of inadvertent disclosure ; supply chain management ; file networks ; security risk of risk ; social network ; inadvertent leakage ; security risk for risk ; web file sharing risk ; file security risk ; file security ; security risk in peer networks ; supply chain risk ; inadvertent disclosure threat ; peer to firm ; risk of financial supply chain ; peer review ; risk of security risk ; security policy risk ; web file ; inadvertent disclosure and threat ; search risk ; social to peer networks ; risk inadvertent disclosure risk ; security risk for financial institutions ; inadvertent leak ; social institutions ; risk of information ; peer to peer file networks ; security risk of financial firms ; inadvertent disclosure activity'\n",
    "absent_prediction = 'financial supply chain ; disclosure ; search patterns ; file sharing ; information security risk ; financial risk ; information security risk ; peer to peer networks ; risk management ; risk ; file sharing risk ; inadvertent disclosure ; information security ; information security ; data security ; supply chain management ; information security risk ; electronic commerce ; privacy ; financial supply chains ; disclosure risk ; vulnerability ; supply chain ; information risk ; financial disclosure ; information security policy ; peer to peer systems ; risk analysis ; risk management ; trust ; risk ; security ; search pattern ; economics ; financial services ; information security management ; peer to peer ; information security ; security policy ; internet ; threat ; peer to peer computing ; risk assessment ; electronic markets ; privacy protection ; financial institutions ; vulnerability ; security risk ; direct analysis ; file sharing networks ; business information ; financial risk management ; information security risk assessment ; information security policy ; disclosure policy ; leakage risk ; security policy ; financial firms ; threat ; security breaches ; experience threat ; information security risk ; business information security ; information security breaches ; decision making ; <unk> ; financial risk analysis ; information security economics ; data envelopment analysis ; disclosure risk ; trust management ; financial disclosure ; disclosure of risk ; risk inadvertent disclosure ; employment disclosure ; public policy ; confidentiality ; financial information systems ; information privacy ; data security risk ; peer to peer security risk ; information security management ; risk analysis ; security management ; disclosure of sources ; vulnerability activity ; search ; file sharing policy ; supply chains ; file sharing ; decision support ; quality of service ; financial risk assessment ; risk management ; data disclosure ; peer to peer security ; file sharing risk ; risk assessment ; privacy preservation ; financial risk ; disclosure policy ; peer to peer networks ; information security ; business information security risk ; information sharing ; quality assurance ; information security risk management ; data mining ; supply chain security ; disclosure control ; search engines ; security risk ; disclosure of sensitive business ; threat activity ; search activity ; file sharing threat ; employment base ; information economics ; public key infrastructure ; quality of services ; financial file systems ; data privacy ; social networks ; security risk ; security analysis ; business information ; disclosure of threat ; risk of financial supply chain ; peer to peer ; inadvertent disclosure risk ; file sharing risk ; quality of service risk ; financial information security ; data security policy ; security policy ; retail policy ; search patterns ; internet security ; information security risk ; security policy ; inadvertent disclosure ; public key cryptography ; financial file sharing ; information security risk analysis ; data security analysis ; financial risk ; search engine ; privacy policy ; peer to peer networks ; disclosure threat ; threat loss ; search behavior ; file security risk ; employment base firm ; information security policy ; key management ; quality of risk ; information disclosure ; disclosure analysis ; security management ; security policies ; information security ; disclosure of inadvertent disclosure ; risk of financial institutions ; file security ; employment ; information security inadvertent disclosure ; it governance ; quality control ; financial file system ; risk analysis ; security policy ; social network analysis ; information security risk assessment ; retail risk ; search log analysis ; internet of things ; financial disclosure risk ; risk inadvertent disclosure ; vulnerability risk ; peer to peer network ; data security ; supply chain networks ; information sharing risk ; economics of information ; financial disclosure risk ; information risk ; design science research ; supply chain security risk ; risk mitigation ; financial services ; security economics ; privacy preserving data mining ; peer to peer ; disclosure of sensitive business information ; threat vulnerability ; file systems ; information assurance ; decision support systems ; business disclosure ; design science ; business information security ; information security economics ; risk mitigation ; security risk assessment ; security risk assessment ; business information security ; threat and vulnerability ; experience leaks ; data security risk ; employment supply chain ; security risk ; decision analysis ; economics of security risk ; financial information services ; information assurance ; business value ; file sharing policy ; risk evaluation ; search engine risk ; internet security risk ; vulnerability activity ; file networks ; employment threat ; information assurance risk ; public administration ; quality of information ; business information security ; information risk management ; data security risk analysis ; peer to peer security policy ; security management ; financial disclosure ; security risk analysis ; security risk management ; business information security risk ; risk of financial supply chain ; risk of inadvertent disclosure ; information risk ; data risk ; public key encryption ; business security ; information assurance risk ; social media ; information security risk analysis ; financial risk management ; search based security ; internet policy ; business supply chain ; risk of inadvertent disclosure ; security threat ; experience experience ; data risk ; business disclosure ; key distribution ; quality management ; financial information sharing ; information risk analysis ; business intelligence ; information security risk management ; retail security ; search engine disclosure ; security risk analysis ; financial supply ; disclosure of sources threat ; experience risk ; supply chain management ; information risk of security risk ; decision trees ; economics of information security ; risk assessment ; design ; supply chains ; risk evaluation ; financial risk analysis ; electronic business ; internet disclosure ; business disclosure ; risk of information ; risk of financial supply chains ; experience leak ; file allocation ; business file sharing ; data security ; public goods ; quality of security risk ; financial information ; information risk assessment ; data disclosure risk ; supply chain disclosure ; security analysis ; retail file sharing ; search log ; trust region ; financial supply chain networks ; risk of security risk ; risk of information ; experience threat activity ; business ; file security risk ; it security ; economics of information systems ; financial information security risk ; information disclosure risk ; social capital ; information disclosure ; financial information systems ; search engine security ; peer to peer network ; risk of financial institutions ; risk of security risk ; experience experience threat ; information security breaches ; inadvertent link ; information security inadvertent ; key agreement ; quality assessment ; information security risk ; security policy ; security risk ; business information management ; file disclosure ; retail information security ; security risk management ; privacy preserving disclosure ; disclosure of sensitive business disclosure ; vulnerability and vulnerability ; peer to peer networking ; inadvertent ; data security risk ; it risk ; business value ; information security risk risk ; design research ; business information systems ; information privacy ; retail security risk ; leakage probability ; security policy analysis ; threat activity ; direct analysis of inadvertent disclosure ; file allocation networks ; supply chain risk ; public key management ; <unk> scheme ; financial risk risk ; security management ; data security and protection ; social networking ; file sharing ; financial risk assessment ; leakage risk analysis ; trust and reputation ; business file sharing ; vulnerability risk ; risk of financial firms ; direct leaked ; information sharing ; business supply chain ; file security ; decision support system ; quality of security ; financial file disclosure ; information assurance policy ; design of algorithms ; supply chain analysis ; security risk ; financial information security ; electronic risk ; privacy preserving ; financial supply chain firms ; vulnerability loss ; direct analysis of documents ; data sharing ; business information security breaches ; it outsourcing ; quality of service disclosure ; financial file networks ; security analysis ; supply chain security management ; information assurance ; disclosure of risk ; security and privacy protection ; security policy risk ; business supply chains ; risk for financial supply chain ; direct method ; business information sharing ; information risk inadvertent disclosure ; key exchange ; business information systems ; file sharing risk ; data security risk assessment ; supply chain security risk management ; information assurance risk ; disclosure control risk ; security policy analysis ; trust in peer networks ; financial supply chain management ; disclosure of sources risk ; risk inadvertent disclosure risk ; direct methods ; file security breaches ; supply chain firms ; information security risk inadvertent disclosure ; public key services ; quality of service analysis ; sensitive disclosure ; information privacy risk ; data security risk management ; peer to peer security breaches ; security economics ; retail file sharing risk ; search log disclosure ; privacy preserving data security ; financial supply chain risk ; disclosure of sensitive disclosure ; risk in peer networks ; direct analysis of security risk ; inadvertent disclosure risk ; business information risk ; information risk of inadvertent disclosure ; decision tree ; economics of it ; business information management ; security risk ; supply chain risk management ; file disclosure risk ; retail file systems ; leakage risk management ; privacy preserving data disclosure ; information security breaches ; risk of large financial institutions ; direct ; information sharing risk ; inadvertent disclosure and threat ; it investment ; financial file sharing risk ; information privacy protection ; design patterns ; peer to peer security disclosure ; file leakage ; disclosure risk analysis ; leakage regulation ; privacy preserving file sharing ; financial supply networks ; risk of financial supply chains ; risk of large firms ; inadvertent disclosure threat ; information risk of security ; quality of service security ; sensitive risk ; information risk risk ; security analysis ; information risk ; financial file sharing ; leakage risk assessment ; privacy protection risk ; business information sharing ; threat and vulnerability ; file sharing threat activity ; business file sharing risk ; it security risk ; financial services discovery ; file disclosure ; data privacy risk ; social networking sites ; information disclosure risk ; retail information security risk ; leakage threat ; trust in networks ; business ; disclosure of threat activity ; risk of information security risk ; data disclosure ; economics of security ; file sharing policy ; data security and privacy ; business file sharing ; information risk management ; retail information systems ; electronic peer to peer networks ; security risk risk ; information risk ; vulnerability activity activity ; direct analysis of financial institutions ; data sharing risk ; inadvertent leak ; key confirmation ; economics of information security risk ; information security ; information security management systems ; security management ; peer review ; file sharing disclosure ; retail risk management ; search based security risk ; business file sharing risk ; disclosure of inadvertent disclosure risk ; risk inadvertent ; file sharing risk inadvertent disclosure ; inadvertent leakage ; public private key infrastructure ; quality assurance risk ; information security policy ; information assurance security risk ; security economics ; supply chain risk ; security risk assessment ; disclosure risk management ; security policy risk ; trust in security ; business information security breaches ; risk in peer to peer ; peer to peer file sharing ; employment base station ; financial services disclosure ; information disclosure policy ; supply chain security policy ; file security risk ; disclosure risk assessment ; search engine security risk ; privacy preserving data envelopment analysis ; business information sharing risk ; risk for security risk ; search patterns in financial institutions ; business information sharing risk ; key management risk ; quality of service security risk ; financial information sharing risk ; information security policy risk ; data security and privacy protection ; business to peer ; information security risk risk ; disclosure control policy ; electronic peer to peer ; privacy preserving data protection'\n",
    "\n",
    "prediction = prediction.split(' ; ')\n",
    "present_prediction = present_prediction.split(' ; ')\n",
    "absent_prediction = absent_prediction.split(' ; ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8214f904-6a4d-4cfb-8d22-016c3a85fa4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620\n",
      "83\n",
      "537\n"
     ]
    }
   ],
   "source": [
    "print(len(prediction))\n",
    "print(len(present_prediction))\n",
    "print(len(absent_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a741a529-e49b-4bc0-8eac-45614832a422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 83+537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadd486-1f27-40ca-b30b-f010c2f7a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_beam = '['access control ; security ; privacy ; encrypted database ; obfuscated ; obfuscation ; group ; databases ; obfuscated record ; privacy ; integrity control ; privacy policy ; security policy ; database ; security control ; encryption e ; database security ; security software ; group privacy ; privacy policy ; access controls ; obfuscated database ; security data security ; encrypted databases ; privacy mechanisms ; security data ; privacy property ; security data integrity ; obfuscated databases ; privacy mechanisms ; encryption e encryption ; privacy policies ; security data obfuscation ; database obfuscation ; privacy property ; encrypted database security ; privacy policy property ; integrity ; obfuscated encryption ; privacy policies ; encrypted queries ; group privacy property ; encryption standard ; obfuscation policy ; obfuscated database security ; obfuscation database ; encryption ; security policy property ; integrity controls ; group database ; obfuscation property ; database integrity ; integrity protection ; encrypted database obfuscation ; group privacy policy ; encrypted signatures ; group mass harvesting ; encryption e control ; group call ; security control software ; integrity data ; privacy policy property ; access ; privacy policy call ; security data protection ; encrypted database software ; databases and group privacy ; obfuscated database obfuscation ; obfuscation and protection ; security software control ; privacy policy policy ; security control policy ; access control policy ; group privacy mechanisms ; security data integrity control ; access control software ; group database security ; encrypted database records ; privacy property protection ; databases and data security ; encryption e data ; privacy policy integrity ; security software administration ; encrypted data ; database and group privacy ; databases and group privacy property ; obfuscated encryption e ; databases and database security ; group mass ; databases and security', 'peer to peer ; peer to peer data integration ; formal framework ; agreement ; peer to peer databases ; computational complexity ; preferences ; peer to peer systems ; data integration ; p2p ; reasoning ; peer to peer agreements ; database management systems ; p2p systems ; peer data integration ; agreement problem ; formal methods ; agreement problems ; database management system ; p2p system ; reasoning about peer to peer ; database management ; reasoning about peer preferences ; peer agreements ; data management ; p2p data integration ; peer to peer data ; data integration systems ; reasoning about preferences ; peer autonomy ; formal reasoning ; agreement systems ; consistent query answering ; peer preferences ; formal complexity ; agreement scheme ; peer data integration systems ; preferences criteria ; data management systems ; reasoning about peer data integration ; data agreements ; peer data ; database repairs ; reasoning about peer ; formal semantics ; reasoning about data integration ; database ; reasoning about peer agreements ; peer to peer data agreements ; data complexity ; peer to peer databases systems ; data analysis ; database systems ; preferences analysis ; peer to peer data warehouses ; data management system ; preferences management ; peer data integration system ; peer to peer data management ; computational complexity theory ; preferences nonnumerical ; database integration ; reasoning about terms ; p2p data integration systems ; peer data agreements ; data integration system ; reasoning theory ; data integration problems ; peer data management ; consistent query answering systems ; data integration algorithms ; reasoning about data integration systems ; peer to peer agreements systems ; reasoning about peer data ; data integration ; p2p systems theory ; peer autonomy and preferences ; database management algorithms', 'online learning ; statistical quantitative rules ; statistical rules ; quantitative rules ; statistical quantitative rule ; quantitative rule ; learning ; sq ; decision making ; rules ; online data mining ; quantitative quantitative rules ; statistical rule ; quantitative ; statistical approach ; quantitative rule building ; online data management ; learning significant rules ; statistical quantitative quantitative rules ; quantitative data mining ; online database ; statistical sq rules ; quantitative data ; market share rules ; database management ; sq rules ; statistical sq ; quantitative rule discovery ; parallel learning ; statistical quantitative probability ; parallel rules ; statistical quantitative ; quantitative quantitative rule ; online algorithms ; statistical rule building ; learning number ; parallel data mining ; online learning ; parallel ; online learning procedure ; sq rules ; statistical quantitative quantitative rule ; quantitative data management ; market share rule ; statistical rule management ; learning to significant rules ; statistical rule discovery ; learning procedure ; market share statistic ; statistical quantitative rule building ; quantitative rule management ; parallel algorithms ; statistical sq rule ; quantitative rule base ; online database management ; online data mining ; learning significant rule ; learning significant statistic ; statistical rule base ; online data ; database management applications ; sq rule ; parallel data management ; online data management algorithms ; database management algorithms ; quantitative rules management ; statistical sq quantitative rules ; rules management ; online data management ; sq learning ; online rules ; quantitative quantitative ; online and consumer data management ; sq rule ; sq data management ; statistical approach rules ; quantitative rules discovery ; market share ; database management systems ; quantitative quantitative index ; statistical quantitative rule discovery ; quantitative quantitative quantitative rules ; decision making procedure ; statistical quantitative rule management ; learning to significant rule ; online and consumer data mining ; statistical rules management', 'sensor networks ; wireless sensor networks ; lifetime ; mixed communication mode ; energy dissipation ; mixed communication modes ; sensor network ; heterogeneous ; wireless sensor network ; energy ; mixed communication range ; communication range ; heterogeneous cluster ; mixed communication ; communication modes ; lifetime optimization ; wsn ; communication mode ; lifetime optimizations ; mixed sensor networks ; heterogeneous cluster heads ; mixed communication layer ; heterogeneous cluster head ; wireless sensor networks ; sensor nodes ; energy efficiency ; mixed mode ; transmission range ; heterogeneous cluster based wsn ; wireless sensors networks ; sensor ; heterogeneous cluster based ; mixed networks ; communication ; heterogeneous clusters ; communication layer ; energy conservation ; sensor networking ; heterogeneous networks ; mixed sensor network ; wireless sensor network ; wireless sensors ; energy balance ; wireless sensor networking ; lifetime control ; wireless sensor ; multi hop ; transmission lines ; sensor network lifetime ; lifetime estimation ; mixed communications modes ; sensor networks lifetime ; heterogeneous node ; mixed communications mode ; sensor networks ; transmission modes ; wsn lifetime ; wireless sensor nodes ; transmission line ; mixed sensor ; transmission mode ; heterogeneous cluster based cluster ; wireless ; energy dissipation energy ; wireless sensors network ; multi mode ; wireless sensors networks ; heterogeneous cluster based cluster heads ; mixed communications ; wireless sensors ; wireless networks ; wireless sensor ; heterogeneous cluster center ; multi hop wireless sensor networks ; wireless ; wireless sensor networking ; heterogeneous cluster based cluster head ; wireless communication ; sensor networks with mixed mode ; wireless communication modes ; wireless sensor nodes ; heterogeneous cluster based wireless sensor ; multi sensor networks ; mixed communications range', 'software distributed systems ; hypertext architecture ; distributed systems ; software distributed system ; hypertext architectures ; distributed system ; hypertext ; multiple architectures ; hypertext hypermedia ; hypermedia ; software architectures ; hypertext crawler ; multiple architecture ; software ; distributed general terms management ; software architecture ; distributed general terms ; parallel crawler ; hypermedia architecture ; general terms management ; crawling ; distributed ; software pipelining ; general ; parallel crawler ; distributed general ; multiple architectures ; hypermedia architectures ; distributed crawler ; software distributed ; hypertext hypermedia architectures ; multiple crawler ; hypermedia architecture ; distributed systems terms ; hypermedia ; hypermedia architecture ; multiple architecture ; hypertext hypermedia architecture ; multiple hypermedia ; parallel crawling ; parallel crawling ; hypermedia architectures ; software distributed general terms management ; parallel ; general terms ; hypermedia architectures ; hypertext hypermedia system ; multiple systems ; software distributed general terms ; parallel crawlers ; multiple architecture architectures ; multiple crawler ; hypertext architecture design ; distributed systems systems ; software distributed general ; hypertext hypermedia systems ; distributed systems architectures ; parallel ; hypermedia hypermedia ; multiple hypermedia architectures ; parallel crawlers ; hypermedia design ; multiple hypermedia systems ; software distributed systems terms ; crawling architecture ; distributed system general terms ; crawling architectures ; distributed system systems ; hypermedia ; crawling process ; multiple hypermedia ; hypertext architecture architectures ; multiple crawling ; parallel hypertext ; distributed system terms ; multiple architecture architectures ; hypertext architecture and design ; multiple architectures architectures ; hypertext architectures for distributed systems ; distributed system architectures ; hypertext architecture h <digit> ; distributed system general terms management ; software distributed systems systems ; hypertext architectures for parallel crawler ; general crawler ; crawling design ; multiple architecture systems ; hypermedia hypermedia architectures ; distributed systems general terms management ; software distributed distributed systems ; hypertext crawler architectures ; distributed general term ; hypermedia design ; distributed general term management ; software distributed system general terms ; hypertext architectures for hypermedia ; multiple hypermedia architectures ; hypermedia hypermedia architecture ; distributed systems general terms', '3g wireless networks ; communication networks ; wireless networks ; computer networks ; 3g network gaming ; wireless communication ; wireless communication networks ; computer communication networks ; 3g network ; wireless ; computer network ; 3g networks ; wireless network ; virtual game ; network gaming ; wireless communication network ; file transfer ; network game ; wireless communications ; 3g wireless network ; virtual game ; computer network architecture ; network games ; wireless network game ; 3g network game ; virtual games ; communication network ; 3g network games ; 3g ; wireless network gaming ; computer communication network ; 3g cellular networks ; wireless network games ; communication ; 3g cellular links ; wireless communication network game ; computer games ; 3g cellular network ; wireless network architecture ; computer network game ; 3g 3g wireless networks ; network game ; computer network games ; wireless communication systems ; virtual games ; 3g cellular ; network games ; communication networking ; virtual ; communication delay ; 3g wireless communication ; 3g wireless networks ; computer network gaming ; network architecture ; wireless communication communication ; communication network architecture ; 3g wireless communication networks ; network gaming ; computer wireless communication networks ; 3g wireless network gaming ; wireless communication network games ; virtual world ; 3g wireless network game ; wireless communication network gaming ; computer communication ; 3g network architecture ; communication network game ; virtual game world ; computer wireless networks ; network gaming game ; virtual networks ; virtual game world ; 3g wireless ; wireless communications networks ; computer communication network architecture ; 3g 3g ; network ; virtual environment ; 3g network gaming game ; wireless communications network ; file sharing ; wireless communication network architecture ; network gaming gaming ; wireless communication communication networks ; computer communication network game ; network gaming games ; network architecture ; 3g wireless network games ; virtual communication ; computer wireless communication ; 3g network gaming gaming ; wireless networks architecture ; computer networks ; 3g networks ; computer wireless communication network ; 3g wireless communication network ; 3g network gaming ; file delivery ; 3g 3g networks ; 3g network ; communication network games ; 3g 3g wireless network ; 3g wireless network', 'cellular automata ; programmable graphics hardware ; cml ; real time ; hardware ; cml ; lattice ; real time visual simulation ; graphics ; discrete lattice ; graphics hardware ; programmable ; coupled map lattice ; lattice values ; real time computation ; programmable graphics ; programmable graphics hardware ; cellular automaton ; hardware acceleration ; real time rendering ; cml cml ; real time simulation ; dynamic system ; discrete ; dynamic systems ; graphics applications ; real time visual convection ; dynamic phenomena ; hardware and visual simulation ; lattice computations ; discrete maps ; cellular hardware ; hardware applications ; discrete surfaces ; programmable graphics applications ; lattice nodes ; discrete lattice values ; dynamic ; lattice and values ; real time visual ; programmable graphics automata ; programmable graphics ; cellular graphics ; lattice and use ; discrete topology ; coupled map lattices ; cml nodes ; cml cml ; programmable ; graphics hardware applications ; real time visual simulations ; programmable graphics graphics hardware ; hardware and use level programming ; real time visual simulator ; dynamic simulation ; hardware visual simulation ; programmable graphics hardware applications ; graphics processors ; programmable graphics applications ; cellular graphics hardware ; cml simulations ; real lattice ; coupled map graphs ; hardware and use ; programmable visual simulation ; graphics hardware hardware ; discrete lattice computations ; coupled map lattice lattice ; cml and boiling ; discrete lattice lattice ; coupled maps lattice ; lattice and nodes ; programmable visual simulation ; cellular ; graphics architectures ; coupled map map lattice ; lattice and computations ; programmable graphics automata ; programmable 3d graphics hardware ; hardware and visual convection ; cml simulations ; coupled maps ; discrete lattice and boiling ; programmable logic ; hardware and graphics ; cml cellular automata ; programmable graphics graphics ; lattice and pixel level programming ; discrete lattice and use ; cellular automata hardware ; hardware and pixel level programming ; real graphics ; cellular automata lattice ; discrete lattice and values ; programmable 3d graphics ; hardware and use programming ; cml and boiling ; programmable graphics hardware hardware ; hardware and graphics hardware ; cellular automata graphics ; cml and cml ; programmable graphics graphics hardware', 'virtual environments ; computer graphics ; haptic ; virtual environment ; skin temperature ; haptic element ; virtual reality ; haptics ; measure of presence ; haptic reactions ; virtual room ; skin conductance ; measure ; graphics ; physiological ; computer interfaces ; physiological reactions ; virtual situation ; graphics presentation ; haptic presentation ; passive element ; physiological reaction ; virtual reality interfaces ; skin ; haptic element interfaces ; haptic element presentation ; virtual environment presence ; computer vision ; graphics realism ; virtual environment interfaces ; passive reactions ; measure presence ; stressful virtual environments ; passive presence ; haptic and computer graphics ; virtual environments presence ; computer graphics interfaces ; virtual environment environment ; graphics user interfaces ; haptic and haptic element ; computer graphics ; graphics interfaces ; haptic element height ; stressful virtual environment ; computer music ; haptic element physiology ; virtual reality environment ; skin temperature conductance ; haptic and haptic reactions ; virtual reality presence ; computer physiology ; haptic and skin temperature ; virtual environment height ; computer graphics presentation ; haptic and haptic ; passive haptic ; virtual reality physiology ; skin rate ; haptic and element ; measure presence ; passive virtual environments ; haptic and skin conductance ; virtual environments interfaces ; passive haptic element ; haptic and computer interfaces ; passive virtual environment ; haptic and passive reactions ; graphics presentation rate ; physiological measures ; virtual environments environment ; graphics user interface ; haptics interfaces ; computer environment ; haptics presence ; measure of presence ; computer graphics presence ; haptic and element presentation ; measure ; graphics user ; physiological responses ; virtual situation presence ; passive virtual reality ; haptic and element interfaces ; stressful virtual reality ; skin rates ; haptic and element physiology ; virtual room presence ; haptics realism ; stressful presence ; haptic and passive element ; virtual environments virtual environment ; skin conductance of presence ; measure and presence ; computer presence ; measure of presence presence ; computer interfaces', 'logistic regression ; contextual ; <digit> character words ; text segmentation ; contextual information ; multi word terms ; overlapping ; segmentation ; mutual information ; overlapping bigrams ; <digit> character word ; character words ; probablity ; multi word ; statistical ; chinese text segmentation ; text ; contextual information formula ; chinese ; character word ; contextual formula ; <digit> ; statistical formula ; <digit> character ; chinese text ; multi word word ; character ; contextual contextual information ; <digit> character segmentation ; character segmentation ; contextual document ; multi word word terms ; logistic logistic regression ; multi word family ; logistic ; <digit> character formula ; mutual information formula ; multi word segmentation ; logistic formula ; overlapping found ; multi character words ; mutual formula ; probablity formula ; <digit> bigrams ; logistic model ; overlapping bigrams found ; multi ; probablity bigram ; multi character word ; text frequency ; overlapping article ; multi character ; character word segmentation ; statistical contextual information ; multi character segmentation ; text clustering ; statistical factor ; text words ; contextual document frequency ; multi terms ; mutual information regression ; probablity local information ; character text ; statistical factors ; multi text segmentation ; character word terms ; overlapping character words ; probablity overlapping ; <digit> word ; contextual document formula ; contextual information regression ; <digit> character word terms ; logistic regression formula ; overlapping character ; multi text ; character text segmentation ; contextual information in chinese text ; multi word word segmentation ; character word words ; overlapping bigrams bigrams ; <digit> english text segmentation ; multi word family terms ; logistic regression regression ; contextual contextual ; multi word word word ; logistic regression using logistic regression ; probablity overlapping bigrams ; chinese text word ; character words in chinese text ; multi word word family ; character words in chinese', 'access control ; trust negotiation ; access control policy ; negotiation ; access control policies ; trust ; security ; access control strategy ; trust negotiation systems ; trust negotiation ; automated trust negotiation ; trust ; access control policy databases ; negotiation systems ; access control policy database ; safety ; access controls ; trust negotiation system ; security policies ; safety model ; trust management ; negotiation system ; policy databases ; security information leakage ; access control protocol ; safety model ; security negotiation ; trust negotiation systems ; digital credentials ; trust protection ; security policy ; sensitive information leakage ; safety ; security policy databases ; access control policy query ; policy database ; negotiation policies ; sensitive information ; policy ; negotiation policy ; access control policy ontologies ; trust negotiation protocols ; trust safety ; security information ; bilateral information exchange ; trust negotiation policy ; trust negotiation system ; sensitive databases ; automated ; security policy database ; bilateral information leakage ; automated trust negotiation systems ; negotiation policy databases ; access controller ; trust management systems ; sensitive information protection ; sensitive credentials ; safety negotiation ; trust safety model ; safety models ; safety negotiation ; trust management ; access system ; automated negotiation ; safety modeling ; bilateral information ; trust protection systems ; digital signatures ; safety models ; security information security ; access control policy database theory ; automated information leakage ; security information theory ; access control strategy databases ; trust management system ; security information gain ; access control policy database security ; security negotiation systems ; access control policy database systems ; trust negotiation policy databases ; negotiation policy database ; access control policies databases ; negotiation systems theory ; access policy ; trust negotiation systems theory ; trust protection ; safety modeling ; security policy ontologies ; sensitive information exchange ; policy ontologies ; security policy negotiation ; policy intelligence ; security information leakage model ; bilateral information protection ; trust negotiation protocols', 'markov chain monte carlo ; unsupervised learning ; learning ; markov chains ; online query interface ; topic ; markov chain monte carlo algorithm ; digital library ; parsing ; markov chain ; artificial intelligence ; unsupervised ; markov chains monte carlo ; online query ; unsupervised learning ; markov chain monte carlo algorithms ; topic models ; digital libraries ; learning algorithms ; topic mixture ; learning technique ; topic model ; markov chains monte carlo algorithm ; online ; monte carlo algorithm ; online learning ; parsing and parsing ; markov chain monte carlo methods ; online query languages ; topic mixture models ; monte carlo ; online query processing ; topic topic ; markov chain monte carlo method ; online query interfaces ; markov carlo ; online queries ; markov models ; unsupervised learning technique ; learning by topics ; markov chains monte carlo algorithms ; online searching ; topic topic models ; monte carlo algorithms ; unsupervised learning algorithms ; learning by detail ; markov processes ; unsupervised query interface ; markov carlo algorithm ; artificial corpora ; parsing and detection ; markov chain methods ; digital forensics ; topic mixture model ; em ; digital corpora ; learning of information ; association rules ; unsupervised query ; learning of abstracts ; markov chain models ; unsupervised learning techniques ; learning by parsing ; association chain monte carlo ; unsupervised ; parsing of abstracts ; markov chain method ; digital learning ; learning by abstracts ; unsupervised learning methods ; learning of topics ; markov chains monte carlo methods ; artificial learning ; topic topic model ; association ; unsupervised model ; parsing of topics ; monte carlo methods ; learning of author trends ; markov chains monte carlo method ; digital library library ; parsing by topics ; association chain ; parsing algorithms ; monte carlo method ; unsupervised author topic models ; learning by author trends ; association models ; artificial neural carlo ; learning of large text collections ; markov carlo algorithms ; online query interface model ; parsing of information ; markov chains chain monte carlo ; artificial neural networks ; topic topic mixture ; association topic models ; digital library and library ; learning of author topic models ; em algorithm ; learning of author topic', 'web search ; search user interfaces ; e g hci ; search interface ; evaluation ; search user interface ; e g ; search interfaces ; classification ; information retrieval ; search speed ; evaluation methodology ; web search user interfaces ; usability ; e g evaluation ; subjective satisfaction ; search ; usability measures ; web search interface ; e hci ; search interface usability ; e ; search user interface usability ; e g test ; web search behavior ; usability measures ; e g evaluation methodology ; search interface usability measures ; e g h3 measure ; e g h3 ; web search user interface ; usability evaluation ; web searching ; e search ; information retrieval ; evaluation framework ; user interfaces ; e g hci e g ; web search process ; usability measure ; storage retrieval ; search user interface usability measures ; web usability ; search user evaluation ; evaluation methods ; storage search ; search user interface evaluation ; evaluation evaluation ; storage and retrieval ; user interface ; usability evaluation ; web user interfaces ; search user interfaces usability ; web usability measures ; search user ; e user interfaces ; search user interfaces evaluation ; search interface evaluation ; usability measurement ; storage information retrieval ; usability ; e g h3 measures ; search user interfaces usability measures ; e g hci hci ; usability measure ; web page ; user interface usability ; e user interface ; storage and information retrieval ; search interface usability measure ; e search user interfaces ; search interfaces usability ; e g h3 classification ; storage user interfaces ; classification methodology ; e g h3 features ; web search user interface usability ; search interface usability evaluation ; e g hci evaluation']'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_env2",
   "language": "python",
   "name": "all_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
