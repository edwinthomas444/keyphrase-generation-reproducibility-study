exp: kpgen-meng17-MagKP_Nlarge-verbatim_append-transformer-L6H8-Copytrue-Covtrue
exp_dir: output/keyphrase/meng17-one2seq/meng17-one2seq-kp20k-v3/kpgen-meng17-MagKP_Nlarge-verbatim_append-transformer-L6H8-BS4096-LR0.05-L6-H8-Dim512-Emb512-Dropout0.1-Copytrue-Covtrue
save_model: models/keyphrase/meng17-one2seq/meng17-one2seq-kp20k-v3/kpgen-meng17-MagKP_Nlarge-verbatim_append-transformer-L6H8-BS4096-LR0.05-L6-H8-Dim512-Emb512-Dropout0.1-Copytrue-Covtrue
log_file: output/keyphrase/meng17-one2seq/meng17-one2seq-kp20k-v3/kpgen-meng17-MagKP_Nlarge-verbatim_append-transformer-L6H8-BS4096-LR0.05-L6-H8-Dim512-Emb512-Dropout0.1-Copytrue-Covtrue/log.txt
wandb_project: kp20k-meng17-one2one

model_type: keyphrase
data_type: keyphrase
data_format: pt
tgt_type: verbatim_append 

multi_dataset: true
shuffle_shards: true
data: none
vocab: /zfs1/pbrusilovsky/rum20/kp/OpenNMT-kpg/data/keyphrase/meng17/magkp20k.vocab.pt
data_ids:
#  - /zfs1/pbrusilovsky/rum20/kp/OpenNMT-kpg/data/keyphrase/meng17/kp20k_pt_shard10k/
#  - /zfs1/pbrusilovsky/rum20/kp/OpenNMT-kpg/data/keyphrase/meng17/magkp_pt_shard10k/
#   - /zfs1/pbrusilovsky/rum20/kp/OpenNMT-kpg/data/keyphrase/meng17/magkp_LN_pt_shard10k/
#   - /zfs1/pbrusilovsky/rum20/kp/OpenNMT-kpg/data/keyphrase/meng17/magkp_Nsmall_pt_shard10k/
  - /zfs1/pbrusilovsky/rum20/kp/OpenNMT-kpg/data/keyphrase/meng17/magkp_Nlarge_pt_shard10k/

data_weights:
  - 1
#  - 1
  # - 1
  # - 1
  # - 1

valid_data_ids:
  - /zfs1/pbrusilovsky/rum20/kp/OpenNMT-kpg/data/keyphrase/meng17/kp20k_pt_shard10k/valid

encoder_type: transformer
decoder_type: transformer
word_vec_size: 512
rnn_size: 512
layers: 6
heads: 8
dropout: 0.1

share_embeddings: 'true'
copy_attn: 'true'
reuse_copy_attn: 'true'
coverage_attn: 'true'
context_gate: both
input_feed: 1
param_init_glorot: 'true'
position_encoding: 'true'

optim: adam
learning_rate: 0.05
param_init: 0
warmup_steps: 8000
decay_method: noam_simple
label_smoothing: 0.1
adam_beta2: 0.998

batch_type: tokens
normalization: tokens
max_generator_batches: 200
accum_count: 4
max_grad_norm: 2.0


# batch_size is actually: num_example * max(#word in src/tgt)
batch_size: 4096
valid_batch_size: 64

train_steps: 300000
valid_steps: 10000
save_checkpoint_steps: 5000
report_every: 100
seed: 3435

log_file_level: DEBUG
tensorboard: 'false'

#tensorboard_log_dir: runs/kp20k.one2one.transformer/

wandb: 'true'
wandb_key: 'c338136c195ab221b8c7cfaa446db16b2e86c6db'


world_size: 1
gpu_ranks:
- 0
#- 1
#- 2
#master_port: 10000