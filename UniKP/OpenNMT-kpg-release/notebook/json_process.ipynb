{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "The goal of this script is to process a few common keyphrase datasets, including\n",
    " - **Tokenize**: by default using method from Meng et al. 2017, which fits more for academic text since it splits strings by hyphen etc. and makes tokens more fine-grained. \n",
    "     - keep [_<>,\\(\\)\\.\\'%]\n",
    "     - replace digits with < digit >\n",
    "     - split by [^a-zA-Z0-9_<>,#&\\+\\*\\(\\)\\.\\'%]\n",
    " - **Determine present/absent phrases**: determine whether a phrase appears verbatim in a text. This is believed a very important step for the evaluation of keyphrase-related tasks, since in general extraction methods cannot recall any phrases don't appear in the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow\n",
    "print(torch.__version__, torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../onmt'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import kp_evaluate\n",
    "import onmt.keyphrase.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "dataset_names = ['inspec', 'krapivin', 'nus', 'semeval', 'kp20k', 'duc', 'stackexchange']\n",
    "\n",
    "# path to the json folder\n",
    "json_base_dir = 'UniKP/UniKeyphrase/data' \n",
    "# store in preprocessed data directory\n",
    "splits = ['train', 'test', 'valid']\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    for split in splits:\n",
    "        print(dataset_name)\n",
    "\n",
    "        input_json_path = os.path.join(json_base_dir, dataset_name, '%s_%s.json' % (dataset_name, split))\n",
    "        if not os.path.isfile(input_json_path):\n",
    "            print('File %s does not exist, skipping..' % os.path.basename(input_json_path))\n",
    "            continue\n",
    "\n",
    "        output_json_path = os.path.join(json_base_dir, dataset_name, '%s_%s_meng17token.json' % (dataset_name, split))\n",
    "\n",
    "        doc_count, present_doc_count, absent_doc_count = 0, 0, 0\n",
    "        tgt_num, present_tgt_num, absent_tgt_num = [], [], []\n",
    "\n",
    "        with open(input_json_path, 'r') as input_json, open(output_json_path, 'w') as output_json:\n",
    "            lines = input_json.readlines()\n",
    "            for json_line in tqdm(lines, total=len(lines)):\n",
    "                json_dict = json.loads(json_line)\n",
    "\n",
    "                if dataset_name == 'stackexchange':\n",
    "                    json_dict['abstract'] = json_dict['question']\n",
    "                    json_dict['keywords'] = json_dict['tags']            \n",
    "                    del json_dict['question']\n",
    "                    del json_dict['tags']\n",
    "                if dataset_name == 'openkp':\n",
    "                    json_dict['abstract'] = json_dict['text']\n",
    "                    json_dict['keywords'] = json_dict['KeyPhrases']\n",
    "                    # no title\n",
    "                    json_dict['title'] = ''\n",
    "                    del json_dict['text']\n",
    "                    del json_dict['KeyPhrases']\n",
    "                if dataset_name == \"kptimes\":\n",
    "                    json_dict[\"keywords\"] = json_dict[\"keyword\"]\n",
    "                    del json_dict[\"keyword\"]\n",
    "\n",
    "                title = json_dict['title']\n",
    "                abstract = json_dict['abstract']\n",
    "                keywords = json_dict['keywords']\n",
    "\n",
    "                if isinstance(keywords, str):\n",
    "                    keywords = keywords.split(';')\n",
    "                    json_dict['keywords'] = keywords\n",
    "                # remove all the abbreviations/acronyms in parentheses in keyphrases\n",
    "                keywords = [re.sub(r'\\(.*?\\)|\\[.*?\\]|\\{.*?\\}', '', kw) for kw in keywords]\n",
    "\n",
    "                # tokenize text\n",
    "                title_token = utils.meng17_tokenize(title)\n",
    "                abstract_token = utils.meng17_tokenize(abstract)\n",
    "                keywords_token = [utils.meng17_tokenize(kw) for kw in keywords]\n",
    "\n",
    "                # replace numbers\n",
    "                title_token = utils.replace_numbers_to_DIGIT(title_token, k=2)\n",
    "                # restrict to maximum 384 tokens for longer datasets (like kptimes, openkp etc)\n",
    "                abstract_token = utils.replace_numbers_to_DIGIT(abstract_token, k=2)[:384]\n",
    "                keywords_token = [utils.replace_numbers_to_DIGIT(kw, k=2) for kw in keywords_token]                \n",
    "                \n",
    "                num_title_tokens = len(title_token)\n",
    "                json_dict['title_len'] = num_title_tokens\n",
    "                \n",
    "                src_token = title_token+[\".\"]+abstract_token\n",
    "                # print('len of source tokens: ', len(src_token))\n",
    "                tgts_token = keywords_token\n",
    "\n",
    "                # split tgts by present/absent\n",
    "                src_seq = src_token\n",
    "                tgt_seqs = tgts_token\n",
    "\n",
    "                present_tgt_flags, _, _ = utils.if_present_duplicate_phrases(src_seq, tgt_seqs)\n",
    "                present_tgts = [tgt for tgt, present in zip(tgt_seqs, present_tgt_flags) if present]\n",
    "                absent_tgts = [tgt for tgt, present in zip(tgt_seqs, present_tgt_flags) if ~present]\n",
    "\n",
    "                doc_count += 1\n",
    "                present_doc_count = present_doc_count + 1 if len(present_tgts) > 0 else present_doc_count\n",
    "                absent_doc_count = absent_doc_count + 1 if len(absent_tgts) > 0 else absent_doc_count\n",
    "\n",
    "                tgt_num.append(len(tgt_seqs))\n",
    "                present_tgt_num.append(len(present_tgts))\n",
    "                absent_tgt_num.append(len(absent_tgts))\n",
    "\n",
    "                # write to output json\n",
    "                tokenized_dict = {'src': src_token, 'tgt': tgts_token, \n",
    "                                  'present_tgt': present_tgts, 'absent_tgt': absent_tgts}\n",
    "                json_dict['meng17_tokenized'] = tokenized_dict\n",
    "                output_json.write(json.dumps(json_dict) + '\\n')\n",
    "\n",
    "        print('#doc=%d, #present_doc=%d, #absent_doc=%d, #tgt=%d, #present=%d, #absent=%d' \n",
    "              % (doc_count, present_doc_count, absent_doc_count, \n",
    "                 sum(tgt_num), sum(present_tgt_num), sum(absent_tgt_num)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Datasets to KPDrop format (output inside ./dataset directory inside KPDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset name:  openkp\n",
      "split:  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 134894/134894 [00:07<00:00, 17016.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split:  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 6614/6614 [00:00<00:00, 19568.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split:  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 6616/6616 [00:00<00:00, 20208.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset name:  kptimes\n",
      "split:  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 259923/259923 [00:17<00:00, 15281.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split:  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 15487.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split:  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 15646.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset name:  stackexchange\n",
      "split:  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 298965/298965 [00:09<00:00, 31895.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split:  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 33192.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split:  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 33822.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset name:  krapivin\n",
      "split:  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 460/460 [00:00<00:00, 5702.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset name:  semeval\n",
      "split:  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 3898.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset name:  inspec\n",
      "split:  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 32686.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset name:  nus\n",
      "split:  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 4711.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# converting to KPDrop Format\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_names = [\"openkp\", \"kptimes\", \"stackexchange\", \"krapivin\", \"semeval\", \"inspec\", \"nus\"]\n",
    "# train dataset names\n",
    "train_dataset_names = [\"openkp\", \"kptimes\", \"stackexchange\"]\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print('dataset name: ', dataset_name)\n",
    "    if dataset_name in train_dataset_names:\n",
    "        splits = [\"train\", \"test\", \"valid\"]\n",
    "    else:\n",
    "        splits = [\"test\"]\n",
    "    \n",
    "    for split in splits:\n",
    "        print('split: ', split)\n",
    "        data_dir = f\"UniKP/UniKeyphrase/data/{dataset_name}\"\n",
    "        out_data_dir = f\"KPDrop/dataset/{dataset_name}\"\n",
    "        if not os.path.exists(out_data_dir):\n",
    "            os.makedirs(out_data_dir, exist_ok=True)\n",
    "        \n",
    "        path = f'{dataset_name}_{split}_meng17token.json'\n",
    "        \n",
    "        kpdrop_tgt = os.path.join(out_data_dir,f'{split}_trg.txt')\n",
    "        kpdrop_src = os.path.join(out_data_dir,f'{split}_src.txt')\n",
    "        in_path = os.path.join(data_dir, path)\n",
    "\n",
    "        with open(in_path, 'r') as f, open(kpdrop_tgt, 'w') as f1, open(kpdrop_src, 'w') as f2:\n",
    "            lines = f.readlines()\n",
    "            for line in tqdm(lines, total=len(lines)):\n",
    "                line = json.loads(line)\n",
    "                # print(line)\n",
    "                source = line['meng17_tokenized']['src']\n",
    "                num_tokens = line['title_len']\n",
    "\n",
    "\n",
    "                source = source[:num_tokens] + ['<eos>'] + source[num_tokens:]\n",
    "                source_text = \" \".join(source)\n",
    "\n",
    "                targets = line['meng17_tokenized']['tgt']\n",
    "                target_list = []\n",
    "                for targ in targets:\n",
    "                    kp = \" \".join(targ)\n",
    "                    target_list.append(kp)\n",
    "                target_text = \";\".join(target_list)\n",
    "\n",
    "                f2.write(source_text.strip().lower()+\"\\n\")\n",
    "                f1.write(target_text.strip().lower()+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
