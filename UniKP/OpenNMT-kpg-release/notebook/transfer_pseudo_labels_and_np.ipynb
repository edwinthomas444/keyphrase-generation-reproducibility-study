{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ihome/hdaqing/rum20/anaconda3/envs/kp/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "exec('from __future__ import unicode_literals')\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../onmt'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from onmt.translate.translator import build_translator\n",
    "from onmt.constants import ModelTask\n",
    "import onmt.keyphrase.eval as eval\n",
    "from onmt.keyphrase.pke.utils import compute_document_frequency\n",
    "from onmt.keyphrase.utils import validate_phrases, if_present_duplicate_phrases\n",
    "from onmt.utils.parse import ArgumentParser\n",
    "import onmt.keyphrase.pke as pke\n",
    "\n",
    "from kp_gen_eval_transfer import _get_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'onmt.keyphrase.eval' from '/zfs1/hdaqing/rum20/kp/OpenNMT-kpg-transfer/onmt/keyphrase/eval.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki-transferred KPs\n",
    "\n",
    "Also see **onmt.keyphrase.kp_inference.py**.\n",
    "\n",
    "This part of notebook is meant to manually find a good prefix `prompt` to generate good pseudo labels. The actually code to generate pseudo labels is\n",
    "```\n",
    "### for kp20k/openkp/kptimes/stackex\n",
    "python kp_gen_magkp_transfer.py -config config/transfer_kp/infer/keyphrase-one2seq-controlled.yml -tasks pred -exp_root_dir /zfs1/hdaqing/rum20/kp/transfer_exps/kp_bart_DA/bart_kppretrain_wiki_1e5_controlled -data_dir /zfs1/hdaqing/rum20/kp/data/kp/oag_v1_cs_nokp/ -output_dir /zfs1/hdaqing/rum20/kp/data/kp/oag_v1_cs_nokp_wikiTL/ -gpu 0 -batch_size 32 -beam_size 1 -max_length 60\n",
    "\n",
    "### for Mag\n",
    "python kp_gen_magkp_transfer_labelling.py -config config/transfer_kp/infer/keyphrase-one2seq-controlled.yml -tasks pred -exp_root_dir /zfs1/hdaqing/rum20/kp/transfer_exps/kp_bart_DA/bart_kppretrain_wiki_1e5_controlled -data_dir /zfs1/hdaqing/rum20/kp/data/kp/oag_v1_cs_nokp/ -output_dir /zfs1/hdaqing/rum20/kp/data/kp/oag_v1_cs_nokp_wikiTL/ -gpu 0 -batch_size 32 -beam_size 1 -max_length 60\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_print(src_text, tgt_kps, pred_kps, pred_scores, unk_token='<unk>'):\n",
    "    src_seq = [t.text.lower() for t in spacy_nlp(src_text, disable=[\"textcat\"])]\n",
    "    tgt_seqs = [[t.text.lower() for t in spacy_nlp(p, disable=[\"textcat\"])] for p in tgt_kps]\n",
    "    pred_seqs = [[t.text.lower() for t in spacy_nlp(p, disable=[\"textcat\"])] for p in pred_kps]\n",
    "\n",
    "    topk_range = ['k', 10]\n",
    "    absent_topk_range = ['M']\n",
    "    metric_names = ['f_score'] # 'precision', 'recall', 'f_score'\n",
    "\n",
    "    # 1st filtering, ignore phrases having <unk> and puncs\n",
    "    valid_pred_flags = validate_phrases(pred_seqs, unk_token)\n",
    "    # 2nd filtering: filter out phrases that don't appear in text, and keep unique ones after stemming\n",
    "    present_pred_flags, _, duplicate_flags = if_present_duplicate_phrases(src_seq, pred_seqs)\n",
    "    # treat duplicates as invalid\n",
    "    valid_pred_flags = valid_pred_flags * ~duplicate_flags if len(valid_pred_flags) > 0 else []\n",
    "    valid_and_present_flags = valid_pred_flags * present_pred_flags if len(valid_pred_flags) > 0 else []\n",
    "    valid_and_absent_flags = valid_pred_flags * ~present_pred_flags if len(valid_pred_flags) > 0 else []\n",
    "\n",
    "    # compute match scores (exact, partial and mixed), for exact it's a list otherwise matrix\n",
    "    match_scores_exact = eval.compute_match_scores(tgt_seqs=tgt_seqs, pred_seqs=pred_seqs,\n",
    "                                              do_lower=True, do_stem=True, type='exact')\n",
    "    # split tgts by present/absent\n",
    "    present_tgt_flags, _, _ = if_present_duplicate_phrases(src_seq, tgt_seqs)\n",
    "    present_tgts = [tgt for tgt, present in zip(tgt_seqs, present_tgt_flags) if present]\n",
    "    absent_tgts = [tgt for tgt, present in zip(tgt_seqs, present_tgt_flags) if ~present]\n",
    "\n",
    "    # filter out results of invalid preds\n",
    "    valid_preds = [seq for seq, valid in zip(pred_seqs, valid_pred_flags) if valid]\n",
    "    valid_present_pred_flags = present_pred_flags[valid_pred_flags]\n",
    "\n",
    "    valid_match_scores_exact = match_scores_exact[valid_pred_flags]\n",
    "\n",
    "    # split preds by present/absent and exact/partial/mixed\n",
    "    valid_present_preds = [pred for pred, present in zip(valid_preds, valid_present_pred_flags) if present]\n",
    "    valid_absent_preds = [pred for pred, present in zip(valid_preds, valid_present_pred_flags) if ~present]\n",
    "    present_exact_match_scores = valid_match_scores_exact[valid_present_pred_flags]\n",
    "    absent_exact_match_scores = valid_match_scores_exact[~valid_present_pred_flags]\n",
    "\n",
    "    all_exact_results = eval.run_classic_metrics(valid_match_scores_exact, valid_preds, tgt_seqs, metric_names, topk_range)\n",
    "    present_exact_results = eval.run_classic_metrics(present_exact_match_scores, valid_present_preds, present_tgts, metric_names, topk_range)\n",
    "    absent_exact_results = eval.run_classic_metrics(absent_exact_match_scores, valid_absent_preds, absent_tgts, metric_names, absent_topk_range)\n",
    "\n",
    "    eval_results_names = ['all_exact', 'present_exact', 'absent_exact']\n",
    "    eval_results_list = [all_exact_results, present_exact_results, absent_exact_results]\n",
    "\n",
    "    print_out = print_predeval_result(src_text,\n",
    "                                      tgt_seqs, present_tgt_flags,\n",
    "                                      pred_seqs, pred_scores, present_pred_flags, valid_pred_flags,\n",
    "                                      valid_and_present_flags, valid_and_absent_flags, match_scores_exact,\n",
    "                                      eval_results_names, eval_results_list)\n",
    "\n",
    "    print('[#present_tgts=%d] ' % len(present_tgts), str(present_tgts))\n",
    "    print('[#absent_tgts=%d]' % len(absent_tgts), str(absent_tgts))\n",
    "    \n",
    "    print('[#valid_present_preds=%d]' % len(valid_present_preds), str(valid_present_preds))\n",
    "    print('[#valid_absent_preds=%d]' % len(valid_absent_preds), str(valid_absent_preds))\n",
    "    \n",
    "    print('match_scores_exact', str(match_scores_exact))\n",
    "    \n",
    "    print('valid_match_scores_exact', str(valid_match_scores_exact))\n",
    "    print('all_exact_results', str(all_exact_results))\n",
    "    \n",
    "    print('present_exact_match_scores', str(present_exact_match_scores))\n",
    "    print('present_exact_results', str(present_exact_results))\n",
    "    \n",
    "    print('absent_exact_match_scores', str(absent_exact_match_scores))\n",
    "    print('absent_exact_results', str(absent_exact_results))\n",
    "    print(print_out)\n",
    "\n",
    "\n",
    "def print_predeval_result(src_text,\n",
    "                          tgt_seqs, present_tgt_flags,\n",
    "                          pred_seqs, pred_scores, present_pred_flags, valid_pred_flags,\n",
    "                          valid_and_present_flags, valid_and_absent_flags, match_scores_exact,\n",
    "                          results_names, results_list):\n",
    "    print_out = '=' * 50\n",
    "    print_out += '\\n[Source]: %s \\n' % (src_text)\n",
    "\n",
    "    print_out += '[GROUND-TRUTH] #(all)=%d, #(present)=%d, #(absent)=%d\\n' % \\\n",
    "                 (len(present_tgt_flags), sum(present_tgt_flags), len(present_tgt_flags)-sum(present_tgt_flags))\n",
    "    print_out += '\\n'.join(['\\t\\t[%s]' % ' '.join(phrase) if is_present else '\\t\\t%s' % ' '.join(phrase) for phrase, is_present in zip(tgt_seqs, present_tgt_flags)])\n",
    "\n",
    "    print_out += '\\n[PREDICTION] #(all)=%d, #(valid)=%d, #(present)=%d, ' \\\n",
    "                 '#(valid&present)=%d, #(valid&absent)=%d\\n' % (len(pred_seqs), sum(valid_pred_flags), sum(present_pred_flags), sum(valid_and_present_flags), sum(valid_and_absent_flags))\n",
    "    print_out += ''\n",
    "    preds_out = ''\n",
    "    for p_id, (word, match, is_valid, is_present) in enumerate(zip(pred_seqs, match_scores_exact, valid_pred_flags, present_pred_flags)):\n",
    "        score = pred_scores[p_id] if pred_scores else \"Score N/A\"\n",
    "\n",
    "        preds_out += '%s\\n' % (' '.join(word))\n",
    "        if is_present:\n",
    "            print_phrase = '[%s]' % ' '.join(word)\n",
    "        else:\n",
    "            print_phrase = ' '.join(word)\n",
    "\n",
    "        if match == 1.0:\n",
    "            correct_str = '[correct!]'\n",
    "        else:\n",
    "            correct_str = ''\n",
    "\n",
    "        pred_str = '\\t\\t[%d] %s\\t%s \\t%s\\n' % (p_id + 1, '[%.4f]' % (-score) if pred_scores else \"Score N/A\",\n",
    "                                                print_phrase, correct_str)\n",
    "        if not is_valid:\n",
    "            pred_str = '\\t%s' % pred_str\n",
    "\n",
    "        print_out += pred_str\n",
    "\n",
    "    print_out += \"\\n ======================================================= \\n\"\n",
    "\n",
    "    print_out += '[GROUND-TRUTH] #(all)=%d, #(present)=%d, #(absent)=%d\\n' % \\\n",
    "                 (len(present_tgt_flags), sum(present_tgt_flags), len(present_tgt_flags)-sum(present_tgt_flags))\n",
    "    print_out += '\\n[PREDICTION] #(all)=%d, #(valid)=%d, #(present)=%d, ' \\\n",
    "                 '#(valid&present)=%d, #(valid&absent)=%d\\n' % (len(pred_seqs), sum(valid_pred_flags), sum(present_pred_flags), sum(valid_and_present_flags), sum(valid_and_absent_flags))\n",
    "\n",
    "    for name, results in zip(results_names, results_list):\n",
    "        # print @5@10@O@M for present_exact, print @50@M for absent_exact\n",
    "        if name in ['all_exact', 'present_exact', 'absent_exact']:\n",
    "            if name.startswith('all') or name.startswith('present'):\n",
    "                topk_list = ['10', 'k']\n",
    "            else:\n",
    "                topk_list = ['M']\n",
    "\n",
    "            for topk in topk_list:\n",
    "                print_out += \"\\n --- batch {} F1 @{}: \\t\".format(name, topk) \\\n",
    "                             + \"{:.4f}\".format(results['f_score@{}'.format(topk)])\n",
    "        else:\n",
    "            # ignore partial for now\n",
    "            continue\n",
    "\n",
    "    print_out += \"\\n =======================================================\"\n",
    "\n",
    "    return print_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load translator and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ihome/hdaqing/rum20/anaconda3/envs/kp/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/ihome/hdaqing/rum20/anaconda3/envs/kp/lib/python3.7/site-packages/torchtext/data/field.py:36: UserWarning: TextMultiField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/ihome/hdaqing/rum20/anaconda3/envs/kp/lib/python3.7/site-packages/torchtext/data/field.py:36: UserWarning: RawField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained vocabulary from /zfs1/hdaqing/rum20/kp/data/kp/hf_vocab/roberta-base-kp\n",
      "Vocab size=50265, base vocab size=50265\n"
     ]
    }
   ],
   "source": [
    "# specify GPU device\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "# Supervised Deep Keyphrase Model, using OpenNMT 2.x pipeline\n",
    "parser = _get_parser()\n",
    "config_path = '/zfs1/hdaqing/rum20/kp/OpenNMT-kpg-transfer/config/transfer_kp/infer/keyphrase-one2seq-controlled.yml'\n",
    "opt = parser.parse_args('-config %s' % (config_path))\n",
    "\n",
    "ckpt_path = '/zfs1/hdaqing/rum20/kp/fairseq-kpg/exps/kp/bart_kppretrain_wiki_1e5/ckpts/checkpoint_step_100000.pt'\n",
    "opt.__setattr__('models', [ckpt_path])\n",
    "opt.__setattr__('fairseq_model', True)\n",
    "opt.__setattr__('encoder_type', 'bart')\n",
    "opt.__setattr__('decoder_type', 'bart')\n",
    "opt.__setattr__('pretrained_tokenizer', True)\n",
    "opt.__setattr__('copy_attn', False)\n",
    "\n",
    "opt.__setattr__('valid_batch_size', 1)\n",
    "opt.__setattr__('batch_size_multiple', 1)\n",
    "opt.__setattr__('bucket_size', 128)\n",
    "opt.__setattr__('pool_factor', 256)\n",
    "\n",
    "opt.__setattr__('beam_size', 1)\n",
    "opt.__setattr__('gpu', 0)\n",
    "\n",
    "if isinstance(opt.data, str): setattr(opt, 'data', json.loads(opt.data.replace('\\'', '\"')))\n",
    "setattr(opt, 'data_task', ModelTask.SEQ2SEQ)\n",
    "ArgumentParser._get_all_transform(opt)\n",
    "\n",
    "translator = build_translator(opt, report_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check if model is on GPU\n",
    "print(translator._gpu)\n",
    "print(translator._use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded #(docs)=10000\n",
      "4399\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'jptimes'\n",
    "dataset_path = '/zfs1/hdaqing/rum20/kp/data/kp/json/%s/test.json' % dataset_name\n",
    "\n",
    "with open(dataset_path, 'r') as f:\n",
    "    ex_dicts = [json.loads(l) for l in f.readlines()]\n",
    "    for ex in ex_dicts:        \n",
    "        if dataset_name.startswith('openkp'):\n",
    "            ex['title'] = ''\n",
    "            ex['abstract'] = ex['text']\n",
    "            ex['keywords'] = ex['KeyPhrases']\n",
    "            ex['dataset_type'] = 'webpage'\n",
    "        elif dataset_name.startswith('stackex'):\n",
    "            ex['abstract'] = ex['question']\n",
    "            ex['keywords'] = ex['tags'].split(';')\n",
    "            ex['dataset_type'] = 'qa'\n",
    "        elif dataset_name.startswith('kp20k') or dataset_name.startswith('duc'):\n",
    "            ex['keywords'] = ex['keywords'].split(';') if isinstance(ex['keywords'], str) else ex['keywords']\n",
    "            ex['dataset_type'] = 'scipaper'\n",
    "        elif dataset_name.startswith('kptimes') or dataset_name.startswith('jptimes'):\n",
    "            ex['keywords'] = ex['keyword'].split(';') if isinstance(ex['keyword'], str) else ex['keyword']\n",
    "            ex['dataset_type'] = 'news'\n",
    "        else:\n",
    "            print('????')\n",
    "\n",
    "\n",
    "print('Loaded #(docs)=%d' % (len(ex_dicts)))\n",
    "doc_id = random.randint(0, len(ex_dicts))\n",
    "doc_id = 4399\n",
    "ex_dict = ex_dicts[doc_id]\n",
    "\n",
    "print(doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained vocabulary from /zfs1/hdaqing/rum20/kp/data/kp/hf_vocab/roberta-base-kp\n",
      "Vocab size=50265, base vocab size=50265\n",
      "Loading pretrained vocabulary from /zfs1/hdaqing/rum20/kp/data/kp/hf_vocab/roberta-base-kp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating in batches: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size=50265, base vocab size=50265\n",
      "<present>10<header>5<category>5<seealso>2<infill>0<s>Search warrants tied to former Trump lawyer Michael Cohen released  . WASHINGTON - Months before the FBI raided Michael Cohen’s office and hotel room, investigators were examining the flow of foreign money into his bank accounts and looking into whether the funds might be connected to a plan to lift sanctions on Russia, according to court filings unsealed Wednesday. The five search warrant applications, made in the early weeks and months of special counsel Robert Mueller’s Russia investigation in 2017, were made public in response to requests from The Associated Press and other media organizations. Ultimately, Cohen was not charged by Mueller or by prosecutors in New York with anything related to Russian collusion or illegal influence peddling, but the documents shed further light on how he capitalized financially on his closeness to the president immediately following the 2016 election. Cohen, who at the time was the president’s personal lawyer and a close aide and confidant, quickly cut deals to act as a highly paid consultant to several foreign and domestic companies with business interests linked to federal government decisions. Investigators said in the warrant applications that a corporate entity Cohen had created, Essential Consultants LLC, had received multiple deposits from foreign businesses and entities, including from companies they said had “significant ties to foreign governments or are entities controlled by foreign governments.” Essential Consultants received funds from U.S. and foreign corporations who appear to have approached Cohen “in connection with political objectives in the Trump administration,” investigators wrote. Investigators were especially curious about deposits of more than $416,000 from an account linked to an investment management firm, Columbus Nova, LLC. The warrants link that firm, and the holding company that controls it, to Viktor Vekselberg, a Russian oligarch with ties to Russian President Vladimir Putin. In an application to search his Trump Organization email account, prosecutors said Cohen exchanged over 230 phone calls and 950 text messages with the CEO of Columbus Nova between Nov. 8, 2016, and July 14, 2017. There were no text messages or phone calls before Election Day in 2016, prosecutors said. Columbus Nova has said in a statement that it is solely owned and controlled by Americans. It has described as false any allegation that Vekselberg used Columbus Nova as a conduit for payments to Cohen. The warrant applications also make clear that investigators at the time were examining whether any of the fund transfers were connected to Cohen’s involvement in a plan, described months earlier in a New York Times story, to try to get the U.S. to lift sanctions on Russia. Cohen has acknowledged offering his insights into Trump’s administration to multiple corporate clients, but said he broke no laws in doing so. Representatives for Cohen, who began serving a three-year prison sentence this month, declined comment Wednesday. The newly unsealed material reveals nothing about Trump’s own role in the crimes that put Cohen behind bars. Cohen is now serving a three-year prison sentence for tax evasion, lying to Congress about a Trump real estate project in Moscow, and campaign finance violations related to hush-money payments he orchestrated to two women who claimed to have had affairs with Trump, the porn actress Stormy Daniels and erotic model Karen McDougal. The warrant applications, which covered requests to search Cohen’s email accounts, including one associated with the Trump Organization, were blacked out in certain sections to protect the secrecy of an ongoing federal investigation into Cohen’s campaign finance crimes. Cohen has said he arranged payments to McDougal and Daniels at Trump’s behest, which the president has denied along with the affairs.\n",
      "michael cohen<sep>donald trump<sep>viktor vekselberg<sep>russia probe<sep>robert mueller<sep>u.s .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating in batches: 1it [00:01,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total translation time (s): 1.530445\n",
      "Average translation time (s): 1.530445\n",
      "Tokens per second: 1.306810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_pres, num_header, num_cat, num_seealso, num_infill = 10, 5, 5, 2, 0\n",
    "\n",
    "control_prefix = '<present>%d<header>%d<category>%d<seealso>%d<infill>%d<s>' \\\n",
    "    % (num_pres, num_header, num_cat, num_seealso, num_infill)\n",
    "\n",
    "new_ex_dict = copy.copy(ex_dict)\n",
    "new_ex_dict['src_control_prefix'] = control_prefix\n",
    "# new_ex_dict['title'] = ex_dict['title']    \n",
    "\n",
    "scores, preds = translator.translate(\n",
    "    src=[new_ex_dict],\n",
    "    batch_size=opt.batch_size,\n",
    "    attn_debug=opt.attn_debug,\n",
    "    opt=opt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of onmt.translate.translator failed: Traceback (most recent call last):\n",
      "  File \"/ihome/hdaqing/rum20/anaconda3/envs/kp/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ihome/hdaqing/rum20/anaconda3/envs/kp/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ihome/hdaqing/rum20/anaconda3/envs/kp/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ihome/hdaqing/rum20/anaconda3/envs/kp/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/zfs1/hdaqing/rum20/kp/OpenNMT-kpg-transfer/onmt/translate/translator.py\", line 24, in <module>\n",
      "    from onmt.keyphrase.eval import eval_and_print\n",
      "ImportError: cannot import name 'eval_and_print' from 'onmt.keyphrase.eval' (/zfs1/hdaqing/rum20/kp/OpenNMT-kpg-transfer/onmt/keyphrase/eval.py)\n",
      "]\n",
      "[autoreload of onmt.transforms.keyphrase failed: Traceback (most recent call last):\n",
      "  File \"/ihome/hdaqing/rum20/anaconda3/envs/kp/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/ihome/hdaqing/rum20/anaconda3/envs/kp/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/ihome/hdaqing/rum20/anaconda3/envs/kp/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/ihome/hdaqing/rum20/anaconda3/envs/kp/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/zfs1/hdaqing/rum20/kp/OpenNMT-kpg-transfer/onmt/transforms/keyphrase.py\", line 13, in <module>\n",
      "    class KeyphraseTransform(Transform):\n",
      "  File \"/zfs1/hdaqing/rum20/kp/OpenNMT-kpg-transfer/onmt/transforms/__init__.py\", line 33, in register_transfrom_cls\n",
      "    'Cannot register duplicate transform ({})'.format(name))\n",
      "ValueError: Cannot register duplicate transform (keyphrase)\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#present_tgts=3]  [['robert', 'mueller'], ['michael', 'cohen'], ['viktor', 'vekselberg']]\n",
      "[#absent_tgts=3] [['u.s', '.'], ['donald', 'trump'], ['russia', 'probe']]\n",
      "[#valid_present_preds=8] [['viktor', 'vekselberg'], ['robert', 'mueller'], ['russian', 'president'], ['russian', 'collusion'], ['vladimir', 'putin'], ['michael', 'cohen'], ['fbi', 'raids'], ['were', 'made', 'public', 'in']]\n",
      "[#valid_absent_preds=8] [['donald', 'trump'], ['search', 'warrant', 'documents'], ['american', 'people', 'of', 'ukrainian', '-', 'jewish', 'descent'], ['american', 'people', 'of', 'russian', '-', 'jewish', 'descent'], ['list', 'of', 'topics', 'characterized', 'as', 'pseudoscience'], ['list', 'of', 'people', 'from', 'new', 'york', 'city'], ['cohen', 'family'], ['in', 'a', 'court', 'filing', 'on']]\n",
      "match_scores_exact [1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "valid_match_scores_exact [1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "all_exact_results {'f_score@k': 0.5, 'f_score@10': 0.5}\n",
      "present_exact_match_scores [1. 1. 0. 0. 0. 1. 0. 0.]\n",
      "present_exact_results {'f_score@k': 0.6666666666666666, 'f_score@10': 0.5454545454545454}\n",
      "absent_exact_match_scores [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "absent_exact_results {'f_score@M': 0.18181818181818182}\n",
      "==================================================\n",
      "[Source]: Search warrants tied to former Trump lawyer Michael Cohen released  . WASHINGTON - Months before the FBI raided Michael Cohen’s office and hotel room, investigators were examining the flow of foreign money into his bank accounts and looking into whether the funds might be connected to a plan to lift sanctions on Russia, according to court filings unsealed Wednesday. The five search warrant applications, made in the early weeks and months of special counsel Robert Mueller’s Russia investigation in 2017, were made public in response to requests from The Associated Press and other media organizations. Ultimately, Cohen was not charged by Mueller or by prosecutors in New York with anything related to Russian collusion or illegal influence peddling, but the documents shed further light on how he capitalized financially on his closeness to the president immediately following the 2016 election. Cohen, who at the time was the president’s personal lawyer and a close aide and confidant, quickly cut deals to act as a highly paid consultant to several foreign and domestic companies with business interests linked to federal government decisions. Investigators said in the warrant applications that a corporate entity Cohen had created, Essential Consultants LLC, had received multiple deposits from foreign businesses and entities, including from companies they said had “significant ties to foreign governments or are entities controlled by foreign governments.” Essential Consultants received funds from U.S. and foreign corporations who appear to have approached Cohen “in connection with political objectives in the Trump administration,” investigators wrote. Investigators were especially curious about deposits of more than $416,000 from an account linked to an investment management firm, Columbus Nova, LLC. The warrants link that firm, and the holding company that controls it, to Viktor Vekselberg, a Russian oligarch with ties to Russian President Vladimir Putin. In an application to search his Trump Organization email account, prosecutors said Cohen exchanged over 230 phone calls and 950 text messages with the CEO of Columbus Nova between Nov. 8, 2016, and July 14, 2017. There were no text messages or phone calls before Election Day in 2016, prosecutors said. Columbus Nova has said in a statement that it is solely owned and controlled by Americans. It has described as false any allegation that Vekselberg used Columbus Nova as a conduit for payments to Cohen. The warrant applications also make clear that investigators at the time were examining whether any of the fund transfers were connected to Cohen’s involvement in a plan, described months earlier in a New York Times story, to try to get the U.S. to lift sanctions on Russia. Cohen has acknowledged offering his insights into Trump’s administration to multiple corporate clients, but said he broke no laws in doing so. Representatives for Cohen, who began serving a three-year prison sentence this month, declined comment Wednesday. The newly unsealed material reveals nothing about Trump’s own role in the crimes that put Cohen behind bars. Cohen is now serving a three-year prison sentence for tax evasion, lying to Congress about a Trump real estate project in Moscow, and campaign finance violations related to hush-money payments he orchestrated to two women who claimed to have had affairs with Trump, the porn actress Stormy Daniels and erotic model Karen McDougal. The warrant applications, which covered requests to search Cohen’s email accounts, including one associated with the Trump Organization, were blacked out in certain sections to protect the secrecy of an ongoing federal investigation into Cohen’s campaign finance crimes. Cohen has said he arranged payments to McDougal and Daniels at Trump’s behest, which the president has denied along with the affairs. \n",
      "[GROUND-TRUTH] #(all)=6, #(present)=3, #(absent)=3\n",
      "\t\tu.s .\n",
      "\t\t[robert mueller]\n",
      "\t\tdonald trump\n",
      "\t\trussia probe\n",
      "\t\t[michael cohen]\n",
      "\t\t[viktor vekselberg]\n",
      "[PREDICTION] #(all)=17, #(valid)=16, #(present)=8, #(valid&present)=8, #(valid&absent)=8\n",
      "\t\t[1] [1.8993]\t[viktor vekselberg] \t[correct!]\n",
      "\t\t[2] [1.8993]\t[robert mueller] \t[correct!]\n",
      "\t\t[3] [1.8993]\t[russian president] \t\n",
      "\t\t[4] [1.8993]\t[russian collusion] \t\n",
      "\t\t[5] [1.8993]\t[vladimir putin] \t\n",
      "\t\t[6] [1.8993]\tdonald trump \t[correct!]\n",
      "\t\t[7] [1.8993]\t[michael cohen] \t[correct!]\n",
      "\t\t[8] [1.8993]\t[fbi raids] \t\n",
      "\t\t[9] [1.8993]\tsearch warrant documents \t\n",
      "\t\t[10] [1.8993]\tamerican people of ukrainian - jewish descent \t\n",
      "\t\t[11] [1.8993]\tamerican people of russian - jewish descent \t\n",
      "\t\t[12] [1.8993]\tlist of topics characterized as pseudoscience \t\n",
      "\t\t[13] [1.8993]\tlist of people from new york city \t\n",
      "\t\t[14] [1.8993]\tcohen family \t\n",
      "\t\t\t[15] [1.8993]\tto the trump organization . the warrants \t\n",
      "\t\t[16] [1.8993]\t[were made public in] \t\n",
      "\t\t[17] [1.8993]\tin a court filing on \t\n",
      "\n",
      " ======================================================= \n",
      "[GROUND-TRUTH] #(all)=6, #(present)=3, #(absent)=3\n",
      "\n",
      "[PREDICTION] #(all)=17, #(valid)=16, #(present)=8, #(valid&present)=8, #(valid&absent)=8\n",
      "\n",
      " --- batch all_exact F1 @10: \t0.5000\n",
      " --- batch all_exact F1 @k: \t0.5000\n",
      " --- batch present_exact F1 @10: \t0.5455\n",
      " --- batch present_exact F1 @k: \t0.6667\n",
      " --- batch absent_exact F1 @M: \t0.1818\n",
      " =======================================================\n"
     ]
    }
   ],
   "source": [
    "src_text = new_ex_dict['title'] + ' . ' + new_ex_dict['abstract']\n",
    "\n",
    "# print results\n",
    "eval_and_print(src_text, tgt_kps=ex_dict['keywords'], pred_kps=preds[0], pred_scores=scores[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun phrases\n",
    "\n",
    "Use min_len=2, max_len=6, ignore single word.\n",
    "See **onmt.keyphrase.extract_np.py** for more examples.\n",
    "\n",
    "Some issues:\n",
    "1. Most common, participles are mistakenly tagged as VERB.\n",
    "  - image segmentation through evolved cellular automata ['NOUN', 'NOUN', 'ADP', 'VERB', 'ADJ', 'NOUN']\n",
    "  - HQCRFF-based modulator ['PROPN', '-', 'VERB', 'NOUN']\n",
    "  - collection of organized data ['NOUN', 'ADP', 'VERB', 'NOUN']\n",
    "  - irregularly-sampled data ['ADV', '-', 'VERB', 'NOUN']\n",
    "2. NP containing numbers, rare.\n",
    "  - sidewall angle of 90 ['ADJ', 'NOUN', 'ADP', 'NUM']\n",
    "3. Single-word phrases are ignored, and seemingly no simple way to resolve. Some abbreviations and acronyms are ignored.\n",
    "4. Starting with ADP.\n",
    "  - in-band zeros ['ADP', '-', 'NOUN', 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_chunks_by_pos_regex(text, min_len, max_len):\n",
    "    '''\n",
    "    https://files.ifi.uzh.ch/cl/hess/classes/ecl1/termerCIE.html\n",
    "        (Adjective | Noun)* (Noun Preposition)? (Adjective | Noun)* Noun\n",
    "    https://www.aclweb.org/anthology/D09-1027.pdf\n",
    "        (JJ)*(NN|NNS|NNP)+\n",
    "    :param doc:\n",
    "    :param min_len:\n",
    "    :param max_len:\n",
    "    :return:\n",
    "    '''\n",
    "    doc = spacy_nlp(text, disable=[\"textcat\"])\n",
    "\n",
    "    np_regex = r'((^ADJ|^NOUN|^PROPN)(ADP|-|ADJ|NOUN|PROPN)*?)?(NOUN|PROPN)+'\n",
    "    cands = []\n",
    "    # a two-layer loop to get all n-grams\n",
    "    for i in range(0, len(doc) - 1):\n",
    "        for k in range(min_len, max_len + 1):\n",
    "            if i + k > len(doc): break\n",
    "            span = doc[i: i + k]\n",
    "            pos = ['-' if t.text=='-' else t.pos_ for t in span]\n",
    "            pos_str = ''.join(pos)\n",
    "\n",
    "            cands.append((span, pos_str, pos))\n",
    "\n",
    "#     for np_id, (np, pos_str, pos) in enumerate(cands):\n",
    "#         print('[%d]' % np_id, np, str(pos), '[match]' if re.fullmatch(np_regex, pos_str) else '')\n",
    "        \n",
    "    cands = [span.text for span, pos_str, pos in cands if re.fullmatch(np_regex, pos_str)]\n",
    "\n",
    "    return cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "kp20k_valid2k\n",
      "****************************************************************************************************\n",
      "0\n",
      "1000\n",
      "****************************************************************************************************\n",
      "openkp_valid2k\n",
      "****************************************************************************************************\n",
      "0\n",
      "1000\n",
      "****************************************************************************************************\n",
      "kptimes_valid2k\n",
      "****************************************************************************************************\n",
      "0\n",
      "1000\n",
      "****************************************************************************************************\n",
      "stackex_valid2k\n",
      "****************************************************************************************************\n",
      "0\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "dataset_names = ['kp20k_train100k', 'kptimes_train100k', 'openkp_train100k', 'stackex_train100k']\n",
    "dataset_names = ['kp20k', 'inspec', 'krapivin', 'nus', 'semeval', 'openkp', 'kptimes', 'jptimes', 'stackex', 'duc']\n",
    "dataset_names = ['kp20k_valid2k', 'openkp_valid2k', 'kptimes_valid2k', 'stackex_valid2k']\n",
    "\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print('*' * 100)\n",
    "    print(dataset_name)\n",
    "    print('*' * 100)\n",
    "    input_path = '/zfs1/hdaqing/rum20/kp/data/kp/json/%s/test.json' % dataset_name\n",
    "    output_path = '/zfs1/hdaqing/rum20/kp/fairseq-kpg/exps/kp_nounphrase/checkpoint_step_9500-data_%s_test.pred' % dataset_name\n",
    "\n",
    "    with open(input_path, 'r') as input_jsonl, open(output_path, 'w') as output_jsonl:\n",
    "        for l_id, l in enumerate(input_jsonl):\n",
    "            if l_id % 1000 == 0: print('%d' % l_id)\n",
    "            ex = json.loads(l)\n",
    "\n",
    "            if dataset_name.startswith('openkp'):\n",
    "                src_text = ex['text']\n",
    "            elif dataset_name.startswith('stackex'):\n",
    "                src_text = ex['title'] + ' . ' + ex['question']\n",
    "            else:\n",
    "                src_text = ex['title'] + ' . ' + ex['abstract']\n",
    "\n",
    "            nps = noun_chunks_by_pos_regex(src_text, min_len=2, max_len=6)\n",
    "\n",
    "#             print(src_text)\n",
    "#             for np_id, np in enumerate(nps):\n",
    "#                 print('[%d]' % np_id, np)\n",
    "\n",
    "            # remove duplicates and write to file\n",
    "            nps = list(set(nps))\n",
    "            output_ex = {'pred_sents': nps}\n",
    "            output_jsonl.write(json.dumps(output_ex) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
